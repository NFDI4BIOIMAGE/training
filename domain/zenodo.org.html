
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Zenodo.org (275) &#8212; NFDI4BioImage Training Materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/css/searchbox.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/0.6.0/lunr.min.js"></script>
    <script src="../_static/js/searchbox.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'domain/zenodo.org';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Statistics" href="../statistics/readme.html" />
    <link rel="prev" title="Www.youtube.com (28)" href="www.youtube.com.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="2025-07-16"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../readme.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="NFDI4BioImage Training Materials - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="NFDI4BioImage Training Materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../readme.html">
                    NFDI4BioImage Training Materials
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">What's new</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../whats_new.html">Recently added (10)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contributing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">How to contribute</a></li>

<li class="toctree-l1"><a class="reference internal" href="../contributing/submit_app.html">Using the Training Materials Submission App</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/format.html">YML format</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By tag</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tags/ai-ready.html">Ai-ready (84)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/artificial_intelligence.html">Artificial intelligence (46)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/bioimage_analysis.html">Bioimage analysis (201)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/bioinformatics.html">Bioinformatics (19)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/data_stewardship.html">Data stewardship (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/fair-principles.html">Fair-principles (27)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/fiji.html">Fiji (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/galaxy.html">Galaxy (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/imagej.html">Imagej (20)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/licensing.html">Licensing (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/metadata.html">Metadata (15)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/napari.html">Napari (14)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/neubias.html">Neubias (27)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/nfdi4bioimage.html">Nfdi4bioimage (46)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/omero.html">Omero (38)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/open_science.html">Open science (9)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/open_source_software.html">Open source software (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/python.html">Python (71)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/reproducibility.html">Reproducibility (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/research_data_management.html">Research data management (140)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/sharing.html">Sharing (12)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/workflow.html">Workflow (9)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tags/workflow_engine.html">Workflow engine (13)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By content type</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../content_types/blog%20post.html">Blog post (28)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/book.html">Book (21)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/code.html">Code (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/collection.html">Collection (84)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/data.html">Data (92)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/document.html">Document (8)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/documentation.html">Documentation (19)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/event.html">Event (8)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/github%20repository.html">Github repository (62)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/notebook.html">Notebook (55)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/online%20tutorial.html">Online tutorial (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/poster.html">Poster (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/preprint.html">Preprint (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/publication.html">Publication (71)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/slides.html">Slides (84)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/tutorial.html">Tutorial (48)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/video.html">Video (38)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/website.html">Website (11)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/workshop.html">Workshop (14)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By license</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../licenses/all_rights_reserved.html">All rights reserved (13)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/apache-2.0.html">Apache-2.0 (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/bsd-2-clause.html">Bsd-2-clause (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/bsd-3-clause.html">Bsd-3-clause (32)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc-by-3.0.html">Cc-by-3.0 (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc-by-4.0.html">Cc-by-4.0 (359)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc-by-nc-sa-4.0.html">Cc-by-nc-sa-4.0 (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc0-1.0.html">Cc0-1.0 (25)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/gpl-2.0.html">Gpl-2.0 (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/gpl-3.0.html">Gpl-3.0 (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/mit.html">Mit (31)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/unknown.html">Unknown (109)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By domain</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bbbc.broadinstitute.org.html">Bbbc.broadinstitute.org (12)</a></li>
<li class="toctree-l1"><a class="reference internal" href="biapol.github.io.html">Biapol.github.io (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="docs.google.com.html">Docs.google.com (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="doi.org.html">Doi.org (289)</a></li>
<li class="toctree-l1"><a class="reference internal" href="f1000research.com.html">F1000research.com (11)</a></li>
<li class="toctree-l1"><a class="reference internal" href="focalplane.biologists.com.html">Focalplane.biologists.com (14)</a></li>
<li class="toctree-l1"><a class="reference internal" href="git.mpi-cbg.de.html">Git.mpi-cbg.de (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="github.com.html">Github.com (143)</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.galaxyproject.org.html">Training.galaxyproject.org (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="www.biorxiv.org.html">Www.biorxiv.org (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="www.ebi.ac.uk.html">Www.ebi.ac.uk (18)</a></li>
<li class="toctree-l1"><a class="reference internal" href="www.nature.com.html">Www.nature.com (18)</a></li>
<li class="toctree-l1"><a class="reference internal" href="www.youtube.com.html">Www.youtube.com (28)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Zenodo.org (275)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../statistics/readme.html">Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../export/readme.html">Open data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gdpr_compliance.html">GDPR Compliance Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../imprint.html">Imprint</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NFDI4BIOIMAGE/training" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NFDI4BIOIMAGE/training/issues/new?title=Issue%20on%20page%20%2Fdomain/zenodo.org.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/domain/zenodo.org.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Zenodo.org (275)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zenodo-und-co-was-bringt-und-wer-braucht-ein-repositorium">“ZENODO und Co.” Was bringt und wer braucht ein Repositorium?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lif-files-misbehaving-in-fiji-but-fine-in-lasx">.lif files misbehaving in fiji but fine in LASX</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frames-of-fluorescent-particles">10 frames of fluorescent particles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-nuclei-annotations-and-stardist-3d-model-s-rat-brain">3D Nuclei annotations and StarDist 3D model(s) (rat brain)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-nuclei-instance-segmentation-dataset-of-fluorescence-microscopy-volumes-of-c-elegans">3D nuclei instance segmentation dataset of fluorescence microscopy volumes of C. elegans</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-towards-reproducible-research">6 Steps Towards Reproducible Research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-glimpse-of-the-open-source-flim-analysis-software-tools-flimfit-flute-and-napari-flim-phasor-plotter">A Glimpse of the Open-Source FLIM Analysis Software Tools FLIMfit, FLUTE and napari-flim-phasor-plotter</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deep-learning-approach-to-quantify-auditory-hair-cells">A deep learning approach to quantify auditory hair cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-journey-to-fair-microscopy-data">A journey to FAIR microscopy data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abdominal-imaging-window-aiw-for-intravital-imaging">Abdominal Imaging Window (AIW) for Intravital Imaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aberrated-bead-stack">Aberrated Bead Stack</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accessible-interactive-spatial-omics-data-visualizations-with-vitessce-and-omero">Accessible Interactive Spatial-Omics Data Visualizations with Vitessce and OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advancing-fair-image-analysis-in-galaxy-tools-workflows-and-training">Advancing FAIR Image Analysis in Galaxy: Tools, Workflows, and Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alles-meins-oder-urheberrechte-klaren-fur-forschungsdaten">Alles meins – oder!? Urheberrechte klären für Forschungsdaten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-annotated-high-content-fluorescence-microscopy-dataset-with-hoechst-33342-stained-nuclei-and-manually-labelled-outlines">An annotated high-content fluorescence microscopy dataset with Hoechst 33342-stained nuclei and manually labelled outlines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#andor-dragonfly-confocal-image-of-bpae-cells-stained-for-actin-ims-file-format">Andor Dragonfly confocal image of BPAE cells stained for actin, IMS file format</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#angebote-der-nfdi-fur-die-forschung-im-bereich-zoologie">Angebote der NFDI für die Forschung im Bereich Zoologie</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-blobs-and-labels-image">Artificial Blobs and Labels image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#astigmatic-4pi-bead-stack">Astigmatic 4Pi bead stack</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-labelling-of-hela-kyoto-cells-using-deep-learning-tools">Automatic labelling of HeLa “Kyoto” cells using Deep Learning tools</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#axioscan-7-fluorescent-channels-not-displaying-in-qupath">Axioscan 7 fluorescent channels not displaying in QuPath</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beads-imaged-over-time">Beads imaged over time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-data-strudel-for-workshop-on-research-data-management-in-tu-dresden-core-facilities">Bio-Image Data Strudel for Workshop on Research Data Management in TU Dresden Core Facilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-analysis-code-generation">Bio-image Analysis Code Generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-analysis-code-generation-using-bia-bob">Bio-image Analysis Code Generation using bia-bob</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-analysis-with-the-help-of-large-language-models">Bio-image Analysis with the Help of Large Language Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-data-science-lectures-2025-uni-leipzig-scads-ai">Bio-image Data Science Lectures 2025 @ Uni Leipzig / ScaDS.AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-data-science-lectures-uni-leipzig-scads-ai">Bio-image Data Science Lectures @ Uni Leipzig / ScaDS.AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bioimage-io-chatbot-globias-seminar">BioImage.IO Chatbot, GloBIAS Seminar</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#breast-cancer-nuclei-images-for-dl-training-zerocostdl4mic-stardist-model">Breast Cancer Nuclei images for DL Training + ZeroCostDL4Mic StarDist Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-fair-image-analysis-pipelines-for-high-content-screening-hcs-data-using-galaxy">Building FAIR image analysis pipelines for high-content-screening (HCS) data using Galaxy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-fair-image-data-ecosystem-for-microscopy-communities">Building a FAIR image data ecosystem for microscopy communities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-national-research-data-infrastructure">Building a National Research Data Infrastructure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#czi-carl-zeiss-image-dataset-with-artificial-test-camera-images-with-various-dimension-for-testing-libraries-reading">CZI (Carl Zeiss Image) dataset with artificial test camera images with various dimension for testing libraries reading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#czi-file-examples">CZI file examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#czi-open-science-program-collection">CZI: Open Science Program Collection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellbindb-a-large-scale-multimodal-annotated-dataset">CellBinDB: A Large-Scale Multimodal Annotated Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-model-for-digital-phase-contrast-images">Cellpose model for Digital Phase Contrast images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-models-for-label-prediction-from-brightfield-and-digital-phase-contrast-images">Cellpose models for Label Prediction from Brightfield and Digital Phase Contrast images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments">Cellpose training data and scripts from “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers">Cellpose training data and scripts from “Machine learning for histological annotation and quantification of cortical layers”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chatgpt-for-image-analysis">ChatGPT for Image Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collaborative-working-and-version-control-with-git-hub">Collaborative Working and Version Control with git[hub]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Collaborative working and  Version Control with Git[Hub]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-1-breast-cancer-cell-dataset">Combining StarDist and TrackMate example 1 -  Breast cancer cell dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-2-t-cell-dataset">Combining StarDist and TrackMate example 2 -  T cell dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-3-flow-chamber-dataset">Combining StarDist and TrackMate example 3 -  Flow chamber dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-the-bids-and-arc-directory-structures-for-multimodal-research-data-organization">Combining the BIDS and ARC Directory Structures for Multimodal Research Data Organization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conference-slides-4th-day-of-intravital-microscopy">Conference Slides - 4th Day of Intravital Microscopy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#crashkurs-forschungsdatenmanagement">Crashkurs Forschungsdatenmanagement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-open-computational-curricula">Creating open computational curricula</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cultivating-open-training">Cultivating Open Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cultivating-open-training-to-advance-bio-image-analysis">Cultivating Open Training to advance Bio-image Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dalia-interchange-format">DALIA Interchange Format</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-stewardship-and-research-data-management-tools-for-multimodal-linking-of-imaging-data-in-plasma-medicine">Data stewardship and research data management tools for multimodal linking of imaging data in plasma medicine</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataviz-protocols-an-introduction-to-data-visualization-protocols-for-wet-lab-scientists">DataViz protocols - An introduction to data visualization protocols for wet lab scientists</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-from-incell-2200-microscope-misread-as-a-plate">Dataset from InCell 2200 microscope misread as a plate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datenmanagement">Datenmanagement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datenmanagement-im-fokus-organisation-speicherstrategien-und-datenschutz">Datenmanagement im Fokus: Organisation, Speicherstrategien und Datenschutz</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datenmanagementplane-erstellen-teil-1">Datenmanagementpläne erstellen - Teil 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datenmanagementplane-erstellen-teil-2">Datenmanagementpläne erstellen - Teil 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deconvolution-test-dataset">Deconvolution Test Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-segmentation-projects-of-fib-sem-dataset-of-u2-os-cell">Deep learning segmentation projects of FIB-SEM dataset of U2-OS cell</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-training-data-jove">Deep learning training data (JOVE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-bacillus-subtilis-fluorescence-segmentation-dataset">DeepBacs – Bacillus subtilis fluorescence segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-escherichia-coli-bright-field-segmentation-dataset">DeepBacs – Escherichia coli bright field segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-mixed-segmentation-dataset-and-stardist-model">DeepBacs – Mixed segmentation dataset and StarDist model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-staphylococcus-aureus-widefield-segmentation-dataset">DeepBacs – Staphylococcus aureus widefield segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-a-training-strategy">Developing a Training Strategy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#development-of-a-platform-for-advanced-optics-education-training-and-prototyping">Development of a platform for advanced optics education, training and prototyping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#digital-phase-contrast-on-primary-dermal-human-fibroblasts-cells">Digital Phase Contrast on Primary Dermal Human Fibroblasts cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-local-topography-on-cell-division-of-staphylococci-sp">Effect of local topography on cell division of Staphylococci sp.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#efficiently-starting-institutional-research-data-management">Efficiently starting institutional research data management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#einblicke-ins-forschungsdatenmanagement-darf-ich-das-veroffentlichen-rechtsfragen-im-umgang-mit-forschungsdaten">Einblicke ins Forschungsdatenmanagement - Darf ich das veröffentlichen? Rechtsfragen im Umgang mit Forschungsdaten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#engineering-a-software-environment-for-research-data-management-of-microscopy-image-data-in-a-core-facility">Engineering a Software Environment for Research Data Management of Microscopy Image Data in a Core Facility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-scientific-ambassadors-program">Euro-BioImaging  Scientific Ambassadors Program</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-eric-annual-report-2022">Euro-BioImaging ERIC Annual Report 2022</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-s-guide-to-fair-bioimage-data-practical-tasks">Euro-BioImaging’s Guide to FAIR BioImage Data - Practical Tasks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-s-template-for-research-data-management-plans">Euro-BioImaging’s Template for Research Data Management Plans</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-batchconvert-v0-0-4">Euro-BioImaging/BatchConvert: v0.0.4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evident-oir-sample-files-tiles-stitched-image-fv-4000">Evident OIR sample files tiles + stitched image - FV 4000</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evident-oir-sample-files-with-lambda-scan-fv-4000">Evident OIR sample files with lambda scan - FV 4000</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-imaris-ims-datasets">Example Imaris ims datasets.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-microscopy-metadata-json-files-produced-using-micro-meta-app-to-document-example-microscopy-experiments-performed-at-individual-core-facilities">Example Microscopy Metadata JSON files produced using Micro-Meta App to document example microscopy experiments performed at individual core facilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-operetta-dataset">Example Operetta Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#excel-template-for-adding-key-value-pairs-to-images">Excel template for adding Key-Value Pairs to images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expansion-and-fluctuations-enhanced-microscopy-for-nanoscale-molecular-profiling-of-cells-and-tissues-data-processing-manual">Expansion and fluctuations-enhanced microscopy for nanoscale molecular profiling of cells and tissues - Data processing manual</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explainable-ai-for-computer-vision">Explainable AI for Computer Vision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fiber-and-vessel-dataset-for-segmentation-and-characterization">Fiber and vessel dataset for segmentation and characterization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forschungsdatenmanagement-zukunftsfest-gestalten-impulse-fur-die-strukturevaluation-der-nationalen-forschungsdateninfrastruktur-nfdi">Forschungsdatenmanagement zukunftsfest gestalten – Impulse für die   Strukturevaluation der Nationalen Forschungsdateninfrastruktur (NFDI)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-cells-to-pixels-bridging-biologists-and-image-analysts-through-a-common-language">From Cells to Pixels: Bridging Biologists and  Image Analysts Through a Common Language</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-paper-to-pixels-navigation-through-your-research-data-presentations-of-speakers">From Paper to Pixels: Navigation through your Research Data - presentations of speakers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#galaxy-meets-omero-overview-on-the-galaxy-omero-suite-and-vizarr-viewer">Galaxy meets OMERO! Overview on the Galaxy OMERO-suite and Vizarr Viewer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gerbi-chat-teil-1-vom-bedarf-bis-zum-groszgerateantrag-schreiben">GerBI-Chat: Teil 1 - Vom Bedarf bis zum Großgeräteantrag-Schreiben</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gerbi-chat-teil-2-wie-schreibe-ich-am-besten-einen-groszegrateantrag">GerBI-Chat: Teil 2 - Wie schreibe ich am besten einen Großegräteantrag</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started-with-python-intro-and-set-up-a-conda-environment">Getting started with Python: intro and set-up a conda environment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#globias-in-person-workshop-2024">GloBIAS in-person workshop 2024</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gut-analysis-toolbox">Gut Analysis Toolbox</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gut-analysis-toolbox-training-data-and-2d-models-for-segmenting-enteric-neurons-neuronal-subtypes-and-ganglia">Gut Analysis Toolbox: Training data and 2D models for segmenting enteric neurons, neuronal subtypes and ganglia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hpa-nucleus-segmentation-dpnunet">HPA Nucleus Segmentation (DPNUnet)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking">HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hackaton-results-conversion-of-knime-image-analysis-workflows-to-galaxy">Hackaton Results - Conversion of KNIME image analysis workflows to Galaxy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hela-kyoto-cells-under-the-scope">HeLa “Kyoto” cells under the scope</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-throughput-automated-data-analysis-and-data-management-workflow-with-cellprofiler-and-omero">High throughput &amp; automated data analysis and data management workflow with Cellprofiler and OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-dab-staining-axioscan-bf-20x">Human DAB staining Axioscan BF 20x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-lung-tissue-microscopy-dic-fluorescence-cell-and-nuclei-semantic-instance-annotations">Human Lung Tissue Microscopy (DIC, Fluorescence, Cell and Nuclei Semantic Instance Annotations)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#i3d-bio-s-omero-training-material-re-usable-adjustable-multi-purpose-slides-for-local-user-training">I3D:bio’s OMERO training material: Re-usable, adjustable, multi-purpose slides for local user training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ics-ids-stitched-file">ICS/IDS stitched file</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-analysis-using-galaxy">Image Analysis using Galaxy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-repository-decision-tree-where-do-i-deposit-my-imaging-data">Image Repository Decision Tree - Where do I deposit my imaging data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-handling-using-fiji-training-materials">Image handling using Fiji - training materials</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imagej-tool-for-percentage-estimation-of-pneumonia-in-lungs">ImageJ tool for percentage estimation of pneumonia in lungs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-kidney">Implantation of abdominal imaging windows on the mouse kidney</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-kidney-short-version">Implantation of abdominal imaging windows on the mouse kidney - short version</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-liver">Implantation of abdominal imaging windows on the mouse liver</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-liver-short-version">Implantation of abdominal imaging windows on the mouse liver - short version</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#incell-datasets-with-mix-of-2d-and-3d-failed-to-be-read">InCell datasets with mix of 2D and 3D failed to be read</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ink-in-a-dish">Ink in a dish</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#insights-and-impact-from-five-cycles-of-essential-open-source-software-for-science">Insights and Impact From Five Cycles of Essential Open Source Software for Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#insights-from-acquiring-open-medical-imaging-datasets-for-foundation-model-development">Insights from Acquiring Open Medical Imaging  Datasets for Foundation Model Development</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Insights from Acquiring Open Medical Imaging Datasets for Foundation Model Development</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#institutionalization-and-collaboration-as-a-way-of-addressing-the-challenges-open-science-presents-to-libraries-the-university-of-konstanz-as-a-national-pioneer">Institutionalization and Collaboration as a Way of Addressing the Challenges Open Science Presents to Libraries: The University of Konstanz as a National Pioneer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-of-bioimage-and-omics-data-resources">Integration of Bioimage and *Omics data resources</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-bioimage-analysis-workflow-with-clij-eabias-2025-training-event">Interactive Bioimage Analysis Workflow with CLIJ (@EABIAS 2025 training event)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-image-data-flow-graphs">Interactive Image Data Flow Graphs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intravital-microscopy-contrasting-agents-for-application-database">Intravital microscopy contrasting agents for application - Database</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-omero-vitessce-an-omero-web-plugin-for-multi-modal-data">Introducing OMERO-vitessce: an OMERO.web plugin for multi-modal data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-omero-frankfurt-online">Introduction to OMERO - Frankfurt - online</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-research-data-management-and-open-research">Introduction to Research Data Management and Open Research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-light-microscopy-widefield-microscopy">Introduction to light-microscopy / Widefield microscopy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-pair-template-for-annotation-in-omero-for-light-microscopy-data-acquired-with-axioscan7-core-facility-cellular-imaging-cfci">Key-Value pair template for annotation in OMERO for light microscopy data acquired with AxioScan7 - Core Facility Cellular Imaging (CFCI)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-pair-template-for-annotation-of-datasets-in-omero-perikles-study">Key-Value pair template for annotation of datasets in OMERO (PERIKLES study)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-pair-template-for-annotation-of-datasets-in-omero-for-light-and-electron-microscopy-data-within-the-research-group-of-prof-muller-reichert">Key-Value pair template for annotation of datasets in OMERO for light- and electron microscopy data within the research group of Prof. Müller-Reichert</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kollaboratives-arbeiten-und-versionskontrolle-mit-git">Kollaboratives Arbeiten und Versionskontrolle mit Git</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kriterienkatalog-fur-materialien-aus-dem-themenbereich-forschungsdatenmanagement">Kriterienkatalog für Materialien aus dem Themenbereich Forschungsdatenmanagement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leo-linking-eln-with-omero">LEO: Linking ELN with OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lmrg-image-analysis-study-fish-datasets">LMRG Image Analysis Study - FISH datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lmrg-image-analysis-study-nuclei-datasets">LMRG Image Analysis Study - nuclei datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lsm-example-j-dubrulle">LSM example J. Dubrulle</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lz4-compressed-imaris-ims-example-datasets">LZ4-compressed Imaris ims example datasets.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#large-language-models-an-introduction-for-life-scientists">Large Language Models: An Introduction for Life Scientists</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#large-tiling-confocal-acquisition-rat-brain">Large tiling confocal acquisition (rat brain)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laulauthom-maskfromrois-fiji-masks-from-rois-plugins-for-fiji-initial-release">LauLauThom/MaskFromRois-Fiji: Masks from ROIs plugins for Fiji - initial release</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laulauthom-maskfromrois-fiji-v1-0-1-better-handle-cancel">LauLauThom/MaskFromRois-Fiji: v1.0.1 - better handle “cancel”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-and-training-bio-image-analysis-in-the-age-of-ai">Learning and Training Bio-image Analysis in the Age of AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leica-lif-file-with-errors-in-channel-order-when-imported-with-bio-formats">Leica (.lif) file with errors in channel order when imported with Bio-formats</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leitfaden-zur-digitalen-datensparsamkeit-mit-praxisbeispielen">Leitfaden zur digitalen Datensparsamkeit (mit Praxisbeispielen)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limeseg-test-datasets">LimeSeg Test Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linked-open-data-for-microbial-population-biology">Linked (Open) Data for Microbial Population Biology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#liver-micrometastases-area-quantification-using-qupath-and-pixel-classifier">Liver Micrometastases area quantification using QuPath and pixel classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lynsec-lymphoma-nuclear-segmentation-and-classification">LyNSeC: Lymphoma Nuclear Segmentation and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#masterclasses-from-the-euro-bioimaging-evolve-mentoring-programme-2025">Masterclasses from the Euro-Bioimaging EVOLVE Mentoring programme 2025</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-reporter-activity-domain-in-epi-aggregates-and-gastruloids-ijm">Measuring reporter activity domain in EPI aggregates and Gastruloids.ijm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations">Melanoma Histopathology Dataset with Tissue and Nuclei Annotations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#membrain-seg-training-data">MemBrain-seg training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memorandum-of-understanding-of-nfdi-consortia-from-earth-chemical-and-life-sciences-to-support-a-network-called-the-geo-chem-life-science-helpdesk-cluster">Memorandum of Understanding of NFDI consortia from Earth-, Chemical and Life Sciences to support a network called the Geo-Chem-Life Science Helpdesk Cluster</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metadata-annotation-workflow-for-omero-with-tabbles">Metadata Annotation Workflow for OMERO with Tabbles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metadata-in-bioimaging">Metadata in Bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#microsam-talks">MicroSam-Talks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modular-training-resources-for-bioimage-analysis">Modular training resources for bioimage analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#morphological-analysis-of-neural-cells-with-weka-and-snt-fiji-plugins">Morphological analysis of neural cells with WEKA and SNT Fiji plugins</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-template-matching-for-object-detection-slides">Multi-Template-Matching for object-detection (slides)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiplexed-histology-of-covid-19-post-mortem-lung-samples-control-case-1-fov1">Multiplexed histology of COVID-19 post-mortem lung samples - CONTROL CASE 1 FOV1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#my-journey-through-bioimage-analysis-teaching-methods-from-classroom-to-cloud">My Journey Through Bioimage Analysis Teaching Methods From Classroom to Cloud</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage">NFDI4BIOIMAGE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis-online-kick-off-2023">NFDI4BIOIMAGE - National Research Data Infrastructure for Microscopy and BioImage Analysis - Online Kick-Off 2023</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis-conference-talk-the-pelagic-imaging-consortium-meets-helmholtz-imaging-5-10-2023-hamburg">NFDI4BIOIMAGE - National Research Data Infrastructure for Microscopy and BioImage Analysis [conference talk: The Pelagic Imaging Consortium meets Helmholtz Imaging, 5.10.2023, Hamburg]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis">NFDI4BIOIMAGE - National Research Data Infrastructure for Microscopy and Bioimage Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-data-management-illustrations-by-henning-falk">NFDI4BIOIMAGE data management illustrations by Henning Falk</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-ta3-hackathon-uoc-2023-cologne-hackathon-2023-github-repository">NFDI4Bioimage - TA3-Hackathon - UoC-2023 (Cologne-Hackathon-2023, GitHub repository)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-calendar-2024-october-original-image">NFDI4Bioimage Calendar 2024 October; original image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-calendar-2025-march-original-image">NFDI4Bioimage Calendar 2025 March; original image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#navigating-the-bioimage-analysis-landscape-understanding-the-community-and-its-collaborative-dynamics">Navigating the Bioimage Analysis Landscape: Understanding the Community  and its Collaborative Dynamics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nd2-does-not-open-in-fiji-bio-formats-8-1-1">Nd2 does not open in Fiji Bio_formats 8.1.1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nd2-does-not-open-in-fiji-bio-formats-8-1-1-additional-files">Nd2 does not open in Fiji Bio_formats 8.1.1 (additional files)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurips-2022-cell-segmentation-competition-dataset">NeurIPS 2022 Cell Segmentation Competition Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-kid-on-the-nfdi-block-nfdi4bioimage-a-national-initiative-for-fair-data-management-in-bioimaging-and-bioimage-analysis">New Kid on the (NFDI) Block: NFDI4BIOIMAGE  - A National Initiative for FAIR Data Management in Bioimaging and Bioimage Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nextflow-scalable-and-reproducible-scientific-workflows">Nextflow: Scalable and reproducible scientific workflows</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ocelot-overlapped-cell-on-tissue-dataset-for-histopathology">OCELOT: Overlapped Cell on Tissue Dataset for Histopathology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ome2024-ngff-challenge-results">OME2024 NGFF Challenge Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#omexcavator-a-tool-for-exporting-and-connecting-bioimaging-specific-metadata-in-wider-knowledge-graphs">OMExcavator: a tool for exporting and connecting  Bioimaging-specific metadata in wider knowledge graphs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#omexcavator-a-tool-for-exporting-and-connecting-domain-specific-metadata-in-a-wider-knowledge-graph">OMExcavator: a tool for exporting and connecting domain-specific metadata in a wider knowledge graph</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#open-science-sharing-licensing">Open Science, Sharing &amp; Licensing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimisation-and-validation-of-a-swarm-intelligence-based-segmentation-algorithm-for-low-contrast-positron-emission-tomography">Optimisation and Validation of a Swarm Intelligence based Segmentation Algorithm for low Contrast Positron Emission Tomography</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimized-cranial-window-implantation-for-subcellular-and-functional-imaging-in-vivo">Optimized cranial window implantation for subcellular and functional imaging in vivo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parhyale-3d-segmentation-dataset">Parhyale 3D segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#platynereis-em-training-data">Platynereis EM training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprint-be-sustainable-recommendations-for-fair-resources-in-life-sciences-research-eosc-life-s-lessons">Preprint: “Be Sustainable”, Recommendations for FAIR Resources in Life Sciences research: EOSC-Life’s Lessons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prodgerlab-stardist-hiv-target-cell-training-set">ProdgerLab-StarDist-HIV Target Cell Training Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-engineering-agentic-workflows-and-multi-modal-large-language-models">Prompt Engineering, Agentic Workflows and Multi-modal Large Language Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qupath-open-source-software-for-analysing-awkward-images">QuPath: Open source software for analysing (awkward) images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rdf-as-a-bridge-to-domain-platforms-like-omero-or-there-and-back-again">RDF as a bridge to domain-platforms like OMERO, or There and back again.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-data-management-on-campus-and-in-nfdi4bioimage">RESEARCH DATA MANAGEMENT on Campus and in NFDI4BIOIMAGE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstructed-images-of-a-2dsim-multiposition-acquisition">Reconstructed images of a 2DSIM multiposition acquisition.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#report-on-a-pilot-study-implementation-of-omero-for-microscopy-data-management">Report on a pilot study:  Implementation of OMERO for  microscopy data management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-data-management-seminar-slides">Research Data Management Seminar - Slides</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-data-managemet-and-how-not-to-get-overwhelmed-with-data">Research Data Managemet and how not to get overwhelmed with data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-tissue-segmentation-dataset">Root tissue segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#round-table-workshop-1-sample-stabilization-in-intravital-imaging">Round Table Workshop 1 - Sample Stabilization in intravital Imaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#round-table-workshop-2-correction-of-drift-and-movement">Round Table Workshop 2 - Correction of Drift and Movement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-data-for-pr-4284-https-github-com-ome-bioformats-pull-4284">Sample data for PR#4284 (https://github.com/ome/bioformats/pull/4284)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sciaugment">SciAugment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmentation-of-nuclei-in-histopathology-images-by-deep-regression-of-the-distance-map">Segmentation of Nuclei in Histopathology Images by deep regression of the distance map</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmenting-cells-in-a-spheroid-in-3d-using-2d-stardist-within-trackmate">Segmenting cells in a spheroid in 3D using 2D StarDist within TrackMate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#single-cell-approach-dissecting-agr-quorum-sensing-dynamics-in-staphylococcus-aureus">Single-cell approach dissecting agr quorum sensing dynamics in Staphylococcus aureus</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#slides-about-flute-a-python-gui-for-interactive-phasor-analysis-of-flim-data">Slides about FLUTE: a Python GUI for interactive phasor analysis of FLIM data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#so-geschlossen-wie-notig-so-offen-wie-moglich-datenschutz-beim-umgang-mit-forschungsdaten">So geschlossen wie nötig, so offen wie möglich - Datenschutz beim Umgang mit Forschungsdaten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stackview-sliceplot-example-data">Stackview sliceplot example data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-adipocyte-segmentation-training-data-training-notebook-and-model">StarDist Adipocyte Segmentation Training data, Training Notebook and Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-model-and-data-for-the-segmentation-of-yersinia-enterocolitica-cells-in-widefield-images">StarDist model and data for the segmentation of Yersinia enterocolitica cells in widefield images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-aspc1-lifeact">StarDist_AsPC1_Lifeact</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-monocytes-dataset">StarDist_BF_Monocytes_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-neutrophil-dataset">StarDist_BF_Neutrophil_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-cancer-cell-dataset-10x">StarDist_BF_cancer_cell_dataset_10x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-cancer-cell-dataset-20x">StarDist_BF_cancer_cell_dataset_20x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-fluorescent-cells">StarDist_Fluorescent_cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-huvec-nuclei-dataset">StarDist_HUVEC_nuclei_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-tumorcell-nuclei">StarDist_TumorCell_nuclei</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-model-and-training-dataset-for-automated-tracking-of-mda-mb-231-and-bt20-cells">Stardist model and training dataset for automated tracking of MDA-MB-231 and BT20 cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-miapaca2-from-cd44">Stardist_MiaPaCa2_from_CD44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structuring-of-data-and-metadata-in-bioimaging-concepts-and-technical-solutions-in-the-context-of-linked-data">Structuring of Data and Metadata in Bioimaging: Concepts and technical Solutions in the Context of Linked Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sustainable-data-stewardship">Sustainable Data Stewardship</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synapsenet-training-data">SynapseNet Training Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-service-for-research-data-management-and-knowledge-discovery-in-low-temperature-plasma-physics">Terminology service for research data management and knowledge discovery in low-temperature plasma physics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-dataset-for-whole-slide-image-registration">Test Dataset for Whole Slide Image Registration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-information-infrastructure-for-bioimage-data-i3d-bio-project-to-advance-fair-microscopy-data-management-for-the-community">The Information Infrastructure for BioImage Data (I3D:bio) project to advance FAIR microscopy data management for the community</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-role-of-helmholtz-centers-in-nfdi4bioimage-a-national-consortium-enhancing-fair-data-management-for-microscopy-and-bioimage-analysis">The role of Helmholtz Centers in NFDI4BIOIMAGE - A national consortium enhancing FAIR data management for microscopy and bioimage analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#thinking-data-management-on-different-scales">Thinking data management on different scales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#towards-preservation-of-life-science-data-with-nfdi4bioimage">Towards Preservation of Life Science Data with NFDI4BIOIMAGE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#towards-transparency-and-knowledge-exchange-in-ai-assisted-data-analysis-code-generation">Towards Transparency and Knowledge Exchange in AI-assisted Data Analysis Code Generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#towards-open-and-standardised-imaging-data-an-introduction-to-bio-formats-ome-tiff-and-ome-zarr">Towards open and standardised imaging data: an introduction to Bio-Formats, OME-TIFF, and OME-Zarr</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-trainer-concept-on-research-data-management">Train-the-Trainer Concept on Research Data Management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-computational-skills-in-the-age-of-ai">Training Computational Skills in the Age of AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-set-of-microscopy-images-for-dietler-et-al-nature-communications-2020">Training set of microscopy images for Dietler et al. Nature Communications 2020</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-language-models-for-bio-image-data-science">Vision Language Models for Bio-image Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset">Volumetric segmentation of biological cells and subcellular structures for optical diffraction tomography images - dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-not-to-do-when-creating-a-data-management-plan-dmp">WHAT NOT TO DO WHEN CREATING A DATA MANAGEMENT PLAN (DMP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#welcome-to-bioimage-town">Welcome to BioImage Town</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#who-you-gonna-call-data-stewards-to-the-rescue">Who you gonna call? - Data Stewards to the rescue</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-for-user-introduction-into-microscopy-omero-and-data-management-at-center-for-advanced-imaging">Workflow for user introduction into microscopy, OMERO and data management at Center for Advanced imaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#working-group-charter-rdm-helpdesk-network">Working Group Charter. RDM Helpdesk Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zeiss-axiozoom-stage-adapter">Zeiss AxioZoom Stage Adapter</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zeiss-axiozoom-stage-adapter-12-6well-plate">Zeiss AxioZoom Stage Adapter - 12/6Well Plate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zeiss-axiozoom-stage-adapter-em-block-holder">Zeiss AxioZoom Stage Adapter - EM block holder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zeiss-axiozoom-stage-adapter-microscope-slides">Zeiss AxioZoom Stage Adapter - Microscope slides</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zerocostdl4mic-stardist-2d-example-training-and-test-dataset-light">ZeroCostDL4Mic - Stardist 2D example training and test dataset (light)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zerocostdl4mic-stardist-example-training-and-test-dataset">ZeroCostDL4Mic - Stardist example training and test dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bina-cc-scalable-strategies-for-a-next-generation-of-fair-bioimaging">[BINA CC] Scalable strategies for a next-generation of FAIR bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cidas-scalable-strategies-for-a-next-generation-of-fair-bioimaging">[CIDAS] Scalable strategies for a next-generation of FAIR bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cmcb-scalable-strategies-for-a-next-generation-of-fair-bioimaging">[CMCB] Scalable strategies for a next-generation of FAIR bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cordi-2023-zarr-a-cloud-optimized-storage-for-interactive-access-of-large-arrays">[CORDI 2023] Zarr: A Cloud-Optimized Storage for Interactive Access of Large Arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#community-meeting-2024-overview-team-image-data-analysis-and-management">[Community Meeting 2024] Overview Team Image Data Analysis and Management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#community-meeting-2024-supporting-and-financing-rdm-projects-within-gerbi">[Community Meeting 2024] Supporting and financing RDM projects within GerBI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi-2024-ai-s-dirty-little-secret-without">[ELMI 2024]  AI’s Dirty Little Secret: Without</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi-2024-ai-s-dirty-little-secret-without-fair-data-it-s-just-fancy-math">[ELMI 2024] AI’s Dirty Little Secret: Without FAIR Data, It’s Just Fancy Math</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi2025-bridging-communities-with-ome-zarr">[ELMI2025] Bridging communities with OME-Zarr</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi2025-the-road-to-ome-zarr-1-0">[ELMI2025] The Road to OME-Zarr 1.0</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi2025-workshop-fair101-navigating-fair-data-from-principles-to-practice">[ELMI2025] Workshop: FAIR101 - Navigating FAIR data from principles to practice</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gbi-eoe-vii-five-or-ten-must-have-items-for-making-it-infrastructure-for-managing-bioimage-data">[GBI EOE VII] Five (or ten) must-have items for making IT infrastructure for managing bioimage data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gbi-eoe-ix-nfdi4bioimage">[GBI EoE IX] NFDI4BIOIMAGE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#i2k-scalable-strategies-for-a-next-generation-of-fair-bioimaging">[I2K] Scalable strategies for a next-generation of FAIR bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n4bi-ahm-welcome-to-bioimage-town">[N4BI AHM] Welcome to BioImage Town</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi-tech-talk-cloud-based-image-science">[NFDI Tech Talk] Cloud Based Image Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#swat4hcls-2023-nfdi4bioimage-perspective-for-a-national-bioimage-standard">[SWAT4HCLS 2023] NFDI4BIOIMAGE: Perspective for a national bioimage standard</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#short-talk-nfdi4bioimage-a-consortium-in-the-national-research-data-infrastructure">[Short Talk] NFDI4BIOIMAGE - A consortium in the National Research Data Infrastructure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webinar-a-journey-to-fair-bioimage-data">[Webinar] A journey to FAIR bioimage data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-material-fit-for-omero-how-imaging-facilities-and-it-departments-work-together-to-enable-rdm-for-bioimaging-october-16-17-2024-heidelberg">[Workshop Material] Fit for OMERO - How imaging facilities and IT departments work together to enable RDM for bioimaging, October 16-17, 2024, Heidelberg</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-bioimage-data-management-and-analysis-with-omero">[Workshop] Bioimage data management and analysis with OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-fair-data-handling-for-microscopy-structured-metadata-annotation-in-omero">[Workshop] FAIR data handling for microscopy: Structured metadata annotation in OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-managing-fair-microscopy-data-at-scale-for-universities-and-research-institutions-an-introduction-for-non-imaging-stakeholders">[Workshop] Managing FAIR microscopy data at scale for universities and research institutions: an introduction for non-imaging stakeholders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-research-data-management-for-microscopy-and-bioimage-analysis">[Workshop] Research Data Management for Microscopy and BioImage Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ilastik-interactive-machine-learning-for-bio-image-analysis">ilastik: interactive machine learning for (bio)image analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imaris-file-not-read-by-bfgetreader">imaris file not read by bfGetReader()</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#martinschatz-cz-scicount-v1-0-0-with-reusable-example-notebooks">martinschatz-cz/SciCount: v1.0.0 with reusable example notebooks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantixed-thedigitalcell-first-complete-code-set">quantixed/TheDigitalCell: First complete code set</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="zenodo-org-275">
<h1><a class="reference external" href="http://Zenodo.org">Zenodo.org</a> (275)<a class="headerlink" href="#zenodo-org-275" title="Link to this heading">#</a></h1>
<section id="zenodo-und-co-was-bringt-und-wer-braucht-ein-repositorium">
<h2>“ZENODO und Co.” Was bringt und wer braucht ein Repositorium?<a class="headerlink" href="#zenodo-und-co-was-bringt-und-wer-braucht-ein-repositorium" title="Link to this heading">#</a></h2>
<p>Elfi Hesse, Jan-Christoph Deinert, Christian Löschen</p>
<p>Published 2021-01-25</p>
<p>Licensed CC-BY-4.0</p>
<p>Die Online-Veranstaltung fand am 21.01.2021 im Rahmen der SaxFDM-Veranstaltungsreihe “Digital Kitchen - Küchengespräche mit SaxFDM” statt. SaxFDM-Sprecherin Elfi Hesse (HTW Dresden) erläuterte zunächst Grundsätzliches zum Thema Repositorien. Anschließend teilten Nutzer (Jan Deinert – HZDR) und Anbieter (Christian Löschen – TU Dresden/ZIH) lokaler Repositorien ihre Erfahrungen mit uns.</p>
<p>Tags: Research Data Management</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/4461261">https://zenodo.org/records/4461261</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4461261">https://doi.org/10.5281/zenodo.4461261</a></p>
</section>
<hr class="docutils" />
<section id="lif-files-misbehaving-in-fiji-but-fine-in-lasx">
<h2>.lif files misbehaving in fiji but fine in LASX<a class="headerlink" href="#lif-files-misbehaving-in-fiji-but-fine-in-lasx" title="Link to this heading">#</a></h2>
<p>Pamela Young</p>
<p>Published 2025-05-07</p>
<p>Licensed CC-BY-4.0</p>
<p>.lif files misbehaving in fiji but fine in LASX.  This data opens fine in LASX but FIJI only likes some of the files.  I think it was captured during a poweroutage so may have lived on a temp drive and been recovered when the power came back.  LASX uses the .lifext but I don’t think FIJI does.  I have included it however since it is part of the dataset output from the microscope.</p>
<p><a class="reference external" href="https://zenodo.org/records/15353569">https://zenodo.org/records/15353569</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15353569">https://doi.org/10.5281/zenodo.15353569</a></p>
</section>
<hr class="docutils" />
<section id="frames-of-fluorescent-particles">
<h2>10 frames of fluorescent particles<a class="headerlink" href="#frames-of-fluorescent-particles" title="Link to this heading">#</a></h2>
<p>Zach Marin, Maohan Su</p>
<p>Published 2024-12-05</p>
<p>Licensed CC-BY-4.0</p>
<p>10 frames of fluorescent particles. They don’t do much, but they are a DCIMG version 0x7 file example.</p>
<p><a class="reference external" href="https://zenodo.org/records/14281237">https://zenodo.org/records/14281237</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14281237">https://doi.org/10.5281/zenodo.14281237</a></p>
</section>
<hr class="docutils" />
<section id="d-nuclei-annotations-and-stardist-3d-model-s-rat-brain">
<h2>3D Nuclei annotations and StarDist 3D model(s) (rat brain)<a class="headerlink" href="#d-nuclei-annotations-and-stardist-3d-model-s-rat-brain" title="Link to this heading">#</a></h2>
<p>Romain Guiet</p>
<p>Published 2022-06-15</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: 3D Nuclei annotations and StarDist3D model(s) (rat brain)</p>
<p>Images:  From a large tiling acquisition ( <a class="reference external" href="https://doi.org/10.5281/zenodo.6646128">https://doi.org/10.5281/zenodo.6646128</a> ) individual Tile (xyz : 1024x1024x62) were downsampled and cropped (128x128x62). Four crops, from different tiles (./annotations_BIOP/images/) were manually annotated with ITK-SNAP (./annotations_BIOP/masks/)</p>
<p>These four images, and their corresponding masks, were cropped into four quadrants (./crops_BIOP_v1/) in order to get 16 different images (64x64x62).</p>
<p>Conda environment: A conda environment was created using the yml file  stardist0.8_TF1.15.yml</p>
<p>Training : Training was performed using the jupyter notebook 1-Training_notebook.ipynb.
Three different trainings (with the same random seed, same anisotropy, patch size and grid) were performed and produced three different models (./models/)</p>
<p>Validation images (from the random seed used) were exported to ease the visual inspection of the results(./val_rdm42/).</p>
<p>Validation:  To save metrics in a csv file and compare predictions to the annotations the jupyter notebook 2-QC_notebook.ipynb can be used on the validation folder.</p>
<p>Large images: To test the model on larger images one can use Whole_ds441.tif (or Crop_ds441.tif )
These images were obtained using the plugin BigSticher on the raw data ( <a class="reference external" href="https://doi.org/10.5281/zenodo.6646128">https://doi.org/10.5281/zenodo.6646128</a> ), resaved as h5 and exported the downsample by 4 version.</p>
<p> </p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/6645978">https://zenodo.org/records/6645978</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6645978">https://doi.org/10.5281/zenodo.6645978</a></p>
</section>
<hr class="docutils" />
<section id="d-nuclei-instance-segmentation-dataset-of-fluorescence-microscopy-volumes-of-c-elegans">
<h2>3D nuclei instance segmentation dataset of fluorescence microscopy volumes of C. elegans<a class="headerlink" href="#d-nuclei-instance-segmentation-dataset-of-fluorescence-microscopy-volumes-of-c-elegans" title="Link to this heading">#</a></h2>
<p>Fuhui Long, Hanchuan Peng, Xiao Liu, Stuart K Kim, Eugene Myers, Dagmar Kainmüller, Martin Weigert</p>
<p>Published 2022-02-01</p>
<p>Licensed CC-BY-4.0</p>
<p>The dataset consists of 28 confocal microscopy volumes of C. elegans worms at the L1 stage and  corresponding stacks of densely annotated nuclei instance segmentation masks.</p>
<ul class="simple">
<li><p>28 raw images and corresponding masks of average dimension (xyz) 1050 x 140 x 140</p></li>
<li><p>Pixelsize (xyz): 0.116 x 0.116 x 0.122μm</p></li>
<li><p>Microscope: Leica confocal microscopy, 63x oil objective</p></li>
</ul>
<p>The original raw data and preliminary annotations were  part of the following publication (please cite if you use the dataset):
 
Long, F., Peng, H., Liu, X., Kim, S. K., &amp; Myers, E. (2009). A 3D digital atlas of C. elegans and its application to single-cell analyses. Nature methods, 6(9), 667-672.</p>
<p>The nuclei annotation masks were further manually curated by Dagmar Kainmueller (MDC Berlin) for the following publication:</p>
<p>Hirsch, P., &amp; Kainmueller, D. (2020). An auxiliary task for learning nuclei segmentation in 3d microscopy images. In Medical Imaging with Deep Learning (pp. 304-321). PMLR.</p>
<p>We provide the dataset already structured into the train/validation/test split as used by the above as well as the following publications: </p>
<p>Weigert, M., Schmidt, U., Haase, R., Sugawara, K., &amp; Myers, G. (2020). Star-convex polyhedra for 3d object detection and segmentation in microscopy. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 3666-3673).
 </p>
<p> </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5942575">https://zenodo.org/records/5942575</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5942575">https://doi.org/10.5281/zenodo.5942575</a></p>
</section>
<hr class="docutils" />
<section id="steps-towards-reproducible-research">
<h2>6 Steps Towards Reproducible Research<a class="headerlink" href="#steps-towards-reproducible-research" title="Link to this heading">#</a></h2>
<p>Heidi Seibold</p>
<p>Licensed CC-BY-4.0</p>
<p>A short book with 6 steps that get you closer to making your work reproducible.</p>
<p>Tags: Reproducibility, Research Data Management</p>
<p>Content type: Book</p>
<p><a class="reference external" href="https://zenodo.org/records/12744715">https://zenodo.org/records/12744715</a></p>
</section>
<hr class="docutils" />
<section id="a-glimpse-of-the-open-source-flim-analysis-software-tools-flimfit-flute-and-napari-flim-phasor-plotter">
<h2>A Glimpse of the Open-Source FLIM Analysis Software Tools FLIMfit, FLUTE and napari-flim-phasor-plotter<a class="headerlink" href="#a-glimpse-of-the-open-source-flim-analysis-software-tools-flimfit-flute-and-napari-flim-phasor-plotter" title="Link to this heading">#</a></h2>
<p>Anca Margineanu, Chiara Stringari, Marcelo Zoccoler, Cornelia Wetzker</p>
<p>Published 2024-03-27</p>
<p>Licensed CC-BY-4.0</p>
<p>The presentations introduce open-source software to read in, visualize and analyse fluorescence lifetime imaging microscopy (FLIM) raw data developed for life scientists. The slides were presented at German Bioimaging (GerBI) FLIM Workshop held February 26 to 29 2024 at the Biomedical Center of LMU München by Anca Margineanu, Chiara Stringari and Conni Wetzker. </p>
<p><a class="reference external" href="https://zenodo.org/records/10886750">https://zenodo.org/records/10886750</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10886750">https://doi.org/10.5281/zenodo.10886750</a></p>
</section>
<hr class="docutils" />
<section id="a-deep-learning-approach-to-quantify-auditory-hair-cells">
<h2>A deep learning approach to quantify auditory hair cells<a class="headerlink" href="#a-deep-learning-approach-to-quantify-auditory-hair-cells" title="Link to this heading">#</a></h2>
<p>Maurizio Cortada, Loïc Sauteur, Michael Lanz, Soledad Levano, Daniel Bodmer</p>
<p>Published 2021-03-09</p>
<p>Licensed CC-BY-4.0</p>
<p>StarDist 2D deep learning model and training dataset.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4590066">https://zenodo.org/records/4590066</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4590066">https://doi.org/10.5281/zenodo.4590066</a></p>
</section>
<hr class="docutils" />
<section id="a-journey-to-fair-microscopy-data">
<h2>A journey to FAIR microscopy data<a class="headerlink" href="#a-journey-to-fair-microscopy-data" title="Link to this heading">#</a></h2>
<p>Stefanie Weidtkamp-Peters, Janina Hanne, Christian Schmidt</p>
<p>Published 2023-05-03</p>
<p>Licensed CC-BY-4.0</p>
<p>Oral presentation, 32nd MoMAN “From Molecules to Man” Seminar, Ulm, online. Monday February 6th, 2023</p>
<p>Abstract:</p>
<p>Research data management is essential in nowadays research, and one of the big opportunities to accelerate collaborative and innovative scientific projects. To achieve this goal, all our data needs to be FAIR (findable, accessible, interoperable, reproducible). For data acquired on microscopes, however, a common ground for FAIR data sharing is still to be established. Plenty of work on file formats, data bases, and training needs to be performed to highlight the value of data sharing and exploit its potential for bioimaging data.</p>
<p>In this presentation, Stefanie Weidtkamp-Peters will introduce the challenges for bioimaging data management, and the necessary steps to achieve data FAIRification. German BioImaging - GMB e.V., together with other institutions, contributes to this endeavor. Janina Hanne will present how the network of imaging core facilities, research groups and industry partners is key to the German bioimaging community’s aligned collaboration toward FAIR bioimaging data. These activities have paved the way for two data management initiatives in Germany: I3D:bio (Information Infrastructure for BioImage Data) and NFDI4BIOIMAGE, a consortium of the National Research Data Infrastructure. Christian Schmidt will introduce the goals and measures of these initiatives to the benefit of imaging scientist’s work and everyday practice.  </p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/7890311">https://zenodo.org/records/7890311</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7890311">https://doi.org/10.5281/zenodo.7890311</a></p>
</section>
<hr class="docutils" />
<section id="abdominal-imaging-window-aiw-for-intravital-imaging">
<h2>Abdominal Imaging Window (AIW) for Intravital Imaging<a class="headerlink" href="#abdominal-imaging-window-aiw-for-intravital-imaging" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-11-15</p>
<p>Licensed CC-BY-4.0</p>
<p>This upload features a simple model for the creation (Manufacturing/Prototyping) of an abdominal imaging window (AIW) for use in mice intravital microscopy.
Manufacture in titanium for chronic implantation. Measures in mm.</p>
<p><a class="reference external" href="https://zenodo.org/records/14168603">https://zenodo.org/records/14168603</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14168603">https://doi.org/10.5281/zenodo.14168603</a></p>
</section>
<hr class="docutils" />
<section id="aberrated-bead-stack">
<h2>Aberrated Bead Stack<a class="headerlink" href="#aberrated-bead-stack" title="Link to this heading">#</a></h2>
<p>Zach Marin</p>
<p>Published 2024-12-03</p>
<p>Licensed CC-BY-4.0</p>
<p>Bead stack taken on lower path of a 4Pi without deformable mirror corrections. DCIMG examples, not for other purposes.</p>
<p><a class="reference external" href="https://zenodo.org/records/14268554">https://zenodo.org/records/14268554</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14268554">https://doi.org/10.5281/zenodo.14268554</a></p>
</section>
<hr class="docutils" />
<section id="accessible-interactive-spatial-omics-data-visualizations-with-vitessce-and-omero">
<h2>Accessible Interactive Spatial-Omics Data Visualizations with Vitessce and OMERO<a class="headerlink" href="#accessible-interactive-spatial-omics-data-visualizations-with-vitessce-and-omero" title="Link to this heading">#</a></h2>
<p>Bortolomeazzi Michele</p>
<p>Published 2025-06-30</p>
<p>Licensed CC-BY-4.0</p>
<p>OMERO is the most used research data management system (RDM) in the bioimaging domain, and has been adopted as a centralized RDM solution by several academic and research institutions. A main reason for this is the ability to directly view and annotate images from a web-based interface. However, this feature of OMERO is currently underpowered for the visualization of very large or multimodal datasets. These datasets, are becoming a more and more common foundation for biological and biomedical studies, due to the recent developments in imaging, and sequencing technologies which enabled their application to spatial-omics. In order to begin to provide this multimodal-data capability to OMERO, we developed omero-vitessce (<a class="github reference external" href="https://github.com/NFDI4BIOIMAGE/omero-vitessce/tree/main">NFDI4BIOIMAGE/omero-vitessce</a>), a new <a class="reference external" href="http://OMERO.web">OMERO.web</a> plugin for viewing data stored in OMERO with the Vitessce (<a class="reference external" href="http://vitessce.io/">http://vitessce.io/</a>) multimodal data viewer. omero-vitessce can be installed as an <a class="reference external" href="http://OMERO.web">OMERO.web</a> plugin with PiPy (<a class="reference external" href="https://pypi.org/project/omero-vitessce/">https://pypi.org/project/omero-vitessce/</a>), and allows users to set up interactive visualizations of their images of cells and tissues through interactive plots which are directly linked to the image. This enables the visual exploration of bioimage-analysis results and of multimodal data such as those generated through spatial-omics experiments. The data visualization is highly customizable and can be configured not only through custom configuration files, but also with the graphical interface provided by the plugin, thus making omero-vitessce a highly user-friendly solution for multimodal data viewing. most biological datasets. We plan to extend the interoperability of omero-vitessce with the OME-NGFF and SpatialData file formats to leverage the efficiency of these cloud optimized formats.</p>
<p><a class="reference external" href="https://zenodo.org/records/15771899">https://zenodo.org/records/15771899</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15771899">https://doi.org/10.5281/zenodo.15771899</a></p>
</section>
<hr class="docutils" />
<section id="advancing-fair-image-analysis-in-galaxy-tools-workflows-and-training">
<h2>Advancing FAIR Image Analysis in Galaxy: Tools, Workflows, and Training<a class="headerlink" href="#advancing-fair-image-analysis-in-galaxy-tools-workflows-and-training" title="Link to this heading">#</a></h2>
<p>Chiang Jurado, Diana, Riccardo Massei, Pavankumar Videm, Anup Kumar, Anne Fouilloux, Leonid Kostrykin, Beatriz Serrano-Solano, Björn Grüning</p>
<p>Published 2025-03-06</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://zenodo.org/records/14979253">https://zenodo.org/records/14979253</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14979253">https://doi.org/10.5281/zenodo.14979253</a></p>
</section>
<hr class="docutils" />
<section id="alles-meins-oder-urheberrechte-klaren-fur-forschungsdaten">
<h2>Alles meins – oder!? Urheberrechte klären für Forschungsdaten<a class="headerlink" href="#alles-meins-oder-urheberrechte-klaren-fur-forschungsdaten" title="Link to this heading">#</a></h2>
<p>Stephan Wünsche</p>
<p>Published 2024-06-04</p>
<p>Licensed CC-BY-4.0</p>
<p>Wem gehören Forschungsdaten? Diese Frage stellt sich bei Daten, an deren Entstehung mehrere Personen beteiligt waren, und besonders bei Textdaten, Bildern und Videos. Hier lernen Sie, für Ihr eigenes Forschungsvorhaben zu erkennen, wessen Urheber- und Leistungsschutzrechte zu berücksichtigen sind. Sie erfahren, wie Sie mit Hilfe von Vereinbarungen frühzeitig Rechtssicherheit herstellen, etwa um Daten weitergeben oder publizieren zu können.
 
 </p>
<p>Tags: Research Data Management, Licensing</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/11472148">https://zenodo.org/records/11472148</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11472148">https://doi.org/10.5281/zenodo.11472148</a></p>
</section>
<hr class="docutils" />
<section id="an-annotated-high-content-fluorescence-microscopy-dataset-with-hoechst-33342-stained-nuclei-and-manually-labelled-outlines">
<h2>An annotated high-content fluorescence microscopy dataset with Hoechst 33342-stained nuclei and manually labelled outlines<a class="headerlink" href="#an-annotated-high-content-fluorescence-microscopy-dataset-with-hoechst-33342-stained-nuclei-and-manually-labelled-outlines" title="Link to this heading">#</a></h2>
<p>Malou Arvidsson, Salma Kazemi Rashed, Sonja Aits</p>
<p>Published 2022-06-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Here we present a benchmarking dataset of fluorescence microscopy images with Hoechst 33342-stained nuclei together with annotations of nuclei, nuclear fragments and micronuclei. Images were randomly selected from an RNA interference screen with a modified U2OS osteosarcoma cell line, acquired on a Thermo Fischer CX7 high-content imaging system at 20x magnification. Labelling was performed by a single annotator and reviewed by a biomedical expert.</p>
<p>The dataset contains 50 images showing over 2000 labelled nuclear objects in total, which is sufficiently large to train well-performing neural networks for instance or semantic segmentation. It is pre-split into training, development and test set, each in a zip file. The dataset should be referred to as Aitslab_bioimaging1. A brief article describing the dataset is also available (Arvidsson M, Kazemi Rashed S, Aits S. 10.1016/j.dib.2022.108769 )</p>
<p>Dataset description:</p>
<p>Fluorescence microscopy images: original .C01 files and files converted to 8-bit .png format (Grayscale)</p>
<p>Annotations: 24-bit .png format (RGB)</p>
<p>Script used to convert C01 to png images: C01_to_png.py file with python code and <a class="reference external" href="http://readme.md">readme.md</a> file with instructions to run it</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/6657260">https://zenodo.org/records/6657260</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6657260">https://doi.org/10.5281/zenodo.6657260</a></p>
</section>
<hr class="docutils" />
<section id="andor-dragonfly-confocal-image-of-bpae-cells-stained-for-actin-ims-file-format">
<h2>Andor Dragonfly confocal image of BPAE cells stained for actin, IMS file format<a class="headerlink" href="#andor-dragonfly-confocal-image-of-bpae-cells-stained-for-actin-ims-file-format" title="Link to this heading">#</a></h2>
<p>Hoku West-Foyle</p>
<p>Published 2025-01-16</p>
<p>Licensed CC0-1.0</p>
<p><a class="reference external" href="https://zenodo.org/records/14675120">https://zenodo.org/records/14675120</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14675120">https://doi.org/10.5281/zenodo.14675120</a></p>
</section>
<hr class="docutils" />
<section id="angebote-der-nfdi-fur-die-forschung-im-bereich-zoologie">
<h2>Angebote der NFDI für die Forschung im Bereich Zoologie<a class="headerlink" href="#angebote-der-nfdi-fur-die-forschung-im-bereich-zoologie" title="Link to this heading">#</a></h2>
<p>Birgitta König-Ries, Robert Haase, Daniel Nüst, Konrad Förstner, Judith Sophie Engel</p>
<p>Published 2024-12-04</p>
<p>Licensed CC-BY-4.0</p>
<p>In diesem Slidedeck geben wir einen Einblick in Angebote und Dienste der Nationalen Forschungsdaten Infrastruktur (NFDI), die Relevant für die Zoologie und angrenzende Disziplinen relevant sein könnten.</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/14278058">https://zenodo.org/records/14278058</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14278058">https://doi.org/10.5281/zenodo.14278058</a></p>
</section>
<hr class="docutils" />
<section id="artificial-blobs-and-labels-image">
<h2>Artificial Blobs and Labels image<a class="headerlink" href="#artificial-blobs-and-labels-image" title="Link to this heading">#</a></h2>
<p>Romain</p>
<p>Published 2023-05-10</p>
<p>Licensed CC-BY-4.0</p>
<p>A groovy script to use in Fiji to generate artificial images and labels, with example images.</p>
<p><a class="reference external" href="https://zenodo.org/records/7919117">https://zenodo.org/records/7919117</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7919117">https://doi.org/10.5281/zenodo.7919117</a></p>
</section>
<hr class="docutils" />
<section id="astigmatic-4pi-bead-stack">
<h2>Astigmatic 4Pi bead stack<a class="headerlink" href="#astigmatic-4pi-bead-stack" title="Link to this heading">#</a></h2>
<p>Zach Marin, Maohan Su</p>
<p>Published 2024-12-06</p>
<p>Licensed CC-BY-4.0</p>
<p>Bead stack taken on a 4Pi. DCIMG 0x1000000 file with a 4-pixel correction requirement.</p>
<p><a class="reference external" href="https://zenodo.org/records/14287640">https://zenodo.org/records/14287640</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14287640">https://doi.org/10.5281/zenodo.14287640</a></p>
</section>
<hr class="docutils" />
<section id="automatic-labelling-of-hela-kyoto-cells-using-deep-learning-tools">
<h2>Automatic labelling of HeLa “Kyoto” cells using Deep Learning tools<a class="headerlink" href="#automatic-labelling-of-hela-kyoto-cells-using-deep-learning-tools" title="Link to this heading">#</a></h2>
<p>Romain Guiet</p>
<p>Published 2022-02-25</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: Automatic labelling of HeLa “Kyoto” cells using Deep Learning tools</p>
<p>Data type: Microscopy images from the dataset “HeLa “Kyoto” cells under the scope”, Brightfield (BF), Digital Phase Contrast (DPC, either “raw” or “square-rooted”), Tubulin and H2B fluorescent channel, paired with their corresponding nuclei or cell/cyto label images.</p>
<p>Labels images: Labels images were generated using the script “prepare_trainingDataset_cellpose.ijm”.</p>
<p>Briefly, for 5 defined time-points (1,10,50,100,150), channels of interest were duplicated, resaved and :</p>
<p>-        nuclei label images were obtained using StarDist on H2B channel</p>
<p>-        cell label images were obtained using Cellpose on Tubulin and H2B channels</p>
<p>A quick visual inspection of the resulting label images concluded that they were satisfying enough, despite certainly not being perfect.</p>
<p>Notes :</p>
<p>-       This labelling strategy:</p>
<p>o   will not produce 100% accurate labels, but they might be more reproducible than labels generated by humans and are (definitely) much faster to obtain.</p>
<p>o   is NOT a recommended way of generating labels images, but for educational purposes.</p>
<p>-       The fluorescent channels are part of the dataset to ease the process of review of the labels and are NOT used for training. We generated the labels from the fluorescent channels to later predict labels from the BF or DPC channels only. As such, the fluorescent channels should not be “reused” with our labels during training.</p>
<p>File format: .tif (16-bit)</p>
<p>Image size: 540x540 (Pixel size: 0.299 nm)</p>
<p> </p>
<p>NOTE: This dataset uses the “HeLa “Kyoto” cells under the scope”  dataset (<a class="reference external" href="https://doi.org/10.5281/zenodo.6139958">https://doi.org/10.5281/zenodo.6139958</a>) to automatically generate annotations</p>
<p>NOTE: This dataset was used to train cellpose models in the following Zenodo entry <a class="reference external" href="https://doi.org/10.5281/zenodo.6140111">https://doi.org/10.5281/zenodo.6140111</a></p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/6140064">https://zenodo.org/records/6140064</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6140064">https://doi.org/10.5281/zenodo.6140064</a></p>
</section>
<hr class="docutils" />
<section id="axioscan-7-fluorescent-channels-not-displaying-in-qupath">
<h2>Axioscan 7 fluorescent channels not displaying in QuPath<a class="headerlink" href="#axioscan-7-fluorescent-channels-not-displaying-in-qupath" title="Link to this heading">#</a></h2>
<p>j</p>
<p>Published 2024-06-25</p>
<p>Hi &#64;ome team,Please find the .czi file attached. When loaded into QuPath using BioFormats, the fluorescence channels populate the brightness/contrast panel but do not show up in the viewer. Re-exporting as OME.Tiff from Zen and loading into QuPath does not help either - the channels do not populate the brightness/contrast panel in this case, and it shows as a RGB image.Please let me know if any further info is needed from me to troubleshoot!
Best,J</p>
<p><a class="reference external" href="https://zenodo.org/records/12533989">https://zenodo.org/records/12533989</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12533989">https://doi.org/10.5281/zenodo.12533989</a></p>
</section>
<hr class="docutils" />
<section id="beads-imaged-over-time">
<h2>Beads imaged over time<a class="headerlink" href="#beads-imaged-over-time" title="Link to this heading">#</a></h2>
<p>Zach Marin</p>
<p>Published 2025-04-04</p>
<p>Licensed CC-BY-4.0</p>
<p>DCIMG 0x1000000 images of beads over time (30 seconds, 0.03 s exposure). </p>
<p><a class="reference external" href="https://zenodo.org/records/15150937">https://zenodo.org/records/15150937</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15150937">https://doi.org/10.5281/zenodo.15150937</a></p>
</section>
<hr class="docutils" />
<section id="bio-image-data-strudel-for-workshop-on-research-data-management-in-tu-dresden-core-facilities">
<h2>Bio-Image Data Strudel for Workshop on Research Data Management in TU Dresden Core Facilities<a class="headerlink" href="#bio-image-data-strudel-for-workshop-on-research-data-management-in-tu-dresden-core-facilities" title="Link to this heading">#</a></h2>
<p>Cornelia Wetzker</p>
<p>Published 2023-11-08</p>
<p>Licensed CC-BY-4.0</p>
<p>This presentation gives a short outline of the complexity of data and metadata in the bioimaging universe. It introduces NFDI4BIOIMAGE as a newly formed consortium as part of the German ‘Nationale Forschungsdateninfrastruktur’ (NFDI) and its goals and tools for data management including its current members on TU Dresden campus.  </p>
<p>Tags: Research Data Management, Nfdi4Bioimage</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/10083555">https://zenodo.org/records/10083555</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10083555">https://doi.org/10.5281/zenodo.10083555</a></p>
</section>
<hr class="docutils" />
<section id="bio-image-analysis-code-generation">
<h2>Bio-image Analysis Code Generation<a class="headerlink" href="#bio-image-analysis-code-generation" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-10-28</p>
<p>Licensed CC-BY-4.0</p>
<p>Large Language Models are changing the way we interact with computers, especially how we write code. In this tutorial, we will generate bio-image analysis code using two LLM-based code generators, bia-bob and git-bob.
<a class="github reference external" href="https://github.com/haesleinhuepf/bia-bob">haesleinhuepf/bia-bob</a>
<a class="github reference external" href="https://github.com/haesleinhuepf/git-bob">haesleinhuepf/git-bob</a>
 </p>
<p><a class="reference external" href="https://zenodo.org/records/14001044">https://zenodo.org/records/14001044</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14001044">https://doi.org/10.5281/zenodo.14001044</a></p>
</section>
<hr class="docutils" />
<section id="bio-image-analysis-code-generation-using-bia-bob">
<h2>Bio-image Analysis Code Generation using bia-bob<a class="headerlink" href="#bio-image-analysis-code-generation-using-bia-bob" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-10-09</p>
<p>Licensed CC-BY-4.0</p>
<p>In this presentation I introduce bia-bob, an AI-based code generator that integrates into Jupyter Lab and allows for easy generation of Bio-Image Analysis Python code. It highlights how to get started with using large language models and prompt engineering to get high-quality bio-image analysis code.</p>
<p>Tags: Artificial Intelligence, Bioimage Analysis</p>
<p><a class="reference external" href="https://zenodo.org/records/13908108">https://zenodo.org/records/13908108</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13908108">https://doi.org/10.5281/zenodo.13908108</a></p>
</section>
<hr class="docutils" />
<section id="bio-image-analysis-with-the-help-of-large-language-models">
<h2>Bio-image Analysis with the Help of Large Language Models<a class="headerlink" href="#bio-image-analysis-with-the-help-of-large-language-models" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-03-13</p>
<p>Licensed CC-BY-4.0</p>
<p>Large Language Models (LLMs) change the way how we use computers. This also has impact on the bio-image analysis community. We can generate code that analyzes biomedical image data if we ask the right prompts. This talk outlines introduces basic principles, explains prompt engineering and how to apply it to bio-image analysis. We also compare how different LLM vendors perform on code generation tasks and which challenges are ahead for the bio-image analysis community.</p>
<p>Tags: Artificial Intelligence, Python</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/10815329">https://zenodo.org/records/10815329</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10815329">https://doi.org/10.5281/zenodo.10815329</a></p>
</section>
<hr class="docutils" />
<section id="bio-image-data-science-lectures-2025-uni-leipzig-scads-ai">
<h2>Bio-image Data Science Lectures 2025 &#64; Uni Leipzig / <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a><a class="headerlink" href="#bio-image-data-science-lectures-2025-uni-leipzig-scads-ai" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2025-07-02</p>
<p>Licensed CC-BY-4.0</p>
<p>These are the PPTx training resources for Students at Uni Leipzig who want to dive into bio-image data science with Python. The material will develop here and in the corresponding github repository between April and July 2025.</p>
<p><a class="reference external" href="https://zenodo.org/records/15793536">https://zenodo.org/records/15793536</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15793536">https://doi.org/10.5281/zenodo.15793536</a></p>
</section>
<hr class="docutils" />
<section id="bio-image-data-science-lectures-uni-leipzig-scads-ai">
<h2>Bio-image Data Science Lectures &#64; Uni Leipzig / <a class="reference external" href="http://ScaDS.AI">ScaDS.AI</a><a class="headerlink" href="#bio-image-data-science-lectures-uni-leipzig-scads-ai" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Licensed CC-BY-4.0</p>
<p>These are the PPTx training resources for Students at Uni Leipzig who want to dive into bio-image data science with Python. The material developed here between April and July 2024.</p>
<p>Tags: Bioimage Analysis, Artificial Intelligence, Python</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/12623730">https://zenodo.org/records/12623730</a></p>
</section>
<hr class="docutils" />
<section id="bioimage-io-chatbot-globias-seminar">
<h2><a class="reference external" href="http://BioImage.IO">BioImage.IO</a> Chatbot, GloBIAS Seminar<a class="headerlink" href="#bioimage-io-chatbot-globias-seminar" title="Link to this heading">#</a></h2>
<p>Caterina Fuster-Barcelo</p>
<p>Published 2024-10-02</p>
<p>Licensed CC-BY-4.0</p>
<p>The dynamic field of bioimage analysis continually seeks innovative tools to democratize access to analysis tools and its documentation. The <a class="reference external" href="http://BioImage.IO">BioImage.IO</a> Chatbot, leveraging state-of-the-art AI technologies including Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), provides an interactive platform that significantly integrates the exploration and application of bioimage analysis tools and models. This seminar will introduce the <a class="reference external" href="http://BioImage.IO">BioImage.IO</a> Chatbot’s capabilities, focusing on how it facilitates access to advanced analysis tools and documentation, allows for the execution of complex models, and enables users to create customized extensions adjusted to specific research needs. In a live demo, attendees will see how to interact with the chatbot and all its assistants and capabilities. Join us to explore how the <a class="reference external" href="http://BioImage.IO">BioImage.IO</a> Chatbot ca transform your research by making sophisticated analysis more intuitive and accessible.</p>
<p><a class="reference external" href="https://zenodo.org/records/13880367">https://zenodo.org/records/13880367</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13880367">https://doi.org/10.5281/zenodo.13880367</a></p>
</section>
<hr class="docutils" />
<section id="breast-cancer-nuclei-images-for-dl-training-zerocostdl4mic-stardist-model">
<h2>Breast Cancer Nuclei images for DL Training + ZeroCostDL4Mic StarDist Model<a class="headerlink" href="#breast-cancer-nuclei-images-for-dl-training-zerocostdl4mic-stardist-model" title="Link to this heading">#</a></h2>
<p>Ofra Golani, Vishnu Mohan, Tamar Geiger</p>
<p>Published 2024-05-21</p>
<p>Licensed CC-BY-4.0</p>
<p>Training dataset:Paired microscopy images (fluorescence) and corresponding masks
Microscopy data type: Fluorescence microscopy and masks obtained via manual correction of automatic segmentation with pre-trained StarDist model (see <a class="github reference external" href="https://github.com/qupath/models/tree/main/stardist">qupath/models</a>) 
Cells were imaged using a 20x objective with a 1x camera adapter was used in conjunction with a pco.edge 4.2 4MP camera on Pannoramic SCAN 150 scanner.
Cell type: FFPE tissue sections were sliced from all cancer-containing paraffin blocks
File format: .tif (8-bit for fluorescence and 16-bit for the masks)
 
StarDist Model:The StarDist model was generated using the ZeroCostDL4Mic platform (Chamier et al., 2021). This custom StarDist model was trained for 100 epochs using 80 manually annotated paired images (image dimensions: (257, 257)) with a batch size of 2, an augmentation factor of 10 and a mae loss function. The StarDist “Versatile fluorescent nuclei” model was used as a training starting point. Key python packages used include TensorFlow (v 2.2.0), Keras (v 1.1.2), CSBdeep (v 0.7.2), NumPy (v 1.21.6), Cuda (v 11..1.105). The training was accelerated using a Tesla P100GPU.The model weights can be used in the ZeroCostDL4Mic StarDist 2D notebook or in the StarDist Fiji plugin. a QuPath-compatible model is also provided.
 
 </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/11235393">https://zenodo.org/records/11235393</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11235393">https://doi.org/10.5281/zenodo.11235393</a></p>
</section>
<hr class="docutils" />
<section id="building-fair-image-analysis-pipelines-for-high-content-screening-hcs-data-using-galaxy">
<h2>Building FAIR image analysis pipelines for high-content-screening (HCS) data using Galaxy<a class="headerlink" href="#building-fair-image-analysis-pipelines-for-high-content-screening-hcs-data-using-galaxy" title="Link to this heading">#</a></h2>
<p>Riccardo Massei, Matthias Bernt, Leonid Kostrykin, Jan Bumberger</p>
<p>Published 2024-05-14</p>
<p>Licensed MIT</p>
<p>Imaging plays a crucial role across various scientific disciplines, particularly in life sciences. However, image data often proves complex, and the volume of images requiring analysis is steadily increasing, especially in high-content screening (HCS) experiments involving cell lines or other organisms. Specifically, analysis pipelines must align to the FAIR principles, ensuring they are reusable and interchangeable across different domains</p>
<p><a class="reference external" href="https://zenodo.org/records/15047849">https://zenodo.org/records/15047849</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15047849">https://doi.org/10.5281/zenodo.15047849</a></p>
</section>
<hr class="docutils" />
<section id="building-a-fair-image-data-ecosystem-for-microscopy-communities">
<h2>Building a FAIR image data ecosystem for microscopy communities<a class="headerlink" href="#building-a-fair-image-data-ecosystem-for-microscopy-communities" title="Link to this heading">#</a></h2>
<p>Isabel Kemmer, Antje Keppler, Beatriz Serrano-Solano, Arina Rybina, Bugra Özdemir, Johanna Bischof, Ayoub El Ghadraoui, John E. Eriksson, Aastha Mathur</p>
<p>Published 2023-03-31</p>
<p>Licensed CC-BY-4.0</p>
<p>Bioimaging has now entered the era of big data with faster than ever development of complex microscopy technologies leading to increasingly complex datasets. This enormous increase in data size and informational complexity within those datasets has brought with it several difficulties in terms of common and harmonized data handling, analysis and management practices, which are currently hampering the full potential of image data being realized. Here we outline a wide range of efforts and solutions currently being developed by the microscopy community to address these challenges on the path towards FAIR bioimage data. We also highlight how different actors in the microscopy ecosystem are working together, creating synergies that develop new approaches, and how research infrastructures, such as Euro-BioImaging, are fostering these interactions to shape the field. </p>
<p><a class="reference external" href="https://zenodo.org/records/7788899">https://zenodo.org/records/7788899</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7788899">https://doi.org/10.5281/zenodo.7788899</a></p>
</section>
<hr class="docutils" />
<section id="building-a-national-research-data-infrastructure">
<h2>Building a National Research Data Infrastructure<a class="headerlink" href="#building-a-national-research-data-infrastructure" title="Link to this heading">#</a></h2>
<p>for Microscopy and BioImage Analysis</p>
<p>Josh Moore</p>
<p>Published 2025-06-30</p>
<p>Licensed CC-BY-4.0</p>
<p>Presentation for the BioImagingUK Meeting taking place from 1200 - 1500 BST on Monday 30 June 2025 at mmc2025 <a class="reference external" href="https://www.mmc-series.org.uk/meetings-features/bioimaginguk-meeting.html">https://www.mmc-series.org.uk/meetings-features/bioimaginguk-meeting.html</a>
This pre-congress meeting provides an opportunity for the UK Bioimaging community to discuss priorities and strategies in national infrastructure, technology development, training, careers and ways to share knowledge across different disciplines. The session will consist of short talks from members of the BioImagingUK community and industrial/institute collaboration partners to update on progress, new opportunities and initiatives. There will be interactive Q+A sessions to encourage discussion and enable emerging priorities and ideas to be highlighted.</p>
<p><a class="reference external" href="https://zenodo.org/records/15756866">https://zenodo.org/records/15756866</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15756866">https://doi.org/10.5281/zenodo.15756866</a></p>
</section>
<hr class="docutils" />
<section id="czi-carl-zeiss-image-dataset-with-artificial-test-camera-images-with-various-dimension-for-testing-libraries-reading">
<h2>CZI (Carl Zeiss Image) dataset with artificial test camera images with various dimension for testing libraries reading<a class="headerlink" href="#czi-carl-zeiss-image-dataset-with-artificial-test-camera-images-with-various-dimension-for-testing-libraries-reading" title="Link to this heading">#</a></h2>
<p>Sebastian Rhode</p>
<p>Published 2022-08-22</p>
<p>Licensed CC-BY-4.0</p>
<p>Set of CZI test images created by using a simulated microscope with a test grayscale camera (no LSM or AiryScan or RGB). The filename indicates the used dimension(s) for the acquisition experiment. The files can be used to test the basic functionality of libraries reading CZI files.</p>
<p>Examples:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>S=2_T=3_CH=1.czi = 2 Scenes, 3 TimePoints and 1 Channel

	Z-Stack was not activated inside acquisition experiment


S=2_T=3_Z=5_CH=2.czi = 2 Scenes, 3 TimePoints, 5-Z-Planes and 1 Channels

	Z-Stack was activated inside acquisition experiment
</pre></div>
</div>
<p>The test files (so far) contain not any data with more “advanced” dimensions like AiryScan rawdata, illumination angles etc. Also no CZI files with pixel type RGB are included yet.</p>
<p> </p>
<p> </p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/7015307">https://zenodo.org/records/7015307</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7015307">https://doi.org/10.5281/zenodo.7015307</a></p>
</section>
<hr class="docutils" />
<section id="czi-file-examples">
<h2>CZI file examples<a class="headerlink" href="#czi-file-examples" title="Link to this heading">#</a></h2>
<p>Nicolas Chiaruttini</p>
<p>Published 2023-08-18</p>
<p>Licensed CC-BY-4.0</p>
<p>A set of public CZI files. These can be used for testing CZI readers.</p>
<ul class="simple">
<li><p>Demo LISH 4x8 15pct 647.czi: A cleared mouse brain acquired with a Zeiss LightSheet Z1 with 32 tiles. Courtesy of the Carl Petersen lab LSENS (<a class="reference external" href="https://www.epfl.ch/labs/lsens">https://www.epfl.ch/labs/lsens</a>). Sampled prepared by Yanqi Liu an imaged by Olivier Burri.</p></li>
<li><p>test_gray.czi: a synthetically generated CZI file without metadata, made by Sebastian Rhode</p></li>
<li><p>Image_1_2023_08_18__14_32_31_964.czi: an example multi-part CZI file, containing only camera noise</p></li>
<li><p>a xt scan, xz scan, xzt scan</p></li>
<li><p>a set of multi angle, multi illumination, mutli tile acquisition, taken on the LightSheet Z1 microscope of the PTBIOP by Lorenzo Talà</p></li>
</ul>
<p><a class="reference external" href="https://zenodo.org/records/8305531">https://zenodo.org/records/8305531</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8305531">https://doi.org/10.5281/zenodo.8305531</a></p>
</section>
<hr class="docutils" />
<section id="czi-open-science-program-collection">
<h2>CZI: Open Science Program Collection<a class="headerlink" href="#czi-open-science-program-collection" title="Link to this heading">#</a></h2>
<p>Content type: Collection</p>
<p><a class="reference external" href="https://zenodo.org/communities/eoss">https://zenodo.org/communities/eoss</a></p>
</section>
<hr class="docutils" />
<section id="cellbindb-a-large-scale-multimodal-annotated-dataset">
<h2>CellBinDB: A Large-Scale Multimodal Annotated Dataset<a class="headerlink" href="#cellbindb-a-large-scale-multimodal-annotated-dataset" title="Link to this heading">#</a></h2>
<p>Can Shi, Jinghong Fan, Zhonghan Deng, Huanlin Liu, Qiang Kang, Yumei Li, Jing Guo, Jingwen Wang, Jinjiang Gong, Sha Liao, Ao Chen, Ying Zhang, Mei Li</p>
<p>Published 2024-11-20</p>
<p>Licensed CC-ZERO</p>
<p>CellBinDB is a large-scale, multimodal annotated dataset for cell segmentation. It contains 1,044 annotated microscope images and 109,083 cell annotations, covering four staining types: DAPI, ssDNA, H&amp;E, and mIF. CellBinDB contains samples from two species, human and mouse, covering more than 30 histologically different tissue types, including disease-related tissues. The images in CellBinDB come from two sources: 844 mouse images from internal experiments and 200 human images from the open access platform 10x Genomics. We annotated all images in CellBinDB and provide two types of image annotations: semantic and instance masks. A xlsx file is attached to record the detailed information of each image.
In addition, we provide the images and annotations of nine other widely used publicly available cell segmentation datasets downloaded from their original sources, retaining their original formats for ease of use. 
The file ‘mixed_licenses.txt’ contains the original accessions of the public datasets used in our project and their associated licenses. Please refer to these links for more information about each dataset and its licensing terms, and use it according to the specifications.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/15370205">https://zenodo.org/records/15370205</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15370205">https://doi.org/10.5281/zenodo.15370205</a></p>
</section>
<hr class="docutils" />
<section id="cellpose-model-for-digital-phase-contrast-images">
<h2>Cellpose model for Digital Phase Contrast images<a class="headerlink" href="#cellpose-model-for-digital-phase-contrast-images" title="Link to this heading">#</a></h2>
<p>Laura Capolupo, Olivier Burri, Romain Guiet</p>
<p>Published 2022-02-09</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: Cellpose model for Digital Phase Contrast images</p>
<p>Data type: Cellpose model, trained via transfer learning from ‘cyto’ model.</p>
<p>Training Dataset: Light microscopy (Digital Phase Contrast) and Manual annotations (10.5281/zenodo.5996883)</p>
<p>Training Procedure: Model was trained using a Cellpose version 0.6.5 with GPU support (NVIDIA GeForce RTX 2080) using default settings as per the Cellpose documentation </p>
<p>python -m cellpose –train –dir TRAINING/DATASET/PATH/train –test_dir TRAINING/DATASET/PATH/test –pretrained_model cyto –chan 0 –chan2 0</p>
<p>The model file (MODEL NAME) in this repository is the result of this training.</p>
<p>Prediction Procedure: Using this model, a label image can be obtained from new unseen images in a given folder with</p>
<p>python -m cellpose –dir NEW/DATASET/PATH –pretrained_model FULL_MODEL_PATH –chan 0 –chan2 0 –save_tif –no_npy</p>
<p><a class="reference external" href="https://zenodo.org/records/6023317">https://zenodo.org/records/6023317</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6023317">https://doi.org/10.5281/zenodo.6023317</a></p>
</section>
<hr class="docutils" />
<section id="cellpose-models-for-label-prediction-from-brightfield-and-digital-phase-contrast-images">
<h2>Cellpose models for Label Prediction from Brightfield and Digital Phase Contrast images<a class="headerlink" href="#cellpose-models-for-label-prediction-from-brightfield-and-digital-phase-contrast-images" title="Link to this heading">#</a></h2>
<p>Romain Guiet, Olivier Burri</p>
<p>Published 2022-02-25</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: Cellpose models for Brightfield and Digital Phase Contrast images</p>
<p>Data type: Cellpose models trained via transfer learning from the ‘nuclei’ and ‘cyto2’ pretrained model with additional Training Dataset . Includes corresponding csv files with ‘Quality Control’ metrics(§) (model.zip).</p>
<p>Training Dataset: Light microscopy (Digital Phase Contrast or Brightfield) and automatic annotations (nuclei or cyto) (<a class="reference external" href="https://doi.org/10.5281/zenodo.6140064">https://doi.org/10.5281/zenodo.6140064</a>)</p>
<p>Training Procedure: The cellpose models were trained using cellpose version 1.0.0 with GPU support (NVIDIA GeForce K40) using default settings as per the Cellpose documentation . Training was done using a Renku environment (renku template).</p>
<p> </p>
<p>Command Line Execution for the different trained models</p>
<p>nuclei_from_bf:</p>
<p>cellpose –train –dir ‘data/train/’ –test_dir ‘data/test/’ –pretrained_model nuclei  –img_filter _bf –mask_filter _nuclei –chan 0 –chan2 0 –use_gpu –verbose</p>
<p>cyto_from_bf:</p>
<p>cellpose –train –dir ‘data/train/’ –test_dir ‘data/test/’ –pretrained_model cyto2 –img_filter _bf –mask_filter _cyto –chan 0 –chan2 0 –use_gpu –verbose</p>
<p> </p>
<p>nuclei_from_dpc:</p>
<p>cellpose –train –dir ‘data/train/’ –test_dir ‘data/test/’ –pretrained_model nuclei  –img_filter _dpc –mask_filter _nuclei –chan 0 –chan2 0 –use_gpu –verbose</p>
<p>cyto_from_dpc:</p>
<p>cellpose –train –dir ‘data/train/’ –test_dir ‘data/test/’ –pretrained_model cyto2 –img_filter _dpc –mask_filter _cyto –chan 0 –chan2 0 –use_gpu –verbose</p>
<p> </p>
<p>nuclei_from_sqrdpc:</p>
<p>cellpose –train –dir ‘data/train/’ –test_dir ‘data/test/’ –pretrained_model nuclei –img_filter _sqrdpc –mask_filter _nuclei –chan 0 –chan2 0 –use_gpu –verbose</p>
<p>cyto_from_sqrdpc:</p>
<p>cellpose –train –dir ‘data/train/’ –test_dir ‘data/test/’ –pretrained_model cyto2 –img_filter _sqrdpc –mask_filter _cyto –chan 0 –chan2 0 –use_gpu –verbose</p>
<p> </p>
<p>NOTE (§): We provide a notebook for Quality Control, which is an adaptation of the “Cellpose (2D and 3D)” notebook from ZeroCostDL4Mic .</p>
<p>NOTE: This dataset used a training dataset from the Zenodo entry(<a class="reference external" href="https://doi.org/10.5281/zenodo.6140064">https://doi.org/10.5281/zenodo.6140064</a>) generated from the “HeLa “Kyoto” cells under the scope”  dataset Zenodo entry(<a class="reference external" href="https://doi.org/10.5281/zenodo.6139958">https://doi.org/10.5281/zenodo.6139958</a>) in order to automatically generate the label images.</p>
<p>NOTE: Make sure that you delete the “_flow” images that are auto-computed when running the training. If you do not, then the flows from previous runs will be used for the new training, which might yield confusing results.</p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/6140111">https://zenodo.org/records/6140111</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6140111">https://doi.org/10.5281/zenodo.6140111</a></p>
</section>
<hr class="docutils" />
<section id="cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments">
<h2>Cellpose training data and scripts from “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”<a class="headerlink" href="#cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments" title="Link to this heading">#</a></h2>
<p>Martin Wohlwend, Olivier Burri, Johan Auwerx</p>
<p>Published 2024-02-27</p>
<p>Licensed CC-BY-4.0</p>
<p>This Workflow contains all the material necessary to reproduce the results of the QuPath analysis performed in the paper
 “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”
Inside this workflow and dataset, you will find the following folders</p>
<p>QuPath Training Project: A QuPath 0.3.2 project containing all the manual annotations (ground truths) used to train the cellpose model, as well as the script to start the training
QuPath Demo Project: A QuPath 0.3.2 project containing an example image that can be segmented using cellpose, followed by the classification of the CD45 expressing fibers
Training Images and Demo Images: The raw whole slide scanner 20x images needed by the above QuPath projects
Model: The fodler contianing the trained cellpose model
Cellpose Training Folder: The exported raw and ground truth images that the above cellpose model was trained on
Scripts: The QuPath scripts, also located in their respective QuPath projects, that were created for this whole workflow
QC: A Jupyter notebook, based on ZeroCostDL4Mic that computes quality metrics in order to assess the performance of the trained cellpose model. The folder also contains the resulting metrics.</p>
<p>Installation and Use
If you are going to use the QuPath projects, you need a local QuPath Installation <a class="reference external" href="https://qupath.github.io/">https://qupath.github.io/</a> that is configured to run the QuPath Cellpose Extension <a class="github reference external" href="https://github.com/BIOP/qupath-extension-cellpose">BIOP/qupath-extension-cellpose</a> as well as a working Cellpose installation <a class="github reference external" href="https://github.com/MouseLand/cellpose">MouseLand/cellpose</a>
Instructions for installation are available from the links above.
After that, you should be able to open the QuPath project, navigate to the “Automate &gt; Project scripts” menu and locate the script you wish to run.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7041137">https://zenodo.org/records/7041137</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7041137">https://doi.org/10.5281/zenodo.7041137</a></p>
</section>
<hr class="docutils" />
<section id="cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers">
<h2>Cellpose training data and scripts from “Machine learning for histological annotation and quantification of cortical layers”<a class="headerlink" href="#cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers" title="Link to this heading">#</a></h2>
<p>Jean Jacquemier, Julie Meystre, Olivier Burri</p>
<p>Published 2024-07-04</p>
<p>Licensed CC-BY-4.0</p>
<p>This Workflow contains all the material necessary to reproduce the cells detection, thanks to the QuPath performed in the paper
 “Machine learning for histological annotation and quantification of cortical layers”
Inside this workflow and dataset, you will find the following folders</p>
<p>QuPath Training Project: A QuPath 0.5.0 project containing all the manual annotations (ground truths) used to train the cellpose model, as well as the script to start the training
Training Images and Demo Images: The raw whole slide scanner images needed by the above QuPath project
Model: The fodler containing the trained cellpose model
cellpose-training Folder: The exported raw and ground truth images that the above cellpose model was trained on
Scripts: The QuPath scripts, also located in their respective QuPath projects, that were created for this whole workflow
QC: A Jupyter notebook, based on ZeroCostDL4Mic that computes quality metrics in order to assess the performance of the trained cellpose model. The folder also contains the resulting metrics.</p>
<p>Installation and Use
If you are going to use the QuPath projects, you need a local QuPath Installation <a class="reference external" href="https://qupath.github.io/">https://qupath.github.io/</a> that is configured to run the QuPath Cellpose Extension <a class="github reference external" href="https://github.com/BIOP/qupath-extension-cellpose">BIOP/qupath-extension-cellpose</a> as well as a working Cellpose installation <a class="github reference external" href="https://github.com/MouseLand/cellpose">MouseLand/cellpose</a>
Instructions for installation are available from the links above.
After that, you should be able to open the QuPath project, navigate to the “Automate &gt; Project scripts” menu and locate the script you wish to run.</p>
<ol class="arabic simple">
<li><p>train a cell segmentation algorithm in the context of the rat brain Layer Boundaries project </p></li>
<li><p>trigger cell segmentation from a QuPath project in a semi-automated pipeline</p></li>
</ol>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/12656468">https://zenodo.org/records/12656468</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12656468">https://doi.org/10.5281/zenodo.12656468</a></p>
</section>
<hr class="docutils" />
<section id="chatgpt-for-image-analysis">
<h2>ChatGPT for Image Analysis<a class="headerlink" href="#chatgpt-for-image-analysis" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-08-25</p>
<p>Licensed CC-BY-4.0</p>
<p>Large Language Models (LLMs) such as ChatGPT are changing the way we interact with computers, including how we analye microscopy imaging data. In this talk I introduce basic concepts of asking LLMs to write code and how to modify the questions to get the best out of it. For trying out these prompt engineering basics there are additional online resources available: <a class="reference external" href="https://scads.github.io/prompt-engineering-basics-2024/intro.html">https://scads.github.io/prompt-engineering-basics-2024/intro.html</a></p>
<p><a class="reference external" href="https://zenodo.org/records/13371196">https://zenodo.org/records/13371196</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13371196">https://doi.org/10.5281/zenodo.13371196</a></p>
</section>
<hr class="docutils" />
<section id="collaborative-working-and-version-control-with-git-hub">
<h2>Collaborative Working and Version Control with git[hub]<a class="headerlink" href="#collaborative-working-and-version-control-with-git-hub" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-01-10</p>
<p>Licensed CC-BY-4.0</p>
<p>This slide deck introduces the version control tool git, related terminology and the Github Desktop app for managing files in Git[hub] repositories. We furthermore dive into:* Working with repositories* Collaborative with others* Github-Zenodo integration* Github pages* Artificial Intelligence answering Github Issues</p>
<p>Tags: Nfdi4Bioimage, Globias, Research Data Management, Research Software Management</p>
<p><a class="reference external" href="https://zenodo.org/records/14626054">https://zenodo.org/records/14626054</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14626054">https://doi.org/10.5281/zenodo.14626054</a></p>
</section>
<hr class="docutils" />
<section id="id1">
<h2>Collaborative working and  Version Control with Git[Hub]<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2025-05-10</p>
<p>Licensed CC-BY-4.0</p>
<p>Working together on the internet presents us with new challenges: Who uploaded a file and when? Who contributed to the project when and why? How can content be merged when multiple team members make changes at the same time? The version control tool Git offers a comprehensive solution to these questions. The online platform <a class="reference external" href="http://GitHub.com">GitHub.com</a> provides a Git-driven platform that enables effective collaboration. Attendees of this hands-on tutorial will learn:</p>
<p>Introduction to version control with Git[Hub]</p>
<p>Working with Git: Pull requests</p>
<p>Resolving merge conflicts</p>
<p>Artificial intelligence that can respond to GitHub issues</p>
<p><a class="reference external" href="https://zenodo.org/records/15379632">https://zenodo.org/records/15379632</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15379632">https://doi.org/10.5281/zenodo.15379632</a></p>
</section>
<hr class="docutils" />
<section id="combining-stardist-and-trackmate-example-1-breast-cancer-cell-dataset">
<h2>Combining StarDist and TrackMate example 1 -  Breast cancer cell dataset<a class="headerlink" href="#combining-stardist-and-trackmate-example-1-breast-cancer-cell-dataset" title="Link to this heading">#</a></h2>
<p>Guillaume Jacquemet</p>
<p>Published 2020-09-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Description: Contains a StarDist example training dataset, a test dataset, and the StarDist model generated using ZeroCostDL4Mic (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Training dataset: 72 Paired microscopy images (fluorescence) and corresponding masks</p>
<p>Microscopy data type: Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Microscope: Spinning disk confocal microscope with a 20x 0.8 NA objective</p>
<p>Cell type: <a class="reference external" href="http://DCIS.COM">DCIS.COM</a> Lifeact-RFP cells</p>
<p>File format: .tif (16-bit for fluorescence and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 634 nm)</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4034976">https://zenodo.org/records/4034976</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4034976">https://doi.org/10.5281/zenodo.4034976</a></p>
</section>
<hr class="docutils" />
<section id="combining-stardist-and-trackmate-example-2-t-cell-dataset">
<h2>Combining StarDist and TrackMate example 2 -  T cell dataset<a class="headerlink" href="#combining-stardist-and-trackmate-example-2-t-cell-dataset" title="Link to this heading">#</a></h2>
<p>Nathan H. Roy, Guillaume Jacquemet</p>
<p>Published 2020-09-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Description: Contains a StarDist example training dataset, a test dataset, and the StarDist model generated using ZeroCostDL4Mic (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Training dataset: 209 Paired microscopy images (brightfield) and corresponding masks</p>
<p>Microscopy data type: brightfield microscopy and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Microscope: Imaging was done using a 10x phase contrast objective at 37°C on a Zeiss Axiovert 200M microscope equipped with an automated X-Y stage and a Roper EMCCD camera. Time-lapse images were collected every 30 sec for 10 min using SlideBook 6 software (Intelligent Imaging Innovations).</p>
<p>File format: .tif (16-bit for brightfield images and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 645 nm)</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4034929">https://zenodo.org/records/4034929</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4034929">https://doi.org/10.5281/zenodo.4034929</a></p>
</section>
<hr class="docutils" />
<section id="combining-stardist-and-trackmate-example-3-flow-chamber-dataset">
<h2>Combining StarDist and TrackMate example 3 -  Flow chamber dataset<a class="headerlink" href="#combining-stardist-and-trackmate-example-3-flow-chamber-dataset" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Guillaume Jacquemet</p>
<p>Published 2020-09-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Description: Contains a StarDist example training dataset, a test dataset, and the StarDist model generated using ZeroCostDL4Mic (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Training dataset: Paired microscopy images (brightfield) and corresponding masks</p>
<p>Microscopy data type: brightfield microscopy and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Microscope: Images were acquired with a brightfield microscope (Zeiss Laser-TIRF 3 Imaging System, Carl Zeiss) and a 10X objective.</p>
<p>File format: .tif (8-bit for brightfield images and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 650 nm)</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4034939">https://zenodo.org/records/4034939</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4034939">https://doi.org/10.5281/zenodo.4034939</a></p>
</section>
<hr class="docutils" />
<section id="combining-the-bids-and-arc-directory-structures-for-multimodal-research-data-organization">
<h2>Combining the BIDS and ARC Directory Structures for Multimodal Research Data Organization<a class="headerlink" href="#combining-the-bids-and-arc-directory-structures-for-multimodal-research-data-organization" title="Link to this heading">#</a></h2>
<p>Torsten Stöter, Tobias Gottschall, Andrea Schrager, Peter Zentis, Monica Valencia-Schneider, Niraj Kandpal, Werner Zuschratter, Astrid Schauss, Timo Dickscheid, Timo Mühlhaus, Dirk von Suchodoletz</p>
<p>Published 2023-09-12</p>
<p>Licensed CC-BY-4.0</p>
<p>Interdisciplinary collaboration and integration of large and diverse datasets are becoming increasingly important. Answering complex research questions requires combining and analysing multimodal datasets. Research data management follows the FAIR principles making data findable, accessible, interoperable, and reusable. However, there are challenges in capturing the entire research cycle and contextualizing data according, not only for the DataPLANT and NFDI4BIOIMAGE communities. To address these challenges, DataPLANT developed a data structure called Annotated Research Context (ARC). The Brain Imaging Data Structure (BIDS) originated from the neuroimaging community extended for microscopic image data. Both concepts provide standardised and file system based data storage structures for organising and sharing research data accompanied with metadata. We exemplarily compare the ARC and BIDS designs and propose structural and metadata mapping.</p>
<p>Tags: Research Data Management</p>
<p>Content type: Poster</p>
<p><a class="reference external" href="https://zenodo.org/records/8349563">https://zenodo.org/records/8349563</a></p>
</section>
<hr class="docutils" />
<section id="conference-slides-4th-day-of-intravital-microscopy">
<h2>Conference Slides - 4th Day of Intravital Microscopy<a class="headerlink" href="#conference-slides-4th-day-of-intravital-microscopy" title="Link to this heading">#</a></h2>
<p>Dr. Hellen Ishikawa-Ankerhold</p>
<p>Published 2024-11-13</p>
<p>Licensed CC-BY-4.0</p>
<p>Conference Slides for the presentation of GerBI e.V. at the 4th Day of Intravital Microscopy in Leuven, Belgium.
Features Structure, activities and Links to join GerBI e.V.</p>
<p><a class="reference external" href="https://zenodo.org/records/14113714">https://zenodo.org/records/14113714</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14113714">https://doi.org/10.5281/zenodo.14113714</a></p>
</section>
<hr class="docutils" />
<section id="crashkurs-forschungsdatenmanagement">
<h2>Crashkurs Forschungsdatenmanagement<a class="headerlink" href="#crashkurs-forschungsdatenmanagement" title="Link to this heading">#</a></h2>
<p>Barbara Weiner, Stephan Wünsche, Stefan Kühne, Pia Voigt, Sebastian Frericks, Clemens Hoffmann, Romy Elze, Ronny Gey</p>
<p>Published 2020-04-30</p>
<p>Licensed CC-BY-4.0</p>
<p>Diese Präsentation bietet einen Einstieg in alle relevanten Bereiche des Forschungsdatenmanagements an der Universität Leipzig. Behandelt werden Grundlagen des Forschungsdatenmanagements, technische, ethische und rechtliche Aspekte sowie die Archivierung und Publikation von Forschungsdaten. Die Präsentation enthält zahlreiche weiterführende Links (rot) und Literaturhinweise.</p>
<p>Ergänzend hierzu wird eine Präsentation mit Übungsaufgaben angeboten, die helfen soll, das Gelernte zu festigen und in der eigenen Forschungspraxis umzusetzen. Den Aufgaben folgen jeweils eine Antwortfolie sowie deren Auflösung.</p>
<p>Tags: Research Data Management</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/3778431">https://zenodo.org/records/3778431</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.3778431">https://doi.org/10.5281/zenodo.3778431</a></p>
</section>
<hr class="docutils" />
<section id="creating-open-computational-curricula">
<h2>Creating open computational curricula<a class="headerlink" href="#creating-open-computational-curricula" title="Link to this heading">#</a></h2>
<p>Kari Jordan, Zhian Kamvar, Toby Hodges</p>
<p>Published 2020-12-11</p>
<p>Licensed CC-BY-4.0</p>
<p>In this interactive session, Carpentries team members will guide attendees through three stages of the backward design process to create a lesson development plan for the open source tool of their choosing. Attendees will leave having identified what practical skills they aim to teach (learning objectives), an approach for designing challenge questions (formative assessment), and mechanisms to give and receive feedback.</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/4317149">https://zenodo.org/records/4317149</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4317149">https://doi.org/10.5281/zenodo.4317149</a></p>
</section>
<hr class="docutils" />
<section id="cultivating-open-training">
<h2>Cultivating Open Training<a class="headerlink" href="#cultivating-open-training" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-03-14</p>
<p>Licensed CC-BY-4.0</p>
<p>In this SaxFDM Digital Kitchen, I introduced current challenges and potential solutions for openly sharing training materials, softly focusing on bio-image analysis. In this field a lot of training materials circulate in private channels, but openly shared, reusable materials, according to the FAIR-principles, are still rare. Using the CC-BY license and uploading materials to publicly acessible repositories are proposed to fill this gap.</p>
<p>Tags: Open Science, Research Data Management, FAIR-Principles, Bioimage Analysis, Licensing</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/10816895">https://zenodo.org/records/10816895</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10816895">https://doi.org/10.5281/zenodo.10816895</a></p>
</section>
<hr class="docutils" />
<section id="cultivating-open-training-to-advance-bio-image-analysis">
<h2>Cultivating Open Training to advance Bio-image Analysis<a class="headerlink" href="#cultivating-open-training-to-advance-bio-image-analysis" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-04-25</p>
<p>Licensed CC-BY-4.0</p>
<p>These slides introduce current challenges and potential solutions for openly sharing training materials, focusing on bio-image analysis. In this field a lot of training materials circulate in private channels, but openly shared, reusable materials, according to the FAIR-principles, are still rare. Using the CC-BY license and publicly acessible repositories are proposed to fill this gap.</p>
<p>Tags: Research Data Management, Licensing, FAIR-Principles</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/11066250">https://zenodo.org/records/11066250</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11066250">https://doi.org/10.5281/zenodo.11066250</a></p>
</section>
<hr class="docutils" />
<section id="dalia-interchange-format">
<h2>DALIA Interchange Format<a class="headerlink" href="#dalia-interchange-format" title="Link to this heading">#</a></h2>
<p>Jonathan Geiger, Petra Steiner, Abdelmoneim Amer Desouki, Frank Lange</p>
<p>Published 2024-06-07</p>
<p>Licensed CC-BY-SA-4.0</p>
<p>The DALIA (Data Literacy Alliance) project aims to develop a knowledge graph for FAIR teaching and learning materials on data literacy, data competencies and research data management (RDM) skills within the National Research Data Infrastructure (NFDI) and the RDM landscape. Such a platform thrives on the participation of users who want to search, create, manage or use teaching and learning materials.
A schematization of the metadata is necessary for the interoperability of teaching and learning materials. This is done by the DALIA Interchange Format (DIF), which provides a framework for making the metadata of teaching and learning materials transparent, comparable and smooth to integrate into the DALIA platform. It includes the description and explanation of the data fields for the online publication of educational resources.
The DIF was developed in close consultation with the scientific community. This development process included several feedback rounds in which valuable feedback was provided and subsequently incorporated into the DIF. This not only contributed to the clear, transparent and user-oriented definitions of the data fields, and to a clear structure, but also to the integration of many existing data standards and to the (special) requirements of the scientific community. The selection of elements is based on the Dublin Core Application Profile.
The DIF is provided as a PDF document and in table form (ODS) to convey the attributes of the teaching and learning materials and their definitions in an easily understandable form and to facilitate communication. It also includes a legend and an example in tabular form. In addition, a template (CSV) with the attributes as column headers is provided, which can be used for recording the metadata of the teaching and learning materials. The tables can also be transferred to technical application profiles.
We would like to thank all the commentators of the previous versions, especially Susanne Arndt, Sophie Boße, Sonja Felder, Marc Fuhrmans, Jan-Michael Haugwitz, Marina Lemaire, Karoline Lemke, Birte Lindstädt, Juliane Röder, and Jakob Voß. Without their feedback and advice, the DIF would be less transparent.</p>
<p><a class="reference external" href="https://zenodo.org/records/11521029">https://zenodo.org/records/11521029</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11521029">https://doi.org/10.5281/zenodo.11521029</a></p>
</section>
<hr class="docutils" />
<section id="data-stewardship-and-research-data-management-tools-for-multimodal-linking-of-imaging-data-in-plasma-medicine">
<h2>Data stewardship and research data management tools for multimodal linking of imaging data in plasma medicine<a class="headerlink" href="#data-stewardship-and-research-data-management-tools-for-multimodal-linking-of-imaging-data-in-plasma-medicine" title="Link to this heading">#</a></h2>
<p>Mohsen Ahmadi, Robert Wagner, Philipp Mattern, Nick Plathe, Sander Bekeschus, Markus M. Becker, Torsten Stöter, Stefanie Weidtkamp-Peters</p>
<p>Published 2023-11-03</p>
<p>Licensed CC-BY-4.0</p>
<p>A more detailed understanding of the effect of plasmas on biological systems can be fostered by combining data from different imaging modalities, such as optical imaging, fluorescence imaging, and mass spectrometry imaging. This, however, requires the implementation and use of sophisticated research data management (RDM) solutions to incorporate the influence of plasma parameters and treatment procedures as well as the effects of plasma on the treated targets. In order to address this, RDM activities on different levels and from different perspectives are started and brought together within the framework of the NFDI consortium NFDI4BIOIMAGE.</p>
<p><a class="reference external" href="https://zenodo.org/records/10069368">https://zenodo.org/records/10069368</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10069368">https://doi.org/10.5281/zenodo.10069368</a></p>
</section>
<hr class="docutils" />
<section id="dataviz-protocols-an-introduction-to-data-visualization-protocols-for-wet-lab-scientists">
<h2>DataViz protocols - An introduction to data visualization protocols for wet lab scientists<a class="headerlink" href="#dataviz-protocols-an-introduction-to-data-visualization-protocols-for-wet-lab-scientists" title="Link to this heading">#</a></h2>
<p>Joachim Goedhart</p>
<p>Published 2024-12-10</p>
<p>Licensed CC-BY-NC-SA-4.0</p>
<p>Tags: Data Visualization, R</p>
<p>Content type: Book</p>
<p><a class="reference external" href="https://zenodo.org/records/7257808">https://zenodo.org/records/7257808</a></p>
<p><a class="reference external" href="https://joachimgoedhart.github.io/DataViz-protocols/">https://joachimgoedhart.github.io/DataViz-protocols/</a></p>
</section>
<hr class="docutils" />
<section id="dataset-from-incell-2200-microscope-misread-as-a-plate">
<h2>Dataset from InCell 2200 microscope misread as a plate<a class="headerlink" href="#dataset-from-incell-2200-microscope-misread-as-a-plate" title="Link to this heading">#</a></h2>
<p>Fabien Kuttler, Rémy Dornier</p>
<p>Published 2025-01-30</p>
<p>Licensed CC-BY-4.0</p>
<p>Two dummy datasets are provided in this repository : </p>
<p>Dataset_Ok : 96 wells, 9 fields of view per well, 4 different channels (DAPI, Cy3, FITC, Brightfield), no Z, no T. The .xcde file of this dataset is correctly read by BioFormats, as the dataset is recognized as a plate, and can be imported on OMERO
Dataset_fail: 20 wells, 4 fields of view per well, 5 channels, with one duplicate (DAPI, FITC, Cy3, Cy5 wix 4 , Cy5 wix 5), no Z, no T. The .xcde file of this dataset is not correctly read by BioFormats and no images are imported on OMERO.</p>
<p>BioFormats version: 8.0.1
A discussion thread has been open on this topic.</p>
<p><a class="reference external" href="https://zenodo.org/records/14769820">https://zenodo.org/records/14769820</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14769820">https://doi.org/10.5281/zenodo.14769820</a></p>
</section>
<hr class="docutils" />
<section id="datenmanagement">
<h2>Datenmanagement<a class="headerlink" href="#datenmanagement" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-04-14</p>
<p>Licensed CC-BY-4.0</p>
<p>In dieser Data Management Session wird der Lebenszyklus von Daten näher beleuchtet. Wie entstehen Daten, was passiert mit ihnen, wenn sie verarbeitet werden? Wem gehören die Daten und wer ist dafür verantwortlich, sie zu veröffentlichen, zu archivieren und gegebenenfalls wiederzuverwenden? Wir werden einen Datenmanagementplan in Gruppenarbeit entwerfen, ggf. mit Hilfe von ChatGPT.</p>
<p>Tags: Research Data Management</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/10970869">https://zenodo.org/records/10970869</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10970869">https://doi.org/10.5281/zenodo.10970869</a></p>
</section>
<hr class="docutils" />
<section id="datenmanagement-im-fokus-organisation-speicherstrategien-und-datenschutz">
<h2>Datenmanagement im Fokus: Organisation, Speicherstrategien und Datenschutz<a class="headerlink" href="#datenmanagement-im-fokus-organisation-speicherstrategien-und-datenschutz" title="Link to this heading">#</a></h2>
<p>Pia Voigt, Carolin Hundt</p>
<p>Published 2024-04-19</p>
<p>Licensed CC-BY-4.0</p>
<p>Workshop zum Thema „Datenmanagement im Fokus: Organisation, Speicherstrategien und Datenschutz“ auf der Data Week Leipzig
Der Umgang mit Daten ist im Alltag nicht immer leicht: Wie und wo speichert man Daten idealerweise? Welche Strategien helfen, den Überblick zu behalten und wie geht man mit personenbezogenen Daten um? Diese Fragen möchten wir gemeinsam mit Ihnen anhand individueller Datenprobleme besprechen und Ihnen Lösungen aufzeigen, wie Sie ihr Datenmanagement effizient gestalten können.</p>
<p>Tags: Research Data Management</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/11107798">https://zenodo.org/records/11107798</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11107798">https://doi.org/10.5281/zenodo.11107798</a></p>
</section>
<hr class="docutils" />
<section id="datenmanagementplane-erstellen-teil-1">
<h2>Datenmanagementpläne erstellen - Teil 1<a class="headerlink" href="#datenmanagementplane-erstellen-teil-1" title="Link to this heading">#</a></h2>
<p>Pia Voigt, Barbara Weiner</p>
<p>Published 2021-03-23</p>
<p>Licensed CC-BY-4.0</p>
<p>Was ist ein Datenmanagementplan? Welche Vorgaben sollte ich beachten? Wie erstelle ich einen solchen für mein Forschungsprojekt und welche nützlichen Tools kann ich hierfür verwenden?</p>
<p>Die Anforderungen der Forschungsförderer zum Datenmanagement steigen stetig. Damit verbunden ist häufig auch das Erstellen eines Datenmanagementplans. Dabei erwarten DFG, BMBF oder die EU jeweils unterschiedliche Angaben zur Erhebung, Speicherung und Veröffentlichung von projektbezogenen Forschungsdaten. Zudem bietet das Erstellen eines Datenmanagementplans viele Vorteile und hilft Ihnen nicht zuletzt, die Anforderungen der guten wissenschaftlichen Praxis strukturiert umzusetzen.</p>
<p>Was im ersten Moment unübersichtlich und überfordernd wirkt, soll in diesem Kurs anhand einer grundlegenden theoretischen Einführung im ersten und praxisorientierter Beispiele im zweiten Teil der Veranstaltung handhabbar gemacht werden. Sie lernen, was hinter den Anforderungen der Forschungsförderer steckt, welche Elemente ein Datenmanagementplan enthalten sollte und wie sie einen solchen mithilfe interaktiver Tools selbst erstellen können.</p>
<p>Tags: Research Data Management</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/4630788">https://zenodo.org/records/4630788</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4630788">https://doi.org/10.5281/zenodo.4630788</a></p>
</section>
<hr class="docutils" />
<section id="datenmanagementplane-erstellen-teil-2">
<h2>Datenmanagementpläne erstellen - Teil 2<a class="headerlink" href="#datenmanagementplane-erstellen-teil-2" title="Link to this heading">#</a></h2>
<p>Pia Voigt, Barbara Weiner</p>
<p>Published 2021-03-30</p>
<p>Licensed CC-BY-4.0</p>
<p>Was ist ein Datenmanagementplan? Welche Vorgaben sollte ich beachten? Wie erstelle ich einen solchen für mein Forschungsprojekt und welche nützlichen Tools kann ich hierfür verwenden?</p>
<p>Die Anforderungen der Forschungsförderer zum Datenmanagement steigen stetig. Damit verbunden ist häufig auch das Erstellen eines Datenmanagementplans. Dabei erwarten DFG, BMBF oder die EU jeweils unterschiedliche Angaben zur Erhebung, Speicherung und Veröffentlichung von projektbezogenen Forschungsdaten. Zudem bietet das Erstellen eines Datenmanagementplans viele Vorteile und hilft Ihnen nicht zuletzt, die Anforderungen der guten wissenschaftlichen Praxis strukturiert umzusetzen.</p>
<p>Was im ersten Moment unübersichtlich und überfordernd wirkt, soll in diesem Kurs anhand einer grundlegenden theoretischen Einführung im ersten und praxisorientierter Beispiele im zweiten Teil der Veranstaltung handhabbar gemacht werden. Sie lernen, was hinter den Anforderungen der Forschungsförderer steckt, welche Elemente ein Datenmanagementplan enthalten sollte und wie sie einen solchen mithilfe interaktiver Tools selbst erstellen können.</p>
<p>Version 2 enthält aktuelle Links und weiterführende Hinweise zu einzelnen Aspekten eines Datenmanagementplans.</p>
<p>Version 3 ist die überarbeitete und aktualisierte Version der ersten beiden und enthält u.a. Hinweise zur Lizenzierung und zu Nutzungsrechten an Forschungsdaten.</p>
<p>Tags: Research Data Management</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/4748534">https://zenodo.org/records/4748534</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4748534">https://doi.org/10.5281/zenodo.4748534</a></p>
</section>
<hr class="docutils" />
<section id="deconvolution-test-dataset">
<h2>Deconvolution Test Dataset<a class="headerlink" href="#deconvolution-test-dataset" title="Link to this heading">#</a></h2>
<p>Romain Guiet</p>
<p>Published 2021-07-14</p>
<p>Licensed CC-BY-4.0</p>
<p>This a test dataset, HeLa cells stained for action using Phalloidin-488 acquired on confocal Zeiss LSM710, which contains</p>
<ul class="simple">
<li><p>Ph488.czi (contains all raw metadata)</p></li>
<li><p>Raw_large.tif ( is the tif version of Ph488.czi, provided for conveninence as tif doesn’t need Bio-Formats to be open in Fiji )</p></li>
<li><p>Raw.tif , is a crop of the large image</p></li>
</ul>
<p>- PSFHuygens_confocal_Theopsf.tif , is a theoretical PSF generated with HuygensPro</p>
<p>- PSFgen_WF_WBpsf.tif  , is a theoretical PSF generated with PSF generator</p>
<ul class="simple">
<li><p>PSFgen_WFsquare_WBpsf.tif, is the result of the square operation on PSFgen_WF_WBpsf.tif , to approximate a confocal PSF</p></li>
</ul>
<p><a class="reference external" href="https://zenodo.org/records/5101351">https://zenodo.org/records/5101351</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5101351">https://doi.org/10.5281/zenodo.5101351</a></p>
</section>
<hr class="docutils" />
<section id="deep-learning-segmentation-projects-of-fib-sem-dataset-of-u2-os-cell">
<h2>Deep learning segmentation projects of FIB-SEM dataset of U2-OS cell<a class="headerlink" href="#deep-learning-segmentation-projects-of-fib-sem-dataset-of-u2-os-cell" title="Link to this heading">#</a></h2>
<p>Belevich Ilya, Eija Jokitalo</p>
<p>Published 2023-10-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This submission includes ground truth datasets that were used to segment the nuclear envelope (NE), mitochondria, endoplasmic reticulum (ER) and Golgi from a human bone osteosarcoma epithelial cell (U2-OS) imaged using focused-ion beam scanning electron microscopy (FIB-SEM).The full FIB-SEM dataset is deposited to EMPIAR (<a class="reference external" href="https://www.ebi.ac.uk/empiar">https://www.ebi.ac.uk/empiar</a>, EMPIAR-11746). </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10043461">https://zenodo.org/records/10043461</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10043461">https://doi.org/10.5281/zenodo.10043461</a></p>
</section>
<hr class="docutils" />
<section id="deep-learning-training-data-jove">
<h2>Deep learning training data (JOVE)<a class="headerlink" href="#deep-learning-training-data-jove" title="Link to this heading">#</a></h2>
<p>Jessica Heebner, Carson Purnell, Ryan Hylton, Mike Marsh, Michael Grillo, Matt Swulius</p>
<p>Published 2022-11-18</p>
<p>Licensed CC-ZERO</p>
<p>Cryo-electron tomography (cryo-ET) allows researchers to image cells in their native, hydrated state at the highest resolution currently possible. However, the technique has several limitations that make analyzing the data it generates time-intensive and difficult. Hand-segmenting a single tomogram can take hours to days of human effort, but the microscope can easily generate 50 or more tomograms a day. Current deep learning segmentation programs for cryo-ET do exist but are limited to segmenting one structure at a time. Here multi-slice U-Net convolutional neural networks are trained and applied to automatically segment multiple structures simultaneously within cryo-tomograms. With proper preprocessing, these networks can be robustly inferred to many tomograms without the need for training individual networks for each tomogram. This workflow dramatically improves the speed with which cryo-electron tomograms can be analyzed by cutting segmentation time down to under 30 min in most cases. Further, segmentations can be used to improve the accuracy of filament tracing within a cellular context and to rapidly extract coordinates for subtomogram averaging.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7335439">https://zenodo.org/records/7335439</a></p>
<p><a class="reference external" href="https://doi.org/10.5061/dryad.rxwdbrvct">https://doi.org/10.5061/dryad.rxwdbrvct</a></p>
</section>
<hr class="docutils" />
<section id="deepbacs-bacillus-subtilis-fluorescence-segmentation-dataset">
<h2>DeepBacs – Bacillus subtilis fluorescence segmentation dataset<a class="headerlink" href="#deepbacs-bacillus-subtilis-fluorescence-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Séamus Holden, Mia Conduit</p>
<p>Published 2021-10-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Training and test images of live B. subtilis cells expressing FtsZ-GFP for the task of segmentation.</p>
<p>Additional information can be found on this github wiki.</p>
<p>The example shows the fluorescence widefield image of live B. subtilis cells expressing FtsZ-GFP and the manually annotated segmentation mask.</p>
<p> </p>
<p>Data type: Paired fluorescence and segmented mask images</p>
<p>Microscopy data type: 2D widefield images (fluorescence) </p>
<p>Microscope: Custom-built 100x inverted microscope bearing a 100x TIRF objective (Nikon CFI Apochromat TIRF 100XC Oil); images were captured on a Prime BSI sCMOS camera (Teledyne Photometrics)</p>
<p>Cell type: B. subtilis strain SH130 grown under agarose pads</p>
<p>File format: .tiff (8-bit) or .png (8-bit)</p>
<p>For segmented masks, binary masks are used for training of CARE/U-Net models, 8-bit .tif ROI maps for training of StarDist models and .png images for training of pix2pix models</p>
<p>Image size: 1024 x 1024 px² (Pixel size: 65 nm)</p>
<p>Image preprocessing: Images were denoised using PureDenoise and resulting 32-bit images were converted into 8-bit images after normalizing to 1% and 99.98% percentiles. Images were manually annotated using the Labkit Fiji plugin</p>
<p> </p>
<p>Author(s): Mia Conduit1,2, Séamus Holden1,3</p>
<p>Contact email: <a class="reference external" href="mailto:Seamus&#46;Holden&#37;&#52;&#48;newcastle&#46;ac&#46;uk">Seamus<span>&#46;</span>Holden<span>&#64;</span>newcastle<span>&#46;</span>ac<span>&#46;</span>uk</a></p>
<p> </p>
<p>Affiliation:</p>
<ol class="arabic simple">
<li><p>Centre for Bacterial Cell Biology, Biosciences Institute, Newcastle University, NE2 4AX UK</p></li>
<li><p>ORCID: 0000-0002-7169-907X</p></li>
</ol>
<p> </p>
<p> Associated publications: Whitley et al., 2021, Nature Communications, <a class="reference external" href="https://doi.org/10.15252/embj.201696235">https://doi.org/10.15252/embj.201696235</a></p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5550968">https://zenodo.org/records/5550968</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5550968">https://doi.org/10.5281/zenodo.5550968</a></p>
</section>
<hr class="docutils" />
<section id="deepbacs-escherichia-coli-bright-field-segmentation-dataset">
<h2>DeepBacs – Escherichia coli bright field segmentation dataset<a class="headerlink" href="#deepbacs-escherichia-coli-bright-field-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Christoph Spahn, Mike Heilemann</p>
<p>Published 2021-10-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Training and test images of live E. coli cells imaged under bright field for the task of segmentation.</p>
<p>Additional information can be found on this github wiki.</p>
<p>The example shows a bright field image of live E. coli cells and the manually annotated segmentation mask.</p>
<p> </p>
<p>Data type: Paired bright field and segmented mask images </p>
<p>Microscopy data type: 2D bright field images recorded at 1 min interval</p>
<p>Microscope: Nikon Eclipse Ti-E equipped with an Apo TIRF 1.49NA 100x oil immersion objective</p>
<p>Cell type: E. coli MG1655 wild type strain (CGSC #6300).</p>
<p>File format: .tif (8-bit)</p>
<p>Image size: 1024 x 1024 px² (79 nm / pixel), 19/15 individual frames (training/test dataset)</p>
<p>1024 x 1024 px² (79 nm / pixel), 9 regions of interest with 80 frames &#64; 1 min time interval (live-cell time series)</p>
<p>Image preprocessing: Raw images were recorded in 16-bit mode (image size 512 x 512 px² &#64; 158 nm/px). Images were upscaled with a factor of 2 (no interpolation) to enable generation of higher-quality segmentation masks. Two sets of mask images are provided: RoiMaps for instance segmentation using e.g. StarDist or binary images for CARE or U-Net.</p>
<p>Author(s): Christoph Spahn1,2, Mike Heilemann1,3</p>
<p>Contact email: <a class="reference external" href="mailto:christoph&#46;spahn&#37;&#52;&#48;mpi-marburg&#46;mpg&#46;de">christoph<span>&#46;</span>spahn<span>&#64;</span>mpi-marburg<span>&#46;</span>mpg<span>&#46;</span>de</a></p>
<p> </p>
<p>Affiliation(s): </p>
<ol class="arabic simple">
<li><p>Institute of Physical and Theoretical Chemistry, Max-von-Laue Str. 7, Goethe-University Frankfurt, 60439 Frankfurt, Germany</p></li>
<li><p>ORCID: 0000-0001-9886-2263 </p></li>
<li><p>ORCID: 0000-0002-9821-3578</p></li>
</ol>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5550935">https://zenodo.org/records/5550935</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5550935">https://doi.org/10.5281/zenodo.5550935</a></p>
</section>
<hr class="docutils" />
<section id="deepbacs-mixed-segmentation-dataset-and-stardist-model">
<h2>DeepBacs – Mixed segmentation dataset and StarDist model<a class="headerlink" href="#deepbacs-mixed-segmentation-dataset-and-stardist-model" title="Link to this heading">#</a></h2>
<p>Christoph Spahn, Mike Heilemann, Séamus Holden, Mia Conduit, Pereira, Pedro Matos, Mariana Pinho</p>
<p>Published 2021-10-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Mixed training and test images of S. aureus, E. coli and B. subtilis for cell segmentation using StarDist, as well as the trained StarDist model.</p>
<p>Additional information can be found on this github wiki.</p>
<p> </p>
<p>Data type: Paired bright field / fluorescence and segmented mask images</p>
<p>Microscopy data type: 2D widefield images; DIC and fluorescence for S. aureus, bright field images for E. coli, and fluorescence images for B. subtilis</p>
<p>Microscopes: </p>
<p>S. aureus: </p>
<p>GE HealthCare Deltavision OMX system (with temperature and humidity control, 37°C) equipped with an Olympus 60x 1.42NA Oil immersion objective and 2 PCO Edge 5.5 sCMOS cameras (one for DIC, one for fluorescence)</p>
<p>E.coli:</p>
<p>Nikon Eclipse Ti-E equipped with an Apo TIRF 1.49NA 100x oil immersion objective</p>
<p>B. subtilis:</p>
<p>Custom-built 100x inverted microscope bearing a 100x TIRF objective (Nikon CFI Apochromat TIRF 100XC Oil); images were captured on a Prime BSI sCMOS camera (Teledyne Photometrics)</p>
<p> </p>
<p>Cell types: S. aureus strain JE2, E. coli MG1655 (CGSC #6300) and B. subtilis strain SH130; all grown under agarose pads</p>
<p>File format: .tif (8-bit and 16-bit)</p>
<p>Image size: 512 x 512 px² &#64; 80 nm pixel size (S. aureus); 1024 x 1024 px² &#64; 79 nm pixel size (E. coli); 1024 x 1024 px² &#64; 65 nm pixel size (B. subtilis)</p>
<p>Image preprocessing: </p>
<p>S. aureus:</p>
<p>Raw images were manually annotated by drawing ellipses in the NR fluorescence image and segmented images were created using the LOCI plugin (“ROI Map”). For training, images and masks were quartered into four 256 x 256 px² patches.</p>
<p>E. coli:</p>
<p>Raw images were recorded in 16-bit mode (image size 512x512 px² &#64; 158 nm/px). Images were upscaled with a factor of 2 (no interpolation) to enable generation of higher-quality segmentation masks.</p>
<p>B. subtilis:</p>
<p>Images were denoised using PureDenoise and resulting 32-bit images were converted into 8-bit images after normalizing to 1% and 99.98% percentiles. Images were manually annotated using the Labkit Fiji plugin</p>
<p> </p>
<p>StarDist model:</p>
<p>The StarDist 2D model was generated using the ZeroCostDL4Mic platform (Chamier et al., 2021). It was trained from scratch for 200 epochs (120 steps/epoch) on 155 paired image patches (image dimensions: (1024, 1024), patch size: (256,256)) with a batch size of 4, 10% validation data, 64 rays on grid 2, a learning rate of 0.0003 and a mae loss function, using the StarDist 2D ZeroCostDL4Mic notebook (v 1.12.2). Key python packages used include tensorflow (v 0.1.12), Keras (v 2.3.1), csbdeep (v 0.6.1), numpy (v 1.19.5), cuda (v 11.0.221). The training was accelerated using a Tesla P100GPU. The dataset was augmented by a factor of 3.</p>
<p> </p>
<p>The model weights can be used in the ZeroCostDL4Mic StarDist 2D notebook, the StarDist Fiji plugin or the TrackMate Fiji plugin (v7+).</p>
<p> </p>
<p>Author(s): Christoph Spahn1,2, Mike Heilemann1,3, Mia Conduit4, Séamus Holden4,5, Pedro Matos Pereira6,7, Mariana Pinho6,8</p>
<p>Contact email: <a class="reference external" href="mailto:christoph&#46;spahn&#37;&#52;&#48;mpi-marburg&#46;mpg&#46;de">christoph<span>&#46;</span>spahn<span>&#64;</span>mpi-marburg<span>&#46;</span>mpg<span>&#46;</span>de</a>, <a class="reference external" href="mailto:Seamus&#46;Holden&#37;&#52;&#48;newcastle&#46;ac&#46;uk">Seamus<span>&#46;</span>Holden<span>&#64;</span>newcastle<span>&#46;</span>ac<span>&#46;</span>uk</a>, <a class="reference external" href="mailto:pmatos&#37;&#52;&#48;itqb&#46;unl&#46;pt">pmatos<span>&#64;</span>itqb<span>&#46;</span>unl<span>&#46;</span>pt</a> and <a class="reference external" href="mailto:mgpinho&#37;&#52;&#48;itqb&#46;unl&#46;pt">mgpinho<span>&#64;</span>itqb<span>&#46;</span>unl<span>&#46;</span>pt</a></p>
<p> </p>
<p>Affiliation(s): </p>
<ol class="arabic simple">
<li><p>Institute of Physical and Theoretical Chemistry, Max-von-Laue Str. 7, Goethe-University Frankfurt, 60439 Frankfurt, Germany</p></li>
<li><p>ORCID: 0000-0001-9886-2263 </p></li>
<li><p>ORCID: 0000-0002-9821-3578</p></li>
<li><p>Centre for Bacterial Cell Biology, Biosciences Institute, Newcastle University, NE2 4AX UK</p></li>
<li><p>ORCID: 0000-0002-7169-907X</p></li>
<li><p>Bacterial Cell Biology, Instituto de Tecnologia Química e Biológica António Xavier, Universidade Nova de Lisboa, Oeiras, Portugal</p></li>
<li><p>ORCID: 0000-0002-1426-9540</p></li>
<li><p>ORCID: 0000-0002-7132-8842</p></li>
</ol>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5551009">https://zenodo.org/records/5551009</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5551009">https://doi.org/10.5281/zenodo.5551009</a></p>
</section>
<hr class="docutils" />
<section id="deepbacs-staphylococcus-aureus-widefield-segmentation-dataset">
<h2>DeepBacs – Staphylococcus aureus widefield segmentation dataset<a class="headerlink" href="#deepbacs-staphylococcus-aureus-widefield-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Pereira, Pedro Matos, Mariana Pinho</p>
<p>Published 2021-10-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Training and test images of live S. aureus cells for the task of cell segmentation.</p>
<p>Additional information can be found in the github wiki.</p>
<p>The example shows the bright field and Nile Red fluorescence image of live S. aureus cells, as well as the manually annotated segmentation mask.</p>
<p> </p>
<p>Data type: Paired DIC/fluorescence and segmented mask images</p>
<p>Microscopy data type: 2D widefield images (DIC and fluorescence)</p>
<p>Microscope:  GE HealthCare Deltavision OMX system (with temperature and humidity control, 37°C) equipped with an Olympus 60x 1.42NA Oil immersion objective and 2 PCO Edge 5.5 sCMOS cameras (one for DIC, one for fluorescence)</p>
<p>Cell type: S. aureus strain JE2 grown under agarose pads</p>
<p>File format: .tif (16-bit)</p>
<p>Image size: 512 x 512 px² (80 nm/px)
Image preprocessing: Raw images were manually annotated by drawing ellipses in the NR fluorescence image and segmented images were created using the LOCI plugin (“ROI Map”). For training, images and masks were quartered into four 256 x 256 px² patches.</p>
<p> </p>
<p>Author(s): Pedro Matos Pereira1,2, Mariana Pinho1,3</p>
<p>Contact email: <a class="reference external" href="mailto:pmatos&#37;&#52;&#48;itqb&#46;unl&#46;pt">pmatos<span>&#64;</span>itqb<span>&#46;</span>unl<span>&#46;</span>pt</a> and <a class="reference external" href="mailto:mgpinho&#37;&#52;&#48;itqb&#46;unl&#46;pt">mgpinho<span>&#64;</span>itqb<span>&#46;</span>unl<span>&#46;</span>pt</a></p>
<p> </p>
<p>Affiliation: </p>
<ol class="arabic simple">
<li><p>Bacterial Cell Biology, Instituto de Tecnologia Química e Biológica António Xavier, Universidade Nova de Lisboa, Oeiras, Portugal</p></li>
<li><p>ORCID: <a class="reference external" href="https://orcid.org/0000-0002-1426-9540">https://orcid.org/0000-0002-1426-9540</a></p></li>
<li><p>ORCID: <a class="reference external" href="https://orcid.org/0000-0002-7132-8842">https://orcid.org/0000-0002-7132-8842</a></p></li>
</ol>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5550933">https://zenodo.org/records/5550933</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5550933">https://doi.org/10.5281/zenodo.5550933</a></p>
</section>
<hr class="docutils" />
<section id="developing-a-training-strategy">
<h2>Developing a Training Strategy<a class="headerlink" href="#developing-a-training-strategy" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-11-08</p>
<p>Licensed CC-BY-4.0</p>
<p>When training people in topics such as programming, bio-image analysis or data science, it makes sense to define a training strategy with a wider perspective than just trainees needs. This slide deck gives insights into aspects to consider when defining a training strategy. It considers funder’s interests, financial aspects, metrics / goals, steps towards sustainability and opportunities for outreach and for founding future collaborations.</p>
<p><a class="reference external" href="https://zenodo.org/records/14053758">https://zenodo.org/records/14053758</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14053758">https://doi.org/10.5281/zenodo.14053758</a></p>
</section>
<hr class="docutils" />
<section id="development-of-a-platform-for-advanced-optics-education-training-and-prototyping">
<h2>Development of a platform for advanced optics education, training and prototyping<a class="headerlink" href="#development-of-a-platform-for-advanced-optics-education-training-and-prototyping" title="Link to this heading">#</a></h2>
<p>Nadine Utz, Sabine Reither, Ruth Hans, Christian Feldhaus</p>
<p>Published 2023-10-05</p>
<p>Licensed CC-BY-4.0</p>
<p>In bio-medical research we often need to combine a broad range of expertise to run complex experiments and analyse and interpret their results. Also, it is desirable that all stakeholders of a project understand all parts of the experiment and analysis to draw and support the right conclusions. For imaging experiments this usually requires a basic understanding of the underlying physics. This has not necessarily been part of the professional training of all stakeholders, e.g. biologists or data scientists. Therefore an affordable platform for easily demonstrating and explaining imaging principles would be desirable.
Building up on a commercially available STEM Optics kit we developed extensions with widely available and affordable components to demonstrate advanced imaging techniques like e.g. confocal, lightsheet, OPT, spectral imaging. All models are quick and easy to build, yet demonstrate the important physical principles each imaging technique is based on.
Further use cases for this kit are training courses, demonstrations for imaging newbies when designing an experiment and outreach activities but also basic level prototyping.</p>
<p><a class="reference external" href="https://zenodo.org/records/10925217">https://zenodo.org/records/10925217</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10925217">https://doi.org/10.5281/zenodo.10925217</a></p>
</section>
<hr class="docutils" />
<section id="digital-phase-contrast-on-primary-dermal-human-fibroblasts-cells">
<h2>Digital Phase Contrast on Primary Dermal Human Fibroblasts cells<a class="headerlink" href="#digital-phase-contrast-on-primary-dermal-human-fibroblasts-cells" title="Link to this heading">#</a></h2>
<p>Laura Capolupo</p>
<p>Published 2022-02-09</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: Digital Phase Contrast on Primary Dermal Human Fibroblasts cells </p>
<p>Data type: Paired microscopy images (Digital Phase Contrast, square rooted) and corresponding labels/masks used for cellpose training (the corresponding Brightfield images are also present), organized as recommended by cellpose documentation.</p>
<p>Microscopy data type: Light microscopy (Digital Phase Contrast and Brighfield )</p>
<p>Manual annotations: Labels/masks obtained via manual segmentation. For each region, all cells were annotated manually. Uncertain objects (Dust, fused cells) were left unannotated, so that the cellpose model (10.5281/zenodo.6023317) may mimic the same user bias during prediction. This was particularly necessary due to the accumulation of floating debris in the center of the well.</p>
<p>Microscope: Perkin Elmer Operetta microscope with a 10x 0.35 NA objective</p>
<p>Cell type: Primary Dermal Human Fibroblasts cells</p>
<p>File format: .tif (16-bit for DPC and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 634 nm)</p>
<p>NOTE : This dataset was used to train cellpose model ( 10.5281/zenodo.6023317 )</p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/5996883">https://zenodo.org/records/5996883</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5996883">https://doi.org/10.5281/zenodo.5996883</a></p>
</section>
<hr class="docutils" />
<section id="effect-of-local-topography-on-cell-division-of-staphylococci-sp">
<h2>Effect of local topography on cell division of Staphylococci sp.<a class="headerlink" href="#effect-of-local-topography-on-cell-division-of-staphylococci-sp" title="Link to this heading">#</a></h2>
<p>Sorzabal Bellido, Ioritz, Luca Barbieri, Beckett, Alison J., Prior, Ian A., Arturo Susarrey-Arce, Tiggelaar, Roald M., Jo Forthergill, Rasmita Raval, Diaz Fernandez, Yuri A.</p>
<p>Published 2021-05-16</p>
<p>Licensed CC-BY-4.0</p>
<p>Dataset.zip</p>
<p>This dataset includes the raw and annotated images used to train a Stardist 2D deep learning model for segmentation of surface attached S.aureus as described in Effect of local topography on cell division of Staphylococci sp.</p>
<p> </p>
<p>Stardist2d_Model.zip</p>
<p>Stardist 2D deep learning model for segmentation of surface attached S.aureus, obtained using the StarDist 2D ZeroCostDL4Mic notebook (v 1.12.3).</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4765599">https://zenodo.org/records/4765599</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4765599">https://doi.org/10.5281/zenodo.4765599</a></p>
</section>
<hr class="docutils" />
<section id="efficiently-starting-institutional-research-data-management">
<h2>Efficiently starting institutional research data management<a class="headerlink" href="#efficiently-starting-institutional-research-data-management" title="Link to this heading">#</a></h2>
<p>Katarzyna Biernacka, Katrin Cortez, Kerstin Helbig</p>
<p>Published 2019-10-15</p>
<p>Licensed CC-BY-4.0</p>
<p>Researchers are increasingly often confronted with research data management (RDM) topics during their work. Higher education institutions therefore begin to offer services for RDM at some point to give support and advice. However, many groundbreaking decisions have to be made at the very beginning of RDM services. Priorities must be set and policies formulated. Likewise, the staff must first be qualified in order to provide advice and adequately deal with the manifold problems awaiting.
The FDMentor project has therefore bundled the expertise of five German universities with different experiences and levels of RDM knowledge to jointly develop strategies, roadmaps, guidelines, and open access training material. Humboldt-Universität zu Berlin, Freie Universität Berlin, Technische Universität Berlin, University of Potsdam, and European University Viadrina Frankfurt (Oder) have worked together on common solutions that are easy to adapt. With funding of the German Federal Ministry of Education and Research, the collaborative project addressed four problem areas: strategy development, legal issues, policy development, and competence enhancement. The aim of the project outcomes is to provide other higher education institutions with the best possible support for the efficient introduction of research data management. Therefore, all project results are freely accessible under the CC-BY 4.0 international license. The early involvement of the community in the form of workshops and the collection of feedback has proven its worth: the FDMentor strategies, roadmaps, guidelines, and training materials are applied and adapted beyond the partner universities.</p>
<p>Tags: Research Data Management</p>
<p>Content type: Document</p>
<p><a class="reference external" href="https://zenodo.org/record/3490058">https://zenodo.org/record/3490058</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.3490058">https://doi.org/10.5281/zenodo.3490058</a></p>
</section>
<hr class="docutils" />
<section id="einblicke-ins-forschungsdatenmanagement-darf-ich-das-veroffentlichen-rechtsfragen-im-umgang-mit-forschungsdaten">
<h2>Einblicke ins Forschungsdatenmanagement - Darf ich das veröffentlichen? Rechtsfragen im Umgang mit Forschungsdaten<a class="headerlink" href="#einblicke-ins-forschungsdatenmanagement-darf-ich-das-veroffentlichen-rechtsfragen-im-umgang-mit-forschungsdaten" title="Link to this heading">#</a></h2>
<p>Stephan Wünsche, Pia Voigt</p>
<p>Published 2021-05-11</p>
<p>Licensed CC-BY-4.0</p>
<p>Diese Präsentation wurde im Zuge der digitalen Veranstaltungsreihe “Einblicke ins Forschungsdatenmanagement” erstellt. Diese findet seit dem SS 2020 an der Universität Leipzig für alle Interessierten zu verschiedenen Themen des Forschungsdatenmanagements statt.</p>
<p>Dieser Teil der Reihe dreht sich um Rechtsfragen im Umgang mit Forschungsdaten und deren Bedeutung für die wissenschaftliche Praxis. Sie finden in der vorliegenden Präsentation einen Überblick über relevante Rechtsbereiche sowie Erläuterungen zum Datenschutz, Urheberrecht und den Grundsätzen der guten wissenschaftlichen Praxis mit Fokus auf deren Bedeutung im Forschungsdatenmanagement.</p>
<p>Tags: Research Data Management, Data Protection</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/4748510">https://zenodo.org/records/4748510</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4748510">https://doi.org/10.5281/zenodo.4748510</a></p>
</section>
<hr class="docutils" />
<section id="engineering-a-software-environment-for-research-data-management-of-microscopy-image-data-in-a-core-facility">
<h2>Engineering a Software Environment for Research Data Management of Microscopy Image Data in a Core Facility<a class="headerlink" href="#engineering-a-software-environment-for-research-data-management-of-microscopy-image-data-in-a-core-facility" title="Link to this heading">#</a></h2>
<p>Kunis</p>
<p>Published 2022-05-30</p>
<p>This thesis deals with concepts and solutions in the field of data management in everyday scientific life for image data from microscopy. The focus of the formulated requirements has so far been on published data, which represent only a small subset of the data generated in the scientific process. More and more, everyday research data are moving into the focus of the principles for the management of research data that were formulated early on (FAIR-principles). The adequate management of this mostly multimodal data is a real challenge in terms of its heterogeneity and scope. There is a lack of standardised and established workflows and also the software solutions available so far do not adequately reflect the special requirements of this area. However, the success of any data management process depends heavily on the degree of integration into the daily work routine. Data management must, as far as possible, fit seamlessly into this process. Microscopy data in the scientific process is embedded in pre-processing, which consists of preparatory laboratory work and the analytical evaluation of the microscopy data. In terms of volume, the image data often form the largest part of data generated within this entire research process. In this paper, we focus on concepts and techniques related to the handling and description of this image data and address the necessary basics. The aim is to improve the embedding of the existing data management solution for image data (OMERO) into the everyday scientific work. For this purpose, two independent software extensions for OMERO were implemented within the framework of this thesis: OpenLink and MDEmic. OpenLink simplifies the access to the data stored in the integrated repository in order to feed them into established workflows for further evaluations and enables not only the internal but also the external exchange of data without weakening the advantages of the data repository. The focus of the second implemented software solution, MDEmic, is on the capturing of relevant metadata for microscopy. Through the extended metadata collection, a corresponding linking of the multimodal data by means of a unique description and the corresponding semantic background is aimed at. The configurability of MDEmic is designed to address the currently very dynamic development of underlying concepts and formats. The main goal of MDEmic is to minimise the workload and to automate processes. This provides the scientist with a tool to handle this complex and extensive task of metadata acquisition for microscopic data in a simple way. With the help of the software, semantic and syntactic standardisation can take place without the scientist having to deal with the technical concepts. The generated metadata descriptions are automatically integrated into the image repository and, at the same time, can be transferred by the scientists into formats that are needed when publishing the data.</p>
<p>Tags: Nfdi4Bioimage, Research Data Managementv</p>
<p><a class="reference external" href="https://zenodo.org/records/6905931">https://zenodo.org/records/6905931</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6905931">https://doi.org/10.5281/zenodo.6905931</a></p>
</section>
<hr class="docutils" />
<section id="euro-bioimaging-scientific-ambassadors-program">
<h2>Euro-BioImaging  Scientific Ambassadors Program<a class="headerlink" href="#euro-bioimaging-scientific-ambassadors-program" title="Link to this heading">#</a></h2>
<p>Beatriz Serrano-Solano</p>
<p>Published 2023-07-25</p>
<p>Licensed CC-BY-4.0</p>
<p>Graduation presentation for the 7th cohort of the Open Seeds mentoring &amp; training program for Open Science ambassadors. The project presented is called “Euro-BioImaging  Scientific Ambassadors Program”.</p>
<p><a class="reference external" href="https://zenodo.org/records/8182154">https://zenodo.org/records/8182154</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8182154">https://doi.org/10.5281/zenodo.8182154</a></p>
</section>
<hr class="docutils" />
<section id="euro-bioimaging-eric-annual-report-2022">
<h2>Euro-BioImaging ERIC Annual Report 2022<a class="headerlink" href="#euro-bioimaging-eric-annual-report-2022" title="Link to this heading">#</a></h2>
<p>Euro-BioImaging ERIC</p>
<p>Published 2023-07-14</p>
<p>Licensed CC-BY-4.0</p>
<p>Euro-BioImaging ERIC is the European landmark research infrastructure for biological and biomedical imaging as recognized by the European Strategy Forum on Research Infrastructures (ESFRI). Euro-BioImaging is the gateway to world-class imaging facilities across Europe. This document is the Euro-BioImaging Annual Report for the year 2022.</p>
<p><a class="reference external" href="https://zenodo.org/records/8146412">https://zenodo.org/records/8146412</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8146412">https://doi.org/10.5281/zenodo.8146412</a></p>
</section>
<hr class="docutils" />
<section id="euro-bioimaging-s-guide-to-fair-bioimage-data-practical-tasks">
<h2>Euro-BioImaging’s Guide to FAIR BioImage Data - Practical Tasks<a class="headerlink" href="#euro-bioimaging-s-guide-to-fair-bioimage-data-practical-tasks" title="Link to this heading">#</a></h2>
<p>Isabel Kemmer, Euro-BioImaging ERIC</p>
<p>Published 2024-06-04</p>
<p>Licensed CC-BY-4.0</p>
<p>Hands-on exercises on FAIR Bioimage Data from the interactive online workshop “Euro-BioImaging’s Guide to FAIR BioImage Data 2024” (<a class="reference external" href="https://www.eurobioimaging.eu/news/a-guide-to-fair-bioimage-data-2024/">https://www.eurobioimaging.eu/news/a-guide-to-fair-bioimage-data-2024/</a>).  Types of tasks included: FAIR characteristics of a real world dataset Data Management Plan (DMP) Journal Policies on FAIR data sharing Ontology search Metadata according to REMBI scheme (Image from: Sarkans, U., Chiu, W., Collinson, L. et al. REMBI: Recommended Metadata for Biological Images—enabling reuse of microscopy data in biology. Nat Methods 18, 1418–1422 (2021). <a class="reference external" href="https://doi.org/10.1038/s41592-021-01166-8">https://doi.org/10.1038/s41592-021-01166-8</a>) Matching datasets to bioimage repositories Browsing bioimage repositories</p>
<p>Tags: Bioimage Analysis, FAIR-Principles, Research Data Management</p>
<p>Content type: Slides, Tutorial</p>
<p><a class="reference external" href="https://zenodo.org/records/11474407">https://zenodo.org/records/11474407</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11474407">https://doi.org/10.5281/zenodo.11474407</a></p>
</section>
<hr class="docutils" />
<section id="euro-bioimaging-s-template-for-research-data-management-plans">
<h2>Euro-BioImaging’s Template for Research Data Management Plans<a class="headerlink" href="#euro-bioimaging-s-template-for-research-data-management-plans" title="Link to this heading">#</a></h2>
<p>Isabel Kemmer, Euro-BioImaging ERIC</p>
<p>Published 2024-06-04</p>
<p>Licensed CC-BY-4.0</p>
<p>Euro-BioImaging has developed a Data Management Plan (DMP) template with questions tailored to bioimaging research projects. Outlining data management practices in this way ensures traceability of project data, allowing for a continuous and unambiguous flow of information throughout the research project. This template can be used to satisfy the requirement to submit a DMP to certain funders. Regardless of the funder, Euro-BioImaging users are encouraged to provide a DMP and can use this template accordingly. 
This DMP template is available as a fillable PDF with further instructions and sample responses available by hovering over the fillable fields. </p>
<p>Tags: Bioimage Analysis, FAIR-Principles, Research Data Management</p>
<p>Content type: Collection, Tutorial</p>
<p><a class="reference external" href="https://zenodo.org/records/11473803">https://zenodo.org/records/11473803</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11473803">https://doi.org/10.5281/zenodo.11473803</a></p>
</section>
<hr class="docutils" />
<section id="euro-bioimaging-batchconvert-v0-0-4">
<h2>Euro-BioImaging/BatchConvert: v0.0.4<a class="headerlink" href="#euro-bioimaging-batchconvert-v0-0-4" title="Link to this heading">#</a></h2>
<p>bugraoezdemir</p>
<p>Published 2024-02-19</p>
<p>Licensed CC-BY-4.0</p>
<p>Changes implemented since v0.0.3</p>
<p>Support provided for file paths with spaces.
Support provided for globbing filenames from s3 for one-to-one conversion (parse_s3_filenames.py modified).
Support provided for single file import from s3 (parse_s3_filenames.py modified).
run_conversion.py replaces batchconvert_cli.sh and construct_cli.py, uniting them.
Error handling updated for each process</p>
<p><a class="reference external" href="https://zenodo.org/records/10679318">https://zenodo.org/records/10679318</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10679318">https://doi.org/10.5281/zenodo.10679318</a></p>
</section>
<hr class="docutils" />
<section id="evident-oir-sample-files-tiles-stitched-image-fv-4000">
<h2>Evident OIR sample files tiles + stitched image - FV 4000<a class="headerlink" href="#evident-oir-sample-files-tiles-stitched-image-fv-4000" title="Link to this heading">#</a></h2>
<p>Nicolas Chiaruttini</p>
<p>Published 2024-09-04</p>
<p>Licensed CC-BY-4.0</p>
<p>The files contained in this repository are confocal images taken with the Evident FV 4000 of a sample containing DAPI and mCherry stains, excited with a 405 nm laser and a 561 nm laser</p>
<p>individual tiles are named <code class="docutils literal notranslate"><span class="pre">tiling-sample-brain-section_A01_G001_{i}.oir</span></code>
The stiched image is named <code class="docutils literal notranslate"><span class="pre">Stitch_A01_G001</span></code> and contains an extra file <code class="docutils literal notranslate"><span class="pre">Stitch_A01_G001_00001</span></code>
Some metadata like the tiles positions are stored in the extra files (omp2info)</p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/13680725">https://zenodo.org/records/13680725</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13680725">https://doi.org/10.5281/zenodo.13680725</a></p>
</section>
<hr class="docutils" />
<section id="evident-oir-sample-files-with-lambda-scan-fv-4000">
<h2>Evident OIR sample files with lambda scan - FV 4000<a class="headerlink" href="#evident-oir-sample-files-with-lambda-scan-fv-4000" title="Link to this heading">#</a></h2>
<p>Nicolas Chiaruttini</p>
<p>Published 2024-07-18</p>
<p>Licensed CC-BY-4.0</p>
<p>The files contained in this repository are confocal images taken with the Evident FV 4000 of a sample containing DAPI and mCherry stains, excited with the 405 nm laser and images for different emission windows (lambda scan).
They are public sample files which goal is to help test edge cases of the bio-formats library (<a class="reference external" href="https://www.openmicroscopy.org/bio-formats/">https://www.openmicroscopy.org/bio-formats/</a>), in particular for the proper handling of lambda scans.</p>
<p>DAPI_mCherry_22Lambda-420-630-w10nm-s10nm.oir : 22 planes, each plane is an emission window, starting from 420 nm up to 630 nm by steps of 10 nm
DAPI_mCherry_4T_5Lambda-420-630-w10nm-s50nm.oir : 20 planes, 5 lambdas from 420 to 630 nm by steps of 50 nm, 4 timepoints
DAPI_mCherry_4Z_5Lambda-420-630-w10nm-s50nm.oir : 20 planes, 5 lambdas from 420 to 630 nm by steps of 50 nm, 4 slices
DAPI-mCherry_3T_4Z_5Lambda-420-630-w10nm-s50nm.oir : 60 planes, 5 lambdas from 420 to 630 nm by steps of 50 nm, 4 slices, 3 timepoints</p>
<p><a class="reference external" href="https://zenodo.org/records/12773657">https://zenodo.org/records/12773657</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12773657">https://doi.org/10.5281/zenodo.12773657</a></p>
</section>
<hr class="docutils" />
<section id="example-imaris-ims-datasets">
<h2>Example Imaris ims datasets.<a class="headerlink" href="#example-imaris-ims-datasets" title="Link to this heading">#</a></h2>
<p>Marco Stucchi</p>
<p>Published 2024-11-28</p>
<p>Licensed CC-BY-4.0</p>
<p>The files contained in this repository are example Imaris ims images.
 
Initially related to <a class="github reference external" href="https://github.com/ome/bioformats/pull/4249">ome/bioformats#4249</a></p>
<p><a class="reference external" href="https://zenodo.org/records/14235726">https://zenodo.org/records/14235726</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14235726">https://doi.org/10.5281/zenodo.14235726</a></p>
</section>
<hr class="docutils" />
<section id="example-microscopy-metadata-json-files-produced-using-micro-meta-app-to-document-example-microscopy-experiments-performed-at-individual-core-facilities">
<h2>Example Microscopy Metadata JSON files produced using Micro-Meta App to document example microscopy experiments performed at individual core facilities<a class="headerlink" href="#example-microscopy-metadata-json-files-produced-using-micro-meta-app-to-document-example-microscopy-experiments-performed-at-individual-core-facilities" title="Link to this heading">#</a></h2>
<p>Alessandro Rigano, Ulrike Boehm, Claire M. Brown, Joel Ryan, James J. Chambers, Robert A. Coleman, Orestis Faklaris, Thomas Guilbert, Michelle S. Itano, Judith Lacoste, Alex Laude, Marco Marcello, Paula Montero-Llopis, Glyn Nelson, Roland Nitschke, Jaime A. Pimentel, Stefanie Weidtkamp-Peters, Caterina Strambio-De-Castillia</p>
<p>Published 2022-01-15</p>
<p>Licensed CC-BY-4.0</p>
<p>Example Microscopy Metadata (Microscope.JSON and Settings.JSON) files produced using Micro-Meta App to document the Hardware Specifications of example Microscopes and the Image Acquisition Settings utilized to acquire example images as listed in the table below.</p>
<p>For each facility, the dataset contains two JSON files:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Microscope.JSON file (e.g., 01_marcello_uliverpool_cci_zeiss_axioobserz1_lsm710.json)
Settings.JSON file (indicated with the name of the image and with the _AS suffix)
</pre></div>
</div>
<p>Micro-Meta App was developed as part of a global community initiative including the 4D Nucleome (4DN) Imaging Working Group, BioImaging North America (BINA) Quality Control and Data Management Working Group, and QUAlity and REProducibility for Instrument and Images in Light Microscopy (QUAREP-LiMi), to extend the Open Microscopy Environment (OME) data model.</p>
<p>The works of this global community effort resulted in multiple publications featured on a recent Nature Methods FOCUS ISSUE dedicated to Reporting and reproducibility in microscopy.</p>
<p>Learn More! For a thorough description of Micro-Meta App consult our recent Nature Methods and <a class="reference external" href="http://BioRxiv.org">BioRxiv.org</a> publications!</p>
<p> </p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>		Nr.
		Manufacturer
		Model
		Tier
		&amp;Epsilon;xperiment Type
		Facility Name
		Department and Institution
		URL
		References
	
	
		1
		Carl Zeiss Microscopy
		Axio Observer Z1 (with LSM 710 scan head)
		1
		3D visualization of superhydrophobic polymer-nanoparticles
		Centre for Cell Imaging (CCI)
		University of Liverpool
		https://cci.liv.ac.uk/equipment_710.html
		Upton et al., 2020
	
	
		2
		Carl Zeiss Microscopy
		Axio Observer (Axiovert 200M)
		2
		&amp;Mu;easurement of illumination stability on Chinese Hamster Ovary cells expressing Paxillin-EGFP
		Advanced BioImaging Facility (ABIF).
		McGill University
		https://www.mcgill.ca/abif/equipment/axiovert-1
		Kiepas et al., 2020
	
	
		3
		Carl Zeiss Microscopy
		Axio Observer Z1 (with Spinning Disk)
		2
		Immunofluorescence imaging of cryosection of Mouse kidney
		Imagerie Cellulaire; Quality Control managed by Miacellavie (https://miacellavie.com/)
		Centre de recherche du Centre Hospitalier Universit&amp;eacute; de Montr&amp;eacute;al (CR CHUM), University of Montreal
		https://www.chumontreal.qc.ca/crchum/plateformes-et-services&amp;nbsp; (the web site is for all core facilities, not specifically for the core facility hosting this microscope)
		Pilliod et al., 2020
	
	
		4
		Carl Zeiss Microscopy
		Axio Imager Z2 (with Apotome)
		2
		Immunofluorescence imaging of mitotic division in Hela cells using&amp;nbsp;&amp;nbsp;
		Bioimaging Unit
		Newcastle University
		https://www.ncl.ac.uk/bioimaging/
		Watson et al., 2020
	
	
		5
		Carl Zeiss Microscopy
		Axio Observer Z1
		2
		Fluorescence microscopy of human skin fibroblasts from Glycogen Storage Disease patients.
		Life Imaging Center (LIC)
		Centre for Integrative Signalling Analysis (CISA), University of Freiburg
		https://miap.eu/equipments/sd-i-abl/
		Hannibal et al., 2020
	
	
		6
		Leica Microsystems
		DMI6000B
		2
		3D immunofluorescence imaging&amp;nbsp; rhinovirus infected macrophages&amp;nbsp;
		IMAG&amp;#39;IC Confocal Microscopy Facility
		Institut Cochin, CNRS, INSERM, Universit&amp;eacute; de Paris
		https://www.institutcochin.fr/core_facilities/confocal-microscopy/cochin-imaging-photonic-microscopy/organigram_team/10054/view
		Jubrail et al., 2020
	
	
		7
		Leica Microsystems
		DM5500B
		2
		Immunofluorescence analysis of the colocalization of PML bodies with DNA double-strand breaks
		Bioimaging Unit
		Edwardson Building on the Campus for Ageing and Vitality, Newcastle University
		https://www.ncl.ac.uk/bioimaging/equipment/leica-dm5500/#overview
		da Silva et al., 2019; Nelson et al., 2012
		&amp;nbsp;&amp;nbsp;
	
	
		8
		Leica Microsystems
		DMI8-CS (with TCS SP8 STED 3X)
		2
		Live-cell imaging of N. benthamiana leaves cells-derived protoplasts
		Center for Advanced Imaging (CAi)
		School of Mathematics/Natural Sciences, Heinrich-Heine-Universit&amp;auml;t D&amp;uuml;sseldorf
		https://www.cai.hhu.de/en/equipment/super-resolution-microscopy/leica-tcs-sp8-sted-3x
		Singer et al., 2017; H&amp;auml;nsch et al., 2020
	
	
		9
		Nikon Instruments
		Eclipse Ti
		2
		Immunofluorescence analysis of the cytoskeleton structure in COS cells
		Advanced Imaging Center (AIC)
		Janelia Research Campus, Howard Hughes Medical Institute
		https://www.janelia.org/support-team/light-microscopy/equipment
		Abdelfattah et al., 2019; Qian et al., 2019; Grimm et al., 2020
	
	
		10
		Nikon Instruments
		Eclipse Ti-E (HCA)
		2
		&amp;Tau;ime-lapse analysis of the bursting behavior of amine-functionalized vesicular assemblies
		Light Microscopy Facility (IALS-LIF)
		Institute for Applied Life Sciences, University of Massachusetts at Amherst
		https://www.umass.edu/ials/light-microscopy
		Fernandez et al., 2020
	
	
		11
		Nikon Instruments/Coleman laboratory (customized)
		TIRF HILO Epifluorescence light Microscope (THEM)/ Eclipse Ti
		2
		Single-particle tracking of Halo-tagged PCNA in Lox cells
		Coleman laboratory
		Anatomy and Structural Biology Department, The Albert Einstein College of Medicine
		https://einsteinmed.org/faculty/12252/robert-coleman/
		Drosopoulos et al., 2020
	
	
		12
		Nikon Instruments
		Eclipse Ti (with Andor Dragon Fly Spinning Disk)
		2
		Investigation of the 3D structure of cerebral organoids
		Montpellier Resources Imagerie
		Centre de Recherche de Biologie cellulaire de Montpellier (MRI-CRBM), CNRS, Univerity of Montpellier
		https://www.mri.cnrs.fr/en/optical-imaging/our-facilities/mri-crbm.html
		Ayala-Nunez et al., 2019
	
	
		13
		Nikon Instruments
		Eclipse Ti2
		2
		&amp;Iota;mmunofluorescence imaging of cryosections of mouse hearth myocardium&amp;nbsp;
		Neuroscience Center Microscopy Core
		Neuroscience Center, University of North Carolina
		https://www.med.unc.edu/neuroscience/core-facilities/neuro-microscopy/
		Aghajanian et al., 2021
	
	
		14
		Nikon Instruments
		Eclipse Ti2
		2
		Live-cell imaging of bacterial cells expressing GFP-PopZ
		Microscopy Resources on the North Quad (MicRoN)
		Harvard Medical School&amp;nbsp;
		https://micron.hms.harvard.edu/
		Lim and Bernhardt 2019; Lim et al., 2019
	
	
		15
		Olympus/Biomedical Imaging Group (customized)
		TIRF Epifluorescence Structured light Microscope (TESM)/IX71
		3
		3D distribution of HIV-1 in the nucleus of human cells
		Biomedical Imaging Group
		Program in Molecular Medicine, University of Massachusetts Medical School
		https://trello.com/b/BQ8zCcQC/tirf-epi-fluorescence-structured-light-microscope
		Navaroli et al., 2012
	
	
		16
		Olympus/Computer Vision Laboratory (customized)
		3D BrightField Scanner/IX71
		3
		Transmitted light brightfield visualization of swimming spermatocytes
		Laboratorio Nacional de Microscopia Avanzada (LNMA) and Computer Vision Laboratory of the Institute of Biotechnology
		Universidad Nacional Autonoma de Mexico (UNAM)
		https://lnma.unam.mx/wp/
		Pimentel et al., 2012; Silva-Villalobos et al., 2014
</pre></div>
</div>
<p>Getting started</p>
<p>Use these videos to get started with using Micro-Meta App after installation into OMERO and downloading the example data files:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Video 1
Video 2
</pre></div>
</div>
<p>More information</p>
<p>For full information on how to use Micro-Meta App please utilize the following resources:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Micro-Meta App website
Full documentation
Installation instructions
Step-by-Step Instructions
Tutorial Videos
</pre></div>
</div>
<p>Background</p>
<p>If you want to learn more about the importance of metadata and quality control to ensure full reproducibility, quality and scientific value in light microscopy, please take a look at our recent publications describing the development of community-driven light 4DN-BINA-OME Microscopy Metadata specifications Nature Methods and <a class="reference external" href="http://BioRxiv.org">BioRxiv.org</a> and our overview manuscript entitled A perspective on Microscopy Metadata: data provenance and quality control.</p>
<p> </p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/5847477">https://zenodo.org/records/5847477</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5847477">https://doi.org/10.5281/zenodo.5847477</a></p>
</section>
<hr class="docutils" />
<section id="example-operetta-dataset">
<h2>Example Operetta Dataset<a class="headerlink" href="#example-operetta-dataset" title="Link to this heading">#</a></h2>
<p>Nicolas Chiaruttini</p>
<p>Published 2023-07-17</p>
<p>Licensed CC-BY-4.0</p>
<p>This is a microscopy image dataset generated by the Perkin Elmer Operetta HCS microscope by of the user of the PTBIOP EPFL facility.
As of the 17th of July 2023, opening this file in ImageJ/Fiji using the BioFormats 6.14 library, this dataset generates a Null Pointer Exception.</p>
<p>A post on <a class="reference external" href="http://forum.image.sc">forum.image.sc</a> is linked to this issue:</p>
<p><a class="reference external" href="https://forum.image.sc/t/null-pointer-exception-in-perkin-elmer-operetta-dataset-with-bio-formats-6-14/83784">https://forum.image.sc/t/null-pointer-exception-in-perkin-elmer-operetta-dataset-with-bio-formats-6-14/83784</a></p>
<p> </p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/8153907">https://zenodo.org/records/8153907</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8153907">https://doi.org/10.5281/zenodo.8153907</a></p>
</section>
<hr class="docutils" />
<section id="excel-template-for-adding-key-value-pairs-to-images">
<h2>Excel template for adding Key-Value Pairs to images<a class="headerlink" href="#excel-template-for-adding-key-value-pairs-to-images" title="Link to this heading">#</a></h2>
<p>Thomas Zobel, Jens Wendt</p>
<p>Published 2024-10-30</p>
<p>Licensed CC-BY-4.0</p>
<p>This Excel Workbook contains some simple Macros to help with the generation of a .csv in the necessary format for Key-Value pair annotations of images in OMERO.
The format is tailored for the <a class="reference external" href="http://OMERO.web">OMERO.web</a> script “KeyVal_from_csv.py”  (from the version &lt;=5.8.3 of the core omero-scripts).
Attached is also a video of Thomas Zobel, the head of the imaging core facility Uni Münster, showcasing the use of the Excel workbook.The video uses a slightly older version of the workbook and OMERO, but the core functionality remains unchanged.
Please keep in mind, that the <a class="reference external" href="http://OMERO.web">OMERO.web</a> script(s) to handle Key-Value Pairs from/to .csv files will undergo a major change very soon.This might break the compatibility with the format used now for the generated .csv from the workbook.</p>
<p><a class="reference external" href="https://zenodo.org/records/14014252">https://zenodo.org/records/14014252</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14014252">https://doi.org/10.5281/zenodo.14014252</a></p>
</section>
<hr class="docutils" />
<section id="expansion-and-fluctuations-enhanced-microscopy-for-nanoscale-molecular-profiling-of-cells-and-tissues-data-processing-manual">
<h2>Expansion and fluctuations-enhanced microscopy for nanoscale molecular profiling of cells and tissues - Data processing manual<a class="headerlink" href="#expansion-and-fluctuations-enhanced-microscopy-for-nanoscale-molecular-profiling-of-cells-and-tissues-data-processing-manual" title="Link to this heading">#</a></h2>
<p>Dominik Kylies, Heil, Hannah S., Vesga, Arturo G., Del Rosario, Mario, Maria Schwerk, Malte Kuehl, Wong, Milagros N., Victor Puelles, Ricardo Henriques</p>
<p>Published 2023/2024</p>
<p>Licensed CC-BY-4.0</p>
<p>Here we provide test datasets and a training manual for the parameter optimization with eSRRF. 
The training manual will guide users through an eSRRF paramenter optimization routine and quantiative image quality assesment with both, the ImagJ-Plugin NanoJ-eSRRF (Chapter 1) and the python implementation NanoPyx-eSRRF (Chapter 2). By showcasing the optimization routine on three differnt test dataset (Chapter 3), providing intermediate results and expected outcome, the users can eaisily learn how to find the optimal processing parameters for eSRRF processing.
Three samples are provided to showcase the eSRRF reconstruction process:</p>
<ol class="arabic simple">
<li><p>Microtubules sample: Set01_DNA-PAINT_Microtubules.tif
DNA-PAINT microscopy measurement of immunolabeled microtubules in fixed COS-7 cells, showing 0.121 localizations per frame and µm^2 (data published in Laine and Heil et al.)
108x90 pixels, 500 frames, pixel size: 160 nm </p></li>
<li><p>Kidney sample: Set02_KidneySDNephrinExM.tif
ExM of human kidney biopsies stained with nephrin (data published in Kylies et al.)
150x150 pixels, 200 frames, pixel size: 102 nm </p></li>
<li><p>Single emitters simulation: Set03_simulation_groundTruth_2p5Sigma - Fluorescence stack_Avg5.tif
Simulated individual molecules emitting placed on concentric rings with radii increasing by 220 nm steps. On each ring the molecules are separated by 57.5, 115, 173, 230, 288 and 345 nm, respectively (data published in Laine and Heil et al.)
33x33 pixels, 100 frames, pixel size: 100 nm </p></li>
<li><p>Test dataset for drift/vibration correction: Set04_ExSRRF_eSRRF_vibration_correction_practice_dataset.tif
EsM of human kidney biopsies stained with nephrin (data published in Kylies et al.)
100x100 pixels, 200 frames, pixel size: 102 nm</p></li>
<li><p>Test dataset for photobleaching: Set05_Photobleaching.tif
ExM of 120 nm Nanorulers (data published in Kylies et al.)
150x150 pixels, 75 frames, pixel size: 64 nm
 
Jupyter-Notebook: ridge_detection.ipynb
With this notebook qantitative image analyis of sturctures resolved with ExSRRF can be performed.
Such as:</p></li>
</ol>
<p>calculation of the target structure density. 
identifying areas with high inter-ridge spacing by maping the distance to the nearest ridge based on Euclidean distance transform. 
measuring the spatial uniformity of the structure of interest by examining the distribution of the local densities and the distances to the nearest ridge. </p>
<p><a class="reference external" href="https://zenodo.org/records/13897937">https://zenodo.org/records/13897937</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13897937">https://doi.org/10.5281/zenodo.13897937</a></p>
</section>
<hr class="docutils" />
<section id="explainable-ai-for-computer-vision">
<h2>Explainable AI for Computer Vision<a class="headerlink" href="#explainable-ai-for-computer-vision" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2025-03-09</p>
<p>Licensed CC-BY-4.0</p>
<p>In this slide deck we learn about the basics of Explainable Artificial Intelligence with a soft focus on Computer Vision. We take a deeper dive in one method: Gradient Class Activation Maps.
Releated exercise materials are available online: <a class="reference external" href="https://haesleinhuepf.github.io/xai/">https://haesleinhuepf.github.io/xai/</a></p>
<p><a class="reference external" href="https://zenodo.org/records/14996127">https://zenodo.org/records/14996127</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14996127">https://doi.org/10.5281/zenodo.14996127</a></p>
</section>
<hr class="docutils" />
<section id="fiber-and-vessel-dataset-for-segmentation-and-characterization">
<h2>Fiber and vessel dataset for segmentation and characterization<a class="headerlink" href="#fiber-and-vessel-dataset-for-segmentation-and-characterization" title="Link to this heading">#</a></h2>
<p>Saqib Qamar, Baba, Abu Imran, Stèphane Verger, Magnus Andersson</p>
<p>Published 2024-05-03</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository hosts a comprehensive collection of datasets used to develop an innovative deep learning model designed to enhance the segmentation and characterization of macerated fibers and vessel forms in microscopy images. Included in the deposit are raw images, alongside meticulously prepared training and validation datasets. We present an automated segmentation approach that utilizes the one-stage YOLOv8 model, which has been specifically adapted to process high-resolution microscopy images up to 32640 x 25920 pixels. Our model excels in cell detection and segmentation, demonstrating exceptional proficiency.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10913446">https://zenodo.org/records/10913446</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10913446">https://doi.org/10.5281/zenodo.10913446</a></p>
</section>
<hr class="docutils" />
<section id="forschungsdatenmanagement-zukunftsfest-gestalten-impulse-fur-die-strukturevaluation-der-nationalen-forschungsdateninfrastruktur-nfdi">
<h2>Forschungsdatenmanagement zukunftsfest gestalten – Impulse für die   Strukturevaluation der Nationalen Forschungsdateninfrastruktur (NFDI)<a class="headerlink" href="#forschungsdatenmanagement-zukunftsfest-gestalten-impulse-fur-die-strukturevaluation-der-nationalen-forschungsdateninfrastruktur-nfdi" title="Link to this heading">#</a></h2>
<p>Steuerungsgremium Allianz-Schwerpunkt, Alexander von Humboldt Foundation, Deutsche Forschungsgemeinschaft, Fraunhofer Society, German Rectors’ Conference, Leibniz Association, German National Academy of Sciences Leopoldina, German Academic Exchange Service, Helmholtz Association of German Research Centres, Max Planck Society</p>
<p>Published 2024-11-04</p>
<p>Licensed CC-BY-4.0</p>
<p>Arbeitspapier des Steuerungsgremiums des Allianz-Schwerpunkts “Digitalität in der Wissenschaft”</p>
<p><a class="reference external" href="https://zenodo.org/records/14032908">https://zenodo.org/records/14032908</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14032908">https://doi.org/10.5281/zenodo.14032908</a></p>
</section>
<hr class="docutils" />
<section id="from-cells-to-pixels-bridging-biologists-and-image-analysts-through-a-common-language">
<h2>From Cells to Pixels: Bridging Biologists and  Image Analysts Through a Common Language<a class="headerlink" href="#from-cells-to-pixels-bridging-biologists-and-image-analysts-through-a-common-language" title="Link to this heading">#</a></h2>
<p>Elnaz Fazeli, Haase Robert, Doube Michael, Miura Kota, Legland David</p>
<p>Published 2024-08-16</p>
<p>Licensed CC-BY-4.0</p>
<p>Bioimaging has transformed our understanding of biological processes, yet extracting meaningful information from complex datasets remains a challenge, particularly for early career scientists. This paper proposes a simplified, systematic approach to bioimage analysis, focusing on categorizing commonly observed structures and shapes, and providing relevant analysis methods. Our approach includes illustrative examples and a visual flowchart, enabling researchers to define analysis objectives clearly. By understanding the diversity of bioimage structures and aligning them with appropriate analysis approaches, the framework empowers researchers to navigate bioimage datasets more efficiently. It also aims to foster a common language between researchers and analysts, thereby enhancing mutual understanding and facilitating effective communication.</p>
<p><a class="reference external" href="https://zenodo.org/records/13331351">https://zenodo.org/records/13331351</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13331351">https://doi.org/10.5281/zenodo.13331351</a></p>
</section>
<hr class="docutils" />
<section id="from-paper-to-pixels-navigation-through-your-research-data-presentations-of-speakers">
<h2>From Paper to Pixels: Navigation through your Research Data - presentations of speakers<a class="headerlink" href="#from-paper-to-pixels-navigation-through-your-research-data-presentations-of-speakers" title="Link to this heading">#</a></h2>
<p>Marcelo Zoccoler, Simon Bekemeier, Tom Boissonnet, Simon Parker, Luca Bertinetti, Marc Gentzel, Riccardo Massei, Cornelia Wetzker</p>
<p>Published 2024-06-10</p>
<p>Licensed CC-BY-4.0</p>
<p>The workshop introduced key topics of research data management (RDM) and the implementation thereof on a life science campus. Internal and external experts of RDM including scientists that apply chosen software tools presented the basic concepts and their implementation to a broad audience. 
Talks covered general aspects of data handling and sorting, naming conventions, data storage repositories and archives, licensing of material, data and code management using git, data protection particularly regarding patient data and in genome sequencing and more. Two data management concepts and exemplary tools were highlighted in particular, being electronic lab notebooks with eLabFTW and the bio-image management software OMERO. Those were chosen because of three aspects: the large benefit of these management tools for a life science campus, their free availability as open source tools with the option of contribution of required functionalities and first existing use cases on campus already supported by CMCB/PoL IT.
Two talks by Robert Haase (<a class="reference external" href="http://ScaDS.AI/">ScaDS.AI/</a> Uni Leipzig) and Robert Müller (Kontaktstelle Forschungsdaten, TU Dresden with contributions from Denise Dörfel) that opened the symposium were shared independently:
<a class="reference external" href="https://zenodo.org/records/11382341">https://zenodo.org/records/11382341</a>
<a class="reference external" href="https://zenodo.org/records/11261115">https://zenodo.org/records/11261115</a>
The workshop organization was funded by the CMCB/PoL Networking Grant and supported by the consortium NFDI4BIOIMAGE (funded by DFG grant number NFDI 46/1, project number 501864659).</p>
<p>Tags: Research Data Management</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/11548617">https://zenodo.org/records/11548617</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11548617">https://doi.org/10.5281/zenodo.11548617</a></p>
</section>
<hr class="docutils" />
<section id="galaxy-meets-omero-overview-on-the-galaxy-omero-suite-and-vizarr-viewer">
<h2>Galaxy meets OMERO! Overview on the Galaxy OMERO-suite and Vizarr Viewer<a class="headerlink" href="#galaxy-meets-omero-overview-on-the-galaxy-omero-suite-and-vizarr-viewer" title="Link to this heading">#</a></h2>
<p>Riccardo Massei, Matthias Bernt, Beatriz Serrano-Solano, Lucille Lopez-Delisle, Jan Bumberger, Björn Grüning, Leonid Kostrykin</p>
<p>Published 2025-03-05</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://zenodo.org/records/14975462">https://zenodo.org/records/14975462</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14975462">https://doi.org/10.5281/zenodo.14975462</a></p>
</section>
<hr class="docutils" />
<section id="gerbi-chat-teil-1-vom-bedarf-bis-zum-groszgerateantrag-schreiben">
<h2>GerBI-Chat: Teil 1 - Vom Bedarf bis zum Großgeräteantrag-Schreiben<a class="headerlink" href="#gerbi-chat-teil-1-vom-bedarf-bis-zum-groszgerateantrag-schreiben" title="Link to this heading">#</a></h2>
<p>Financial &amp; Legal Framework of Core Facilities, Elmar Endl, Jana Hedrich, Juliane Hoth, Julia Nagy, Astrid Schauss, Nina Schulze, Silke Tulok</p>
<p>Published 2024-09-11</p>
<p>Licensed CC-BY-4.0</p>
<p>Die GermanBioImaging (GerBI-GMB) - Deutsche Gesellschaft für Mikroskopie und Bildanalyse e.V. bietet über regelmäßig stattfindende Treffen (GerBI-Chats) die Möglichkeit zum aktiven Austausch der Mitglieder untereinander. Das GerBI-GMB Team “Legal und Finacial Framwork”, welches sich mit administrativen Aufgaben rund um das Core Facility Management beschäftigt, nutzt diese Möglichkeit zum aktiven Austausch innerhalb des Netzwerkes und darüber hinaus. 
Der Beschaffungsprozess von Forschungsgroßgeräten ist komplex und je nach Institution unterschiedlich geregelt. Aus unserer Sicht lässt sich dieser Prozess grob in drei Stufen aufteilen:</p>
<p>Bedarfsanmeldung
Antragsvorbereitung und -fertigstellung
Antragsbewilligung und Nutzung </p>
<p>Dieser hier enthaltene Beitrag ist der Initialvortrag des GerBi-Chats zum Teil 1 - Von der Bedarfsanmeldung bis zum Beginn der Antragststellung. Die weiteren Stufen der Großgerätebeschaffung werden in nachfolgenden Beiträgen behandelt.</p>
<p><a class="reference external" href="https://zenodo.org/records/13810879">https://zenodo.org/records/13810879</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13810879">https://doi.org/10.5281/zenodo.13810879</a></p>
</section>
<hr class="docutils" />
<section id="gerbi-chat-teil-2-wie-schreibe-ich-am-besten-einen-groszegrateantrag">
<h2>GerBI-Chat: Teil 2 - Wie schreibe ich am besten einen Großegräteantrag<a class="headerlink" href="#gerbi-chat-teil-2-wie-schreibe-ich-am-besten-einen-groszegrateantrag" title="Link to this heading">#</a></h2>
<p>Financial &amp; Legal Framework of Core Facilities, Elmar Endl, Jana Hedrich, Juliane Hoth, Julia Nagy, Astrid Schauss, Nina Schulze, Silke Tulok</p>
<p>Published 2024-10-02</p>
<p>Licensed CC-BY-4.0</p>
<p>Die GermanBioImaging (GerBI-GMB) - Deutsche Gesellschaft für Mikroskopie und Bildanalyse e.V. bietet über regelmäßig stattfindende Treffen (GerBI-Chats) die Möglichkeit zum aktiven Austausch der Mitglieder untereinander. Das GerBI-GMB Team “Legal und Finacial Framwork”, welches sich mit administrativen Aufgaben rund um das Core Facility Management beschäftigt, nutzt diese Möglichkeit zum aktiven Austausch innerhalb des Netzwerkes und darüber hinaus. 
Der Beschaffungsprozess von Forschungsgroßgeräten ist komplex und je nach Institution unterschiedlich geregelt. Aus unserer Sicht lässt sich dieser Prozess grob in drei Stufen aufteilen:</p>
<p>Bedarfsanmeldung
Antragsvorbereitung und -fertigstellung
Antragsbewilligung und Nutzung </p>
<p>Nach dem Initialvortrag der GerBI-Chat Reihe, in dem das Thema Bedarfsanmeldung im Fokus stand, geht es im hier enthaltenen zweiten Teil „Antragsvorbereitung und -fertigstellung: Wie schreibe ich am besten einen Großgeräteantrag?“ um die Beantragung von Forschungsgroßgeräten nach Art. 91b GG.</p>
<p><a class="reference external" href="https://zenodo.org/records/13807114">https://zenodo.org/records/13807114</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13807114">https://doi.org/10.5281/zenodo.13807114</a></p>
</section>
<hr class="docutils" />
<section id="getting-started-with-python-intro-and-set-up-a-conda-environment">
<h2>Getting started with Python: intro and set-up a conda environment<a class="headerlink" href="#getting-started-with-python-intro-and-set-up-a-conda-environment" title="Link to this heading">#</a></h2>
<p>Riccardo Massei</p>
<p>Published 2024-10-09</p>
<p>Licensed CC-BY-4.0</p>
<p>YMIA python event 2024
Presentation :  “Getting started with Python: intro and set-up a conda environment with Dr. Riccardo Massei”</p>
<p><a class="reference external" href="https://zenodo.org/records/13908480">https://zenodo.org/records/13908480</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13908480">https://doi.org/10.5281/zenodo.13908480</a></p>
</section>
<hr class="docutils" />
<section id="globias-in-person-workshop-2024">
<h2>GloBIAS in-person workshop 2024<a class="headerlink" href="#globias-in-person-workshop-2024" title="Link to this heading">#</a></h2>
<p>Christa Walther</p>
<p>Published 2025-04-07</p>
<p>Licensed CC-BY-4.0</p>
<p>This document reports on the first in-person workshop supported by GloBIAS. Each session has its own chapter provided by the people chairing the sessions, summarising the outputs achieved. </p>
<p><a class="reference external" href="https://zenodo.org/records/15168241">https://zenodo.org/records/15168241</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15168241">https://doi.org/10.5281/zenodo.15168241</a></p>
</section>
<hr class="docutils" />
<section id="gut-analysis-toolbox">
<h2>Gut Analysis Toolbox<a class="headerlink" href="#gut-analysis-toolbox" title="Link to this heading">#</a></h2>
<p>Luke Sorensen, Ayame Saito, Sabrina Poon, Noe Han, Myat, Ryan Hamnett, Peter Neckel, Adam Humenick, Keith Mutunduwe, Christie Glennan, Narges Mahdavian, JH Brookes, Simon, M McQuade, Rachel, PP Foong, Jaime, Estibaliz Gómez-de-Mariscal, Muñoz Barrutia, Arrate, Kaltschmidt, Julia A., King, Sebastian K., Robert Haase, Simona Carbone, A. Veldhuis, Nicholas, P. Poole, Daniel, Pradeep Rajasekhar</p>
<p>Published 2025-02-23</p>
<p>Licensed BSD-3-CLAUSE</p>
<p>Full Changelog: <a class="reference external" href="https://github.com/pr4deepr/GutAnalysisToolbox/compare/v0.7...v1.0">https://github.com/pr4deepr/GutAnalysisToolbox/compare/v0.7…v1.0</a>
Skip versions to 1.0
Fixed major bugs:</p>
<p>Use deepImageJ to run Stardist models, due to issue with tensorflow in Fiji
Fixed ganglia model to be compatible with new versions of deepImageJ
Updated all scripts to accommodate for new deepImageJ workflow
Added macros to generate user dialog when running GAT for first time</p>
<p><a class="reference external" href="https://zenodo.org/records/14913673">https://zenodo.org/records/14913673</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14913673">https://doi.org/10.5281/zenodo.14913673</a></p>
</section>
<hr class="docutils" />
<section id="gut-analysis-toolbox-training-data-and-2d-models-for-segmenting-enteric-neurons-neuronal-subtypes-and-ganglia">
<h2>Gut Analysis Toolbox: Training data and 2D models for segmenting enteric neurons, neuronal subtypes and ganglia<a class="headerlink" href="#gut-analysis-toolbox-training-data-and-2d-models-for-segmenting-enteric-neurons-neuronal-subtypes-and-ganglia" title="Link to this heading">#</a></h2>
<p>Luke Sorensen, Ayame Saito, Sabrina Poon, Myat Noe Han, Adam Humenick, Peter Neckel, Keith Mutunduwe, Christie Glennan, Narges Mahdavian, Simon JH Brookes, Rachel M McQuade, Jaime PP Foong, Sebastian K. King, Estibaliz  Gómez-de-Mariscal, Arrate Muñoz-Barrutia, Robert Haase, Simona Carbone, Nicholas A. Veldhuis, Daniel P. Poole, Pradeep Rajasekhar</p>
<p>Published 2025-05-01</p>
<p>Licensed CC-BY-4.0</p>
<p>This upload is associated with the software, Gut Analysis Toolbox (GAT).
If you use it please cite:
Sorensen et al. Gut Analysis Toolbox: Automating quantitative analysis of enteric neurons. J Cell Sci 2024; jcs.261950. doi: <a class="reference external" href="https://doi.org/10.1242/jcs.261950">https://doi.org/10.1242/jcs.261950</a>
The upload contains StarDist models for segmenting enteric neurons in 2D, enteric neuronal subtypes in 2D and FPN+ResNet101 model for enteric ganglia in 2D in gut wholemount tissue. GAT is implemented in Fiji, but the models can be used in any software that supports StarDist and the use of 2D UNet models. The files here also consist of Python notebooks (Google Colab), training and test data as well as reports on model performance.
Note: The enteric ganglia model is has been updated to v3 which uses pytorch and is a different architecture (FPN+ResNet101).
The model files are located in the respective folders as zip files. The folders have also been zipped:</p>
<p>Neuron (Hu; StarDist model):</p>
<p>Main folder: 2D_enteric_neuron_model_QA.zip
StarDist Model File:2D_enteric_neuron_v4_1.zip 
DeepImageJ compatible model: 2D_enteric_neuron.bioimage.io.model.zip (used currently in GAT)</p>
<p>Neuronal subtype (StarDist model): </p>
<p>Main folder: 2D_enteric_neuron_subtype_model_QA.zip
Model File: 2D_enteric_neuron_subtype_v4.zip
DeepImageJ compatible model: 2D_enteric_neuron_subtype.bioimage.io.model.zip (used currently in GAT)</p>
<p>Enteric ganglia (2D FPN_ResNet101; Use in FIJI with deepImageJ)</p>
<p>Main folder: 2D_enteric_ganglia_v3_training.zip
Model File: 2D_Ganglia_RGB_v3.bioimage.io.model.zip (used currently in GAT)</p>
<p>For the all models, files included are:</p>
<p>Model for segmenting cells or ganglia in 2D FIJI. StarDist or 2D UNet.
Training and Test datasets used for training.
Google Colab notebooks used for training and quality assurance (ZeroCost DL4Mic notebooks).
Python notebook and code for training ganglia model with QA.
Quality assurance reports generated from above notebooks.
StarDist model exported for use in QuPath.</p>
<p>The model files can be used within can be used within the software, StarDist. They are intended to be used within FIJI or QuPath, but can be used in any software that supports the implementation of StarDist in 2D.
Data:
All the images were collected from 4 different research labs and a public database (SPARC database) to account for variations in image acquisition, sample preparation and immunolabelling.
For enteric neurons the pan-neuronal marker, Hu has been used and the  2D wholemounts images from mouse, rat and human tissue.
For enteric neuronal subtypes, 2D images for nNOS, MOR, DOR, ChAT, Calretinin, Calbindin, Neurofilament, CGRP and SST from mouse tissue have been used..
25 images were used from the following entries in the SPARC database:</p>
<p>Howard, M. (2021). 3D imaging of enteric neurons in mouse (Version 1) [Data set]. SPARC Consortium.
Graham, K. D., Huerta-Lopez, S., Sengupta, R., Shenoy, A., Schneider, S., Wright, C. M., Feldman, M., Furth, E., Lemke, A., Wilkins, B. J., Naji, A., Doolin, E., Howard, M., &amp; Heuckeroth, R. (2020). Robust 3-Dimensional visualization of human colon enteric nervous system without tissue sectioning (Version 1) [Data set]. SPARC Consortium.
Wang, L., Yuan, P.-Q., Gould, T. and Tache, Y. (2021). Antibodies Tested in theColon – Mouse (Version 1) [Data set]. SPARC Consortium. doi:10.26275/i7dl-58h</p>
<p>Additional images for new ganglia model:</p>
<p>Hamnett, R., Dershowitz, L. B., Sampathkumar, V., Wang, Z., Gomez-Frittelli, J., De Andrade, V., Kasthuri, N., Druckmann, S. and Kaltschmidt, J. A. (2022b). Regional cytoarchitecture of the adult and developing mouse enteric nervous system. Curr. Biol. 32, 4483-4492.e5.</p>
<p>The images have been acquired using a combination different microscopes. The images for the mouse tissue were acquired using: </p>
<p>Leica TCS-SP8 confocal system (20x HC PL APO NA 1.33, 40 x HC PL APO NA 1.3) </p>
<p>Leica TCS-SP8 lightning confocal system (20x HC PL APO NA 0.88) </p>
<p>Zeiss Axio Imager M2 (20X HC PL APO NA 0.3) </p>
<p>Zeiss Axio Imager Z1 (10X HC PL APO NA 0.45) </p>
<p>Human tissue images were acquired using: </p>
<p>IX71 Olympus microscope (10X HC PL APO NA 0.3) </p>
<p>For more information, visit the Documentation website.
NOTE: The images for enteric neurons and neuronal subtypes have been rescaled to 0.568 µm/pixel for mouse and rat. For human neurons, it has been rescaled to 0.9 µm/pixel . This is to ensure the neuronal cell bodies have similar pixel area across images. The area of cells in pixels can vary based on resolution of image, magnification of objective used, animal species (larger animals -&gt; larger neurons) and potentially how the tissue is stretched during wholemount preparation 
Average neuron area for neuronal model: 701.2 ± 195.9 pixel2 (Mean ± SD, 6267 cells)
Average neuron area for neuronal subtype model: 880.9 ± 316 pixel2 (Mean ± SD, 924 cells)
Software References:
Stardist
Schmidt, U., Weigert, M., Broaddus, C., &amp; Myers, G. (2018, September). Cell detection with star-convex polygons. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 265-273). Springer, Cham.
deepImageJ
Gómez-de-Mariscal, E., García-López-de-Haro, C., Ouyang, W., Donati, L., Lundberg, E., Unser, M., Muñoz-Barrutia, A. and Sage, D., 2021. DeepImageJ: A user-friendly environment to run deep learning models in ImageJ. Nature Methods, 18(10), pp.1192-1195.
ZeroCost DL4Mic
von Chamier, L., Laine, R.F., Jukkala, J., Spahn, C., Krentzel, D., Nehme, E., Lerche, M., Hernández-Pérez, S., Mattila, P.K., Karinou, E. and Holden, S., 2021. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nature communications, 12(1), pp.1-18.</p>
<p><a class="reference external" href="https://zenodo.org/records/15314214">https://zenodo.org/records/15314214</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15314214">https://doi.org/10.5281/zenodo.15314214</a></p>
</section>
<hr class="docutils" />
<section id="hpa-nucleus-segmentation-dpnunet">
<h2>HPA Nucleus Segmentation (DPNUnet)<a class="headerlink" href="#hpa-nucleus-segmentation-dpnunet" title="Link to this heading">#</a></h2>
<p>Hao Xu, Wei Ouyang</p>
<p>Published 2023-03-02</p>
<p>Licensed CC-BY-4.0</p>
<p>Download RDF Package</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7690494">https://zenodo.org/records/7690494</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7690494">https://doi.org/10.5281/zenodo.7690494</a></p>
</section>
<hr class="docutils" />
<section id="ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking">
<h2>HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking<a class="headerlink" href="#ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking" title="Link to this heading">#</a></h2>
<p>Estibaliz Gómez-de-Mariscal, Hasini Jayatilaka, Denis Wirtz, Arrate Muñoz-Barrutia</p>
<p>Published 2021-12-13</p>
<p>Licensed CC-BY-4.0</p>
<p>Human fibrosarcoma HT1080WT (ATCC) cells at low cell densities embedded in 3D collagen type I matrices [1]. The time-lapse videos were recorded every 2 minutes for 16.7 hours and covered a field of view of 1002 pixels × 1004 pixels with a pixel size of 0.802 μm/pixel The videos were pre-processed to correct frame-to-frame drift artifacts, resulting in a final size of 983 pixels × 985 pixels pixels.</p>
<p>Hasini Jayatilaka, Anjil Giri, Michelle Karl, Ivie Aifuwa, Nicholaus J Trenton, Jude M Phillip, Shyam Khatau, and Denis Wirtz. EB1 and cytoplasmic dynein mediate protrusion dynamics for efficient 3-dimensional cell migration. FASEB J., 32(3):1207–1221, 2018. ISSN 0892-6638. doi: 10.1096/fj.201700444RR.</p>
<p>Further information about how to use this data is given in <a class="github reference external" href="https://github.com/esgomezm/microscopy-dl-suite-tf">esgomezm/microscopy-dl-suite-tf</a></p>
<p>This dataset is provided together with the following preprint and if you use it, we would like to kindly ask you to cite it properly:</p>
<p>Estibaliz Gómez-de-Mariscal, Hasini Jayatilaka, Özgün Çiçek, Thomas Brox, Denis Wirtz, Arrate Muñoz-Barrutia, <em>Search for temporal cell segmentation robustness in phase-contrast microscopy videos</em>, arXiv 2021 (arXiv:2112.08817)</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5979761">https://zenodo.org/records/5979761</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5979761">https://doi.org/10.5281/zenodo.5979761</a></p>
</section>
<hr class="docutils" />
<section id="hackaton-results-conversion-of-knime-image-analysis-workflows-to-galaxy">
<h2>Hackaton Results - Conversion of KNIME image analysis workflows to Galaxy<a class="headerlink" href="#hackaton-results-conversion-of-knime-image-analysis-workflows-to-galaxy" title="Link to this heading">#</a></h2>
<p>Riccardo Massei</p>
<p>Published 2024-03-07</p>
<p>Licensed CC-BY-4.0</p>
<p>Results of the project “Conversion of KNIME image analysis workflows to Galaxy” during the Hackathon “Image Analysis in Galaxy” (Freiburg 26 Feb - 01 Mar 2024)
 </p>
<p><a class="reference external" href="https://zenodo.org/records/10793700">https://zenodo.org/records/10793700</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10793700">https://doi.org/10.5281/zenodo.10793700</a></p>
</section>
<hr class="docutils" />
<section id="hela-kyoto-cells-under-the-scope">
<h2>HeLa “Kyoto” cells under the scope<a class="headerlink" href="#hela-kyoto-cells-under-the-scope" title="Link to this heading">#</a></h2>
<p>Romain Guiet</p>
<p>Published 2022-02-25</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: HeLa “Kyoto” cells under the scope</p>
<p>Microscope: Perkin Elmer Operetta microscope with a 20x N.A. 0.8 objective and an Andor Zyla 5.5 camera.</p>
<p>Microscopy data type: The time-lapse datasets were acquired every 15 minutes, for 60 hours. From the individual plan images (channels, time-points, field of view exported by the PerkinElmer software Harmony) multi-dimension images were generated using the Operetta_Importer-0.1.21  with a downscaling of 4. </p>
<p>Channel 1 : Low Contrast DPC (Digital Phase Contrast)</p>
<p>Channel 2 : High Contrast DPC</p>
<p>Channel 3 : Brightfield</p>
<p>Channel 4 : EGFP-α-tubulin</p>
<p>Channel 5 : mCherry-H2B</p>
<p>File format: .tif (16-bit)</p>
<p>Image size: 540x540 (Pixel size: 0.299 nm), 5c, 1z , 240t</p>
<p> </p>
<p>Cell type: HeLa “Kyoto” cells, expressing EGFP-α-tubulin and mCherry-H2B ( Schmitz et al, 2010 )</p>
<p>Protocol: Cells were resuspended in Imaging media and were seeded in a microscopy grade 96 wells plate ( CellCarrier Ultra 96, Perkin Elmer). The day after seeding, and for 60 hours, images were acquired in 3 wells, in 25 different fields of view, every 15 minutes.</p>
<p>Imaging media: DMEM red-phenol-free media (FluoroBrite™ DMEM, Gibco) complemented with Fetal Calf Serum and Glutamax.</p>
<p> </p>
<p>NOTE: This dataset was used to automatically generate label images in the following Zenodo entry:  <a class="reference external" href="https://doi.org/10.5281/zenodo.6140064">https://doi.org/10.5281/zenodo.6140064</a></p>
<p>NOTE: This dataset was used to train the cellpose models in the following Zenodo entry: <a class="reference external" href="https://doi.org/10.5281/zenodo.6140111">https://doi.org/10.5281/zenodo.6140111</a></p>
<p><a class="reference external" href="https://zenodo.org/records/6139958">https://zenodo.org/records/6139958</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6139958">https://doi.org/10.5281/zenodo.6139958</a></p>
</section>
<hr class="docutils" />
<section id="high-throughput-automated-data-analysis-and-data-management-workflow-with-cellprofiler-and-omero">
<h2>High throughput &amp; automated data analysis and data management workflow with Cellprofiler and OMERO<a class="headerlink" href="#high-throughput-automated-data-analysis-and-data-management-workflow-with-cellprofiler-and-omero" title="Link to this heading">#</a></h2>
<p>Sarah Weischer, Jens Wendt, Thomas Zobel</p>
<p>Licensed CC-BY-4.0</p>
<p>In this workshop a fully integrated data analysis solutions employing OMERO and commonly applied image analysis tools (e.g., CellProfiler, Fiji) using existing python interfaces (OMERO Python language bindings, ezOmero, Cellprofiler Python API) is presented.</p>
<p>Tags: OMERO, Data Analysis, Bioimage Analysis</p>
<p>Content type: Collection</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.8139353">https://zenodo.org/doi/10.5281/zenodo.8139353</a></p>
</section>
<hr class="docutils" />
<section id="human-dab-staining-axioscan-bf-20x">
<h2>Human DAB staining Axioscan BF 20x<a class="headerlink" href="#human-dab-staining-axioscan-bf-20x" title="Link to this heading">#</a></h2>
<p>Mario Garcia</p>
<p>Published 2024-05-21</p>
<p>Licensed CC-BY-4.0</p>
<p>Human brain tissue with DAB immunostaining. Image acquired by BF microscopy in  Zeiss Axioscan at 20x. </p>
<p><a class="reference external" href="https://zenodo.org/records/11234863">https://zenodo.org/records/11234863</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11234863">https://doi.org/10.5281/zenodo.11234863</a></p>
</section>
<hr class="docutils" />
<section id="human-lung-tissue-microscopy-dic-fluorescence-cell-and-nuclei-semantic-instance-annotations">
<h2>Human Lung Tissue Microscopy (DIC, Fluorescence, Cell and Nuclei Semantic Instance Annotations)<a class="headerlink" href="#human-lung-tissue-microscopy-dic-fluorescence-cell-and-nuclei-semantic-instance-annotations" title="Link to this heading">#</a></h2>
<p>Melanie Dohmen, Mirja Mittermaier, Andreas Hocke</p>
<p>Published 2024-02-22</p>
<p>The zip file contains 3 folders (annotations, images and training_splits).The annotation folder contains 3 folders (cell_instances, nuclei_instances and semantic). Cell and nuclei instance annotations are long int tif images, containing numbered instance ids and 0 in the background. Semantic annotations are 8-bit int png files containing the class ids (0: background, 1: normal tissue, 2: erythrocytes, 3: alveolar epithelial type 2 cells, 4: alveolar macrophages, 5: other nuclei, 6: alveolar epithelial type 2 cell nuclei, 7: alveolar macrophage nuclei, 8: cell debris).
The image folder contains 4 folders (CD68, DAPI, DIC, proSPC), where DIC contains float valued background-corrected differential interference contrast images, the others contain normalized float-valued fluorescence channels of a multi-plex staining with CD-68 (whole alveolar macrophages), DAPI (any cell nuclei), proSPC (cytoplasm of alveolar epithelial type 2 cell). All images are in tif format.
The training split folder contains 3 text files, with the image prefix (compared to images and annotations without ending, i.e. e.g. without “_DIC.tif”) of all cases in the respective subset. With a total of 68 cases, there are 51 cases in the train set, 7 cases in the validation set and 10 cases in the test set.The lung tissue origins from lung surgery of patients, but does not include resected tumors. Please see reference [1]. The images were acquired with a laser scanning microscope with 40x magnification and 1024 x 1024 pixels per image.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10669918">https://zenodo.org/records/10669918</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10669918">https://doi.org/10.5281/zenodo.10669918</a></p>
</section>
<hr class="docutils" />
<section id="i3d-bio-s-omero-training-material-re-usable-adjustable-multi-purpose-slides-for-local-user-training">
<h2>I3D:bio’s OMERO training material: Re-usable, adjustable, multi-purpose slides for local user training<a class="headerlink" href="#i3d-bio-s-omero-training-material-re-usable-adjustable-multi-purpose-slides-for-local-user-training" title="Link to this heading">#</a></h2>
<p>Christian Schmidt, Michele Bortolomeazzi, Tom Boissonnet, Carsten Fortmann-Grote, Julia Dohle, Peter Zentis, Niraj Kandpal, Susanne Kunis, Thomas Zobel, Stefanie Weidtkamp-Peters, Elisa Ferrando-May</p>
<p>Published 2023-11-13</p>
<p>Licensed CC-BY-4.0</p>
<p>The open-source software OME Remote Objects (OMERO) is a data management software that allows storing, organizing, and annotating bioimaging/microscopy data. OMERO has become one of the best-known systems for bioimage data management in the bioimaging community. The Information Infrastructure for BioImage Data (I3D:bio) project facilitates the uptake of OMERO into research data management (RDM) practices at universities and research institutions in Germany. Since the adoption of OMERO into researchers’ daily routines requires intensive training, a broad portfolio of training resources for OMERO is an asset. On top of using the OMERO guides curated by the Open Microscopy Environment Consortium (OME) team, imaging core facility staff at institutions where OMERO is used often prepare additional material tailored to be applicable for their own OMERO instances. Based on experience gathered in the Research Data Management for Microscopy group (RDM4mic) in Germany, and in the use cases in the I3D:bio project, we created a set of reusable, adjustable, openly available slide decks to serve as the basis for tailored training lectures, video tutorials, and self-guided instruction manuals directed at beginners in using OMERO. The material is published as an open educational resource complementing the existing resources for OMERO contributed by the community.</p>
<p>Tags: OMERO, Research Data Management, Nfdi4Bioimage, I3Dbio</p>
<p>Content type: Slides, Video</p>
<p><a class="reference external" href="https://zenodo.org/records/8323588">https://zenodo.org/records/8323588</a></p>
<p><a class="reference external" href="https://www.youtube.com/playlist?list=PL2k-L-zWPoR7SHjG1HhDIwLZj0MB_stlU">https://www.youtube.com/playlist?list=PL2k-L-zWPoR7SHjG1HhDIwLZj0MB_stlU</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8323588">https://doi.org/10.5281/zenodo.8323588</a></p>
</section>
<hr class="docutils" />
<section id="ics-ids-stitched-file">
<h2>ICS/IDS stitched file<a class="headerlink" href="#ics-ids-stitched-file" title="Link to this heading">#</a></h2>
<p>IMCF</p>
<p>Published 2024-06-13</p>
<p>Licensed CC-BY-4.0</p>
<p>Hi &#64;ome team !
We usually use ICS/IDS file formats as an output to our stitching pipeline as the reading and writing is pretty fast. However, it seems that since Bio-Formats 7.x opening the files is not working anymore.
I tried with a Fiji with Bio-Formats 6.10.1 and the files open, but more recent versions give an issue.
 
java.lang.NullPointerException
at loci.formats.in.ICSReader.initFile(ICSReader.java:1481)
at loci.formats.FormatReader.setId(FormatReader.java:1480)
at loci.plugins.in.ImportProcess.initializeFile(ImportProcess.java:498)
at loci.plugins.in.ImportProcess.execute(ImportProcess.java:141)
at loci.plugins.in.Importer.showDialogs(Importer.java:156)
at loci.plugins.in.Importer.run(Importer.java:77)
at loci.plugins.LociImporter.run(LociImporter.java:78)
at ij.IJ.runUserPlugIn(IJ.java:244)
at ij.IJ.runPlugIn(IJ.java:210)
at ij.Executer.runCommand(Executer.java:152)
at ij.Executer.run(Executer.java:70)
at ij.IJ.run(IJ.java:326)
at ij.IJ.run(IJ.java:337)
at ij.macro.Functions.doRun(Functions.java:703)
at ij.macro.Functions.doFunction(Functions.java:99)
at ij.macro.Interpreter.doStatement(Interpreter.java:281)
at ij.macro.Interpreter.doStatements(Interpreter.java:267)
at ij.macro.Interpreter.run(Interpreter.java:163)
at ij.macro.Interpreter.run(Interpreter.java:93)
at ij.macro.MacroRunner.run(MacroRunner.java:146)
at java.lang.Thread.run(Thread.java:750)</p>
<p>You can find one example file at this link 1.
Thanks for your help !Best,Laurent</p>
<p><a class="reference external" href="https://zenodo.org/records/11637422">https://zenodo.org/records/11637422</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11637422">https://doi.org/10.5281/zenodo.11637422</a></p>
</section>
<hr class="docutils" />
<section id="image-analysis-using-galaxy">
<h2>Image Analysis using Galaxy<a class="headerlink" href="#image-analysis-using-galaxy" title="Link to this heading">#</a></h2>
<p>Beatriz Serrano-Solano, Leonid Kostrykin, Anne Fouilloux, Riccardo Massei</p>
<p>Published 2025-02-28</p>
<p>Licensed CC-BY-4.0</p>
<p>GloBIAS seminar series
 
Part 3 in the topic: 
Infrastructure for deploying image analysis workflows
Image analysis using Galaxy</p>
<p>Beatrix Serrano-Solano, Euro-BioImaging ERIC Bio-Hub, EMBL Heidelberg, Germany
&amp; Anne Fouilloux , Simula Research Laboratory, Oslo, Norway
&amp; Leonid Kostrykin, Biomedical Computer Vision Group, Heidelberg University, BioQuant, IPMB, Heidelberg, Germany
&amp; Ricardo Massei, Helmholtz Center for Environmental Research, UFZ, Leipzig, Germany
Abstract: This webinar will introduce the Galaxy Image Analysis Community and highlight our mission to advance the development of FAIR and reproducible image analysis workflows. As part of our commitment to making image data analysis more accessible and collaborative, we will showcase how Galaxy can serve the imaging community. The session will explore Galaxy’s capabilities for integrating popular image analysis tools, interactive environments, and notebooks, making it a versatile platform for researchers across various scientific domains. We will also present how Galaxy facilitates the creation and sharing of reusable workflows, promoting open science and fostering collaboration. To give participants hands-on insight, we’ll provide a live demonstration on designing and running image analysis workflows within Galaxy. </p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/14944040">https://zenodo.org/records/14944040</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14944040">https://doi.org/10.5281/zenodo.14944040</a></p>
</section>
<hr class="docutils" />
<section id="image-repository-decision-tree-where-do-i-deposit-my-imaging-data">
<h2>Image Repository Decision Tree - Where do I deposit my imaging data<a class="headerlink" href="#image-repository-decision-tree-where-do-i-deposit-my-imaging-data" title="Link to this heading">#</a></h2>
<p>Isabel Kemmer, Feriel Romdhane, Euro-BioImaging ERIC</p>
<p>Published 2025-05-15</p>
<p>Licensed CC-BY-4.0</p>
<p>Depositing data in quality data repositories is one crucial step towards FAIR (Findable, Accessible, Interoperable, and Reusable) data. Accordingly, Euro-BioImaging strongly encourages sharing scientific imaging data in established, thematic repositories. 
To guide you in the selection of appropriate repositories, we have created an overview of available repositories for different types of image data, including their scope and requirements. This decision tree guides you through questions about your data and directs you to the correct repository, and/or provides instructions for further processing to meet the critera of the repositories. 
Three seperate trees are provided for different classes of imaging data: open bioimage data, preclinical data, and human imaging data. These versions with three trees can be used for web-view. Update: also the editable versions in powerpoint format (.pptx) are now provided. Please be aware that opening the versions with another program might lead to shifted formatting.
Update: we now also provide ready-to-print versions designed to be printed on A3 format. One page shows the open bioimaging data tree and one page combines the preclinical and human imaging data trees. Also the editable versions of these are provided.</p>
<p><a class="reference external" href="https://zenodo.org/records/15425770">https://zenodo.org/records/15425770</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15425770">https://doi.org/10.5281/zenodo.15425770</a></p>
</section>
<hr class="docutils" />
<section id="image-handling-using-fiji-training-materials">
<h2>Image handling using Fiji - training materials<a class="headerlink" href="#image-handling-using-fiji-training-materials" title="Link to this heading">#</a></h2>
<p>Joanna Pylvänäinen</p>
<p>Published 2025-01-30</p>
<p>Licensed MIT</p>
<p>Description:This training package provides a guide to image processing and analysis using ImageJ/Fiji, an open-source software widely used in biological and medical imaging. The manual includes step-by-step exercises demonstrating practical workflows for measuring size distribution and signal intensity using both conventional thresholding and advanced tools like StarDist. This resource is ideal for researchers, students, and professionals looking to enhance their image analysis skills using Fiji.
Key topics include:</p>
<p>Image calibration and intensity adjustments
Channel splitting and merging
Projection techniques and scale bar addition
Segmentation and thresholding methods
Quantitative analysis of nuclei and fluorescence signal intensity</p>
<p>Publication Date: January 2025
Keywords: Fiji, ImageJ, Image Analysis, Microscopy, Segmentation, Particle Analysis, 3D Visualization, StarDist
License: MIT</p>
<p><a class="reference external" href="https://zenodo.org/records/14771563">https://zenodo.org/records/14771563</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14771563">https://doi.org/10.5281/zenodo.14771563</a></p>
</section>
<hr class="docutils" />
<section id="imagej-tool-for-percentage-estimation-of-pneumonia-in-lungs">
<h2>ImageJ tool for percentage estimation of pneumonia in lungs<a class="headerlink" href="#imagej-tool-for-percentage-estimation-of-pneumonia-in-lungs" title="Link to this heading">#</a></h2>
<p>Martin Schätz, Olga Rubešová, Jan Mareš, Alan Spark</p>
<p>Published 2025-07-07</p>
<p>Licensed CC-BY-4.0</p>
<p>The software tool is developed on demand of Radiological Department of Faculty Hospital of Královské Vinohrady, with the aim to provide a tool to estimate the percentage of pneumonia (or COVID-19 presence) in lungs. Paper Estimation of Covid-19 lungs damage based on computer tomography images analysis presenting the tool is available on F1000reserach DOI: 10.12688/f1000research.109020.1. The underlying dataset is published in Zenodo (DOI:10.5281/zenodo.5805939). One of the challenges was to design a tool that would be available without complicated install procedures and would process data in a reasonable time even on office computers. For this reason, 8-bit and 16-bit version of the tool exists. The FIJI software (or ImageJ with Bio-Formats plugin installed) was selected as the best candidate. Examples of use and tutorials are available at GitHub. 
The third version includes an intra-variabilty analysis, containing evaluation both for percentage and score metrics.
Underlying data:DOI:10.5281/zenodo.5805939The first five datasets are analyzed using this tool, with results and parameters to repeat the analysis in results_csv.csv or results.xlsx.
Contributions:Martin SCHÄTZ:       Coding, tool testing, data curation, data set analysisOlga RUBEŠOVÁ:    Code review, tutorial preparation, tool testing, data set analysisJan MAREŠ:             Tool testing, data set analysis, funding acquisitionAlan SPARK:             Tool testing
The work was funded by the Ministry of Education, Youth and Sports by grant ‘Development of Advanced Computational Algorithms for evaluating post-surgery rehabilitation’ number LTAIN19007. The work was also supported from the grant of Specific university research – grant No FCHI 2022-001.
 </p>
<p><a class="reference external" href="https://zenodo.org/records/15827771">https://zenodo.org/records/15827771</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15827771">https://doi.org/10.5281/zenodo.15827771</a></p>
</section>
<hr class="docutils" />
<section id="implantation-of-abdominal-imaging-windows-on-the-mouse-kidney">
<h2>Implantation of abdominal imaging windows on the mouse kidney<a class="headerlink" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-kidney" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-09-04</p>
<p>Licensed CC-BY-ND-4.0</p>
<p>This video describes the surgical process of implanting an abdominal imaging window (AIW) on the kidney of mice. This window can be used for acute or longitudinal imaging. All experiments have been reviewed and approved by the local authorities (Landesdirektion Sachsen).
Implantation of chronic abdominal windows allows for microscopical investigation of highly dynamic processes in physiological and pathological circumstances and is generally tolerated well by experimental animals. It enables insights which otherwise could only be obtained using high numbers of experimental animals. The method can be regarded as reduction approach in terms of 3R implementation.
This upload contains the full version and is distributed under CC BY-ND 4.0 license to inhibit decontextualized misuse. Please check license terms for usage, especially for remixing/transforming! If you want to remix the material, get in contact with the author.</p>
<p><a class="reference external" href="https://zenodo.org/records/13682928">https://zenodo.org/records/13682928</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13682928">https://doi.org/10.5281/zenodo.13682928</a></p>
</section>
<hr class="docutils" />
<section id="implantation-of-abdominal-imaging-windows-on-the-mouse-kidney-short-version">
<h2>Implantation of abdominal imaging windows on the mouse kidney - short version<a class="headerlink" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-kidney-short-version" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-09-09</p>
<p>Licensed CC-BY-ND-4.0</p>
<p>This video describes the surgical process of implanting an abdominal imaging window (AIW) on the kidney of mice. This window can be used for acute or longitudinal imaging. All experiments have been reviewed and approved by the local authorities (Landesdirektion Sachsen).
Implantation of chronic abdominal windows allows for microscopical investigation of highly dynamic processes in physiological and pathological circumstances and is generally tolerated well by experimental animals. It enables insights which otherwise could only be obtained using high numbers of experimental animals. The method can be regarded as reduction approach in terms of 3R implementation.
This upload contains the shortened version and is distributed under CC BY-ND 4.0 license to inhibit decontextualized misuse. Please check license terms for usage, especially for remixing/transforming! If you want to remix the material, get in contact with the author.</p>
<p><a class="reference external" href="https://zenodo.org/records/13736240">https://zenodo.org/records/13736240</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13736240">https://doi.org/10.5281/zenodo.13736240</a></p>
</section>
<hr class="docutils" />
<section id="implantation-of-abdominal-imaging-windows-on-the-mouse-liver">
<h2>Implantation of abdominal imaging windows on the mouse liver<a class="headerlink" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-liver" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-09-04</p>
<p>Licensed CC-BY-ND-4.0</p>
<p>This video describes the surgical process of implanting an abdominal imaging window (AIW) on the liver of mice. This window can be used for acute or longitudinal imaging. All experiments have been reviewed and approved by the local authorities (Landesdirektion Sachsen).
Implantation of chronic abdominal windows allows for microscopical investigation of highly dynamic processes in physiological and pathological circumstances and is generally tolerated well by experimental animals. It enables insights which otherwise could only be obtained using high numbers of experimental animals. The method can be regarded as reduction approach in terms of 3R implementation.
This upload contains the full version and is distributed under CC BY-ND 4.0 license to inhibit decontextualized misuse. Please check license terms for usage, especially for remixing/transforming! If you want to remix the material, get in contact with the author.</p>
<p><a class="reference external" href="https://zenodo.org/records/13683167">https://zenodo.org/records/13683167</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13683167">https://doi.org/10.5281/zenodo.13683167</a></p>
</section>
<hr class="docutils" />
<section id="implantation-of-abdominal-imaging-windows-on-the-mouse-liver-short-version">
<h2>Implantation of abdominal imaging windows on the mouse liver - short version<a class="headerlink" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-liver-short-version" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-09-09</p>
<p>Licensed CC-BY-ND-4.0</p>
<p>This video describes the surgical process of implanting an abdominal imaging window (AIW) on the liver of mice. This window can be used for acute or longitudinal imaging. All experiments have been reviewed and approved by the local authorities (Landesdirektion Sachsen).
Implantation of chronic abdominal windows allows for microscopical investigation of highly dynamic processes in physiological and pathological circumstances and is generally tolerated well by experimental animals. It enables insights which otherwise could only be obtained using high numbers of experimental animals. The method can be regarded as reduction approach in terms of 3R implementation.
This upload contains the short version and is distributed under CC BY-ND 4.0 license to inhibit decontextualized misuse. Please check license terms for usage, especially for remixing/transforming! If you want to remix the material, get in contact with the author.</p>
<p><a class="reference external" href="https://zenodo.org/records/13736218">https://zenodo.org/records/13736218</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13736218">https://doi.org/10.5281/zenodo.13736218</a></p>
</section>
<hr class="docutils" />
<section id="incell-datasets-with-mix-of-2d-and-3d-failed-to-be-read">
<h2>InCell datasets with mix of 2D and 3D failed to be read<a class="headerlink" href="#incell-datasets-with-mix-of-2d-and-3d-failed-to-be-read" title="Link to this heading">#</a></h2>
<p>Fabien Kuttler, Rémy Dornier</p>
<p>Published 2025-01-31</p>
<p>Licensed CC-BY-4.0</p>
<p>The provided dataset contains 2 wells, 4 fields of view, 4 channels, no T but different number of Z according to the channel</p>
<p>Cy3 : 1 Z
DAPI : 16 Z
FITC : 1 Z
Brightfield : 1 Z</p>
<p>The mix 2D/3D is not correctly supported and the .xcde file cannot be read.
A discussion thread is already open on that topic.
Bio-Formats version : 8.0.1
 </p>
<p><a class="reference external" href="https://zenodo.org/records/14777242">https://zenodo.org/records/14777242</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14777242">https://doi.org/10.5281/zenodo.14777242</a></p>
</section>
<hr class="docutils" />
<section id="ink-in-a-dish">
<h2>Ink in a dish<a class="headerlink" href="#ink-in-a-dish" title="Link to this heading">#</a></h2>
<p>Cavanagh</p>
<p>Published 2024-09-03</p>
<p>Licensed CC0-1.0</p>
<p>A test data set for troublshooting. no scientific meaning.</p>
<p><a class="reference external" href="https://zenodo.org/records/13642395">https://zenodo.org/records/13642395</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13642395">https://doi.org/10.5281/zenodo.13642395</a></p>
</section>
<hr class="docutils" />
<section id="insights-and-impact-from-five-cycles-of-essential-open-source-software-for-science">
<h2>Insights and Impact From Five Cycles of Essential Open Source Software for Science<a class="headerlink" href="#insights-and-impact-from-five-cycles-of-essential-open-source-software-for-science" title="Link to this heading">#</a></h2>
<p>Kate Hertweck, Carly Strasser, Dario Taraborelli</p>
<p>Licensed CC-BY-4.0</p>
<p>Open source software (OSS) is essential for advancing scientific discovery, particularly in biomedical research, yet funding to support these vital tools has been limited. The Chan Zuckerberg Initiative’s Essential Open Source Software for Science (EOSS) program has significantly contributed to this field by providing $51.8 million in funding over five years to support the maintenance, growth, and community engagement of critical OSS tools. The program has impacted scientific OSS projects by improving their technical outputs, community building, and sustainability practices, and fostering collaborations within the OSS community. Additionally, EOSS funding has enhanced diversity, equity, and inclusion within the OSS community, although changes in principal investigator demographics were not observed. The funded projects have had a substantial impact on biomedical research by improving the usability and accessibility of scientific software, which has led to increased adoption and advancements in various biomedical fields.</p>
<p>Tags: Open Source Software, Funding, Sustainability</p>
<p>Content type: Publication</p>
<p><a class="reference external" href="https://zenodo.org/records/11201216">https://zenodo.org/records/11201216</a></p>
</section>
<hr class="docutils" />
<section id="insights-from-acquiring-open-medical-imaging-datasets-for-foundation-model-development">
<h2>Insights from Acquiring Open Medical Imaging  Datasets for Foundation Model Development<a class="headerlink" href="#insights-from-acquiring-open-medical-imaging-datasets-for-foundation-model-development" title="Link to this heading">#</a></h2>
<p>Stefan Dvoretskii</p>
<p>Published 2024-04-10</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://zenodo.org/records/11503289">https://zenodo.org/records/11503289</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11503289">https://doi.org/10.5281/zenodo.11503289</a></p>
</section>
<hr class="docutils" />
<section id="id2">
<h2>Insights from Acquiring Open Medical Imaging Datasets for Foundation Model Development<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>Stefan Dvoretskii</p>
<p>Published 2024-04-10</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://zenodo.org/records/13380289">https://zenodo.org/records/13380289</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13380289">https://doi.org/10.5281/zenodo.13380289</a></p>
</section>
<hr class="docutils" />
<section id="institutionalization-and-collaboration-as-a-way-of-addressing-the-challenges-open-science-presents-to-libraries-the-university-of-konstanz-as-a-national-pioneer">
<h2>Institutionalization and Collaboration as a Way of Addressing the Challenges Open Science Presents to Libraries: The University of Konstanz as a National Pioneer<a class="headerlink" href="#institutionalization-and-collaboration-as-a-way-of-addressing-the-challenges-open-science-presents-to-libraries-the-university-of-konstanz-as-a-national-pioneer" title="Link to this heading">#</a></h2>
<p>Sophie Habinger, Maximilian Heber, Sonja Kralj, Emilia Mikautsch</p>
<p>Published 2024-07-09</p>
<p>Licensed CC-BY-4.0</p>
<p>The rise of Open Science (OS) and the academic community’s needs that come with it bring about a range of challenges for academic libraries. To face these challenges, the University of Konstanz has created a competence unit called Team Open Science in the Communication, Information, Media Center (KIM) - a joint unit of library and IT infrastructure. The Team creates synergies within itself and across the library. In December 2023, it involved 12 staff members specialising in open access (OA), research data management (RDM), open educational resources (OER) and virtual research environments (VRE). It collaborates closely with other KIM departments. This submission shall serve as a best practice example for the impact of OS on research libraries and, beyond that, the impact of research libraries on universities.
To enhance and foster OS, the Team provides individual consultations, services and office hours for researchers. Here, it collaborates closely with other librarians like subject specialists and the Team University Publications. Along similar lines, the KIM offers institutional repositories for publications (KOPS) and research data (KonDATA). Beyond that, the Team provides solutions to host OA journals and analyses researchers’ VRE needs to decide on implementation options. In sum, the Team is the central OS contact point for the entire university, underlining the major role the library holds in making institutional impact.
Furthermore, the Team had the leading role in creating the University of Konstanz’ OS Policy, one of the first ones passed by a German university. This policy stands out because it encompasses various OS domains. It demands, among other things, that text publications be made OA and that research data be managed according to relevant subject-specific standards. If permissible and reasonable, it demands that research data should be made publicly available at the earliest possible time. Along these lines, the policy has a large impact on how the library handles closed access books and subscription-based journals. As a consequence, OA is pursued wherever possible, leading to the highest OA quota of all German universities. In that sense, the Team is a crucial driving force of OS in the University of Konstanz, which ties in with the library’s major role of open research transformation.
Beyond the University of Konstanz, the Team is involved in a range of national and international projects collaborating with other libraries. On a national level, they lead the project open.access-network which provides an information platform for researchers and librarians and connects the German-speaking OA community through events like bar camps. The project KOALA-AV supports libraries in establishing consortial solutions for financing Diamond OA publications. Moreover, the Team is involved in the federal state initiative for RDM in Baden-Württemberg (bwFDM). Here, the Team is in charge of <a class="reference external" href="http://forschungsdaten.info">forschungsdaten.info</a>, the German-speaking countries’ leading RDM information platform, which will be offered in English within the next years. Internationally, the Team cooperates with librarians and other OS professionals from the European Reform University Alliance (ERUA) and the European University for Well-Being (EUniWell), establishing formats for best practice exchange, such as monthly OS Meet-Ups.</p>
<p><a class="reference external" href="https://zenodo.org/records/12699637">https://zenodo.org/records/12699637</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12699637">https://doi.org/10.5281/zenodo.12699637</a></p>
</section>
<hr class="docutils" />
<section id="integration-of-bioimage-and-omics-data-resources">
<h2>Integration of Bioimage and *Omics data resources<a class="headerlink" href="#integration-of-bioimage-and-omics-data-resources" title="Link to this heading">#</a></h2>
<p>Carsten Fortmann-Grote, Mariana Meireles</p>
<p>Published 2025-02-03</p>
<p>This Poster was presented at the 2025 All Hands Meeting of the NFDI4BIOIMAGE Consortium. It presents the current state of data integration activities at the MPI for Evolutionary Biology. Various data and metadata resources such as the internal image data repository OMERO and the Electronic Lab Notebook System OpenBIS are converted into a RDF Knowledge Graph utilizing a R2RML mapping scheme based on the Ontop-VKG framework. The materialized Knowledge Graph is then served via the QLever SPARQL endpoint and user interface. A graphical query editor (SPARNatural) assists users with no SPARQL knowledge in constructing their queries by selecting triple elements from dropdown menus and other widgets. We also present a benchmark comparison of query response times on 10 selected SPARQL queries run against three different endpoint/triplestore implementations. </p>
<p><a class="reference external" href="https://zenodo.org/records/14792534">https://zenodo.org/records/14792534</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14792534">https://doi.org/10.5281/zenodo.14792534</a></p>
</section>
<hr class="docutils" />
<section id="interactive-bioimage-analysis-workflow-with-clij-eabias-2025-training-event">
<h2>Interactive Bioimage Analysis Workflow with CLIJ (&#64;EABIAS 2025 training event)<a class="headerlink" href="#interactive-bioimage-analysis-workflow-with-clij-eabias-2025-training-event" title="Link to this heading">#</a></h2>
<p>Wei-Chen Chu</p>
<p>Published 2025-03-23</p>
<p>Licensed CC-BY-4.0</p>
<p>Presentation file used in the EABIAS training event: EABIAS/2025-ImageJ-Micro-Image-Analysis-and-Programming_Taipei (Lesson_04)Video Recording (in Mandarin): <a class="reference external" href="https://www.youtube.com/watch?v=uheSMSENnzE">https://www.youtube.com/watch?v=uheSMSENnzE</a></p>
<p><a class="reference external" href="https://zenodo.org/records/15070246">https://zenodo.org/records/15070246</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15070246">https://doi.org/10.5281/zenodo.15070246</a></p>
</section>
<hr class="docutils" />
<section id="interactive-image-data-flow-graphs">
<h2>Interactive Image Data Flow Graphs<a class="headerlink" href="#interactive-image-data-flow-graphs" title="Link to this heading">#</a></h2>
<p>Martin Schätz</p>
<p>Published 2022-10-17</p>
<p>Licensed CC-BY-4.0</p>
<p>The slides were presented during the Macro programming with ImageJ workshop (<a class="reference external" href="https://www.16mcm.cz/programme/#workshops">https://www.16mcm.cz/programme/#workshops</a>) which was part of the 16th Multinational Congress on Microscopy. It is a collection and “reshuffle” of slides originally made by Robert Haase on topics from Image Analysis in general up to User-friendly GPU-accelerated bio-image analysis and CLIJ2.</p>
<p><a class="reference external" href="https://zenodo.org/records/7215114">https://zenodo.org/records/7215114</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7215114">https://doi.org/10.5281/zenodo.7215114</a></p>
</section>
<hr class="docutils" />
<section id="intravital-microscopy-contrasting-agents-for-application-database">
<h2>Intravital microscopy contrasting agents for application - Database<a class="headerlink" href="#intravital-microscopy-contrasting-agents-for-application-database" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-06-19</p>
<p>Licensed CC-BY-4.0</p>
<p>This is a set of databases containing published use of substances which can be applied to rodents in order to contrast specific structures for optical intravital microscopy.
The first dataset contains applied final dosages, calculated for 25g-mice, as well as the orignally published amounts, concentrations and application routes of agents directly applied into the target organism.
The second dataset contains dosages and cell numbers for the external contrastation and subsequent application of cells into the target organism.
Filtering possible for organ system and contrasted structure/cell type in both datasets, substance class and fluorescent detection windows can be filtered in the dataset for direct agent application.
Source publications are listed by DOI.
 </p>
<p><a class="reference external" href="https://zenodo.org/records/12166710">https://zenodo.org/records/12166710</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12166710">https://doi.org/10.5281/zenodo.12166710</a></p>
</section>
<hr class="docutils" />
<section id="introducing-omero-vitessce-an-omero-web-plugin-for-multi-modal-data">
<h2>Introducing OMERO-vitessce: an <a class="reference external" href="http://OMERO.web">OMERO.web</a> plugin for multi-modal data<a class="headerlink" href="#introducing-omero-vitessce-an-omero-web-plugin-for-multi-modal-data" title="Link to this heading">#</a></h2>
<p>Michele Bortolomeazzi, Christian Schmidt, Jan-Philipp Mallm</p>
<p>Published 2025-02-07</p>
<p>Licensed CC-BY-4.0</p>
<p>omero-vitessce: an <a class="reference external" href="http://OMERO.web">OMERO.web</a> plugin for multi-modal data viewing.
OMERO is the most used research data management system (RDM) in the bioimaging domain, and has been adopted as a centralized RDM solution by several academic and research institutions. A main reason for this is the ability to directly view and annotate images from a web-based interface. However, this feature of OMERO is currently underpowered for the visualization of very large or multimodal datasets. These datasets, are becoming a more and more common foundation for biological and biomedical studies, due to the recent developments in imaging, and sequencing technologies which enabled their application to spatial-omics. In order to begin to provide this multimodal-data capability to OMERO, we developed omero-vitessce (<a class="github reference external" href="https://github.com/NFDI4BIOIMAGE/omero-vitessce/tree/main">NFDI4BIOIMAGE/omero-vitessce</a>), a new <a class="reference external" href="http://OMERO.web">OMERO.web</a> plugin for viewing data stored in OMERO with the Vitessce (<a class="reference external" href="http://vitessce.io/">http://vitessce.io/</a>) multimodal data viewer. omero-vitessce can be installed as an <a class="reference external" href="http://OMERO.web">OMERO.web</a> plugin with PiPy (<a class="reference external" href="https://pypi.org/project/omero-vitessce/">https://pypi.org/project/omero-vitessce/</a>), and allows users to set up interactive visualizations of their images of cells and tissues through interactive plots which are directly linked to the image. This enables the visual exploration of bioimage-analysis results and of multimodal data such as those generated through spatial-omics experiments. The data visualization is highly customizable and can be configured not only through custom configuration files, but also with the graphical interface provided by the plugin, thus making omero-vitessce a highly user-friendly solution for multimodal data viewing. most biological datasets. We plan to extend the interoperability of omero-vitessce with the OME-NGFF and SpatialData file formats to leverage the efficiency of these cloud optimized formats.
The three files in this Zenodo Record are all the same poster saved in different format all with high resolution images.</p>
<p><a class="reference external" href="https://zenodo.org/records/14832855">https://zenodo.org/records/14832855</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14832855">https://doi.org/10.5281/zenodo.14832855</a></p>
</section>
<hr class="docutils" />
<section id="introduction-to-omero-frankfurt-online">
<h2>Introduction to OMERO - Frankfurt - online<a class="headerlink" href="#introduction-to-omero-frankfurt-online" title="Link to this heading">#</a></h2>
<p>Michele Bortolomeazzi, Tom Boissonnet</p>
<p>Published 2025-04-05</p>
<p>Licensed CC-BY-4.0</p>
<p>These slides were presented during an online introductory session to OMERO for the UB Frankfurt.
The two-hour session consisted of a first part highlighting the benefits that image data management brings to the lab. The second part showcased image analysis workflows with a Fiji macro and a Python notebook.
 </p>
<p><a class="reference external" href="https://zenodo.org/records/15152576">https://zenodo.org/records/15152576</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15152576">https://doi.org/10.5281/zenodo.15152576</a></p>
</section>
<hr class="docutils" />
<section id="introduction-to-research-data-management-and-open-research">
<h2>Introduction to Research Data Management and Open Research<a class="headerlink" href="#introduction-to-research-data-management-and-open-research" title="Link to this heading">#</a></h2>
<p>Shanmugasundaram</p>
<p>Published 2024-05-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Introduction to RDM primarily for researchers. Can be seen as primer to all other materials in this catalogue.</p>
<p>Tags: Research Data Management, Open Science</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/4778265">https://zenodo.org/records/4778265</a></p>
</section>
<hr class="docutils" />
<section id="introduction-to-light-microscopy-widefield-microscopy">
<h2>Introduction to light-microscopy / Widefield microscopy<a class="headerlink" href="#introduction-to-light-microscopy-widefield-microscopy" title="Link to this heading">#</a></h2>
<p>Thomas Laurent</p>
<p>Published 2022-05-10</p>
<p>Licensed OTHER-AT</p>
<p>This is a short introduction to light-microscopy, illustrated with widefield microscopy.</p>
<p>It introduces :</p>
<ul class="simple">
<li><p>upright and inverted widefield microscopes</p></li>
<li><p>the transmitted and fluorescent light-path</p></li>
</ul>
<p>- contrasting methods (optical and at the sample level)</p>
<ul class="simple">
<li><p>the molecular principle of fluorescence (Perrin-Jablonski)</p></li>
<li><p>objective, resolution and limitations of the method (diffraction, diffusion/scattering)</p></li>
</ul>
<p>In addition to the PPT (with few animations), a lighter PDF version is provided for preview in Zenodo.</p>
<p> </p>
<p>Illustrations are mostly extracted from the ThermoFisher Molecular Probes School of Fluorescence educator packet and from the course material from Micron Facility in Oxford.</p>
<p>As stated in the presentation, illustrations are copyrighted but can be reproduced provided the original attribution is conserved.</p>
<p><a class="reference external" href="https://zenodo.org/records/6535296">https://zenodo.org/records/6535296</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6535296">https://doi.org/10.5281/zenodo.6535296</a></p>
</section>
<hr class="docutils" />
<section id="key-value-pair-template-for-annotation-in-omero-for-light-microscopy-data-acquired-with-axioscan7-core-facility-cellular-imaging-cfci">
<h2>Key-Value pair template for annotation in OMERO for light microscopy data acquired with AxioScan7 - Core Facility Cellular Imaging (CFCI)<a class="headerlink" href="#key-value-pair-template-for-annotation-in-omero-for-light-microscopy-data-acquired-with-axioscan7-core-facility-cellular-imaging-cfci" title="Link to this heading">#</a></h2>
<p>Silke Tulok, Anja Nobst, Anett Jannasch, Tom Boissonnet, Gunar Fabig</p>
<p>Published 2024-06-28</p>
<p>Licensed CC-BY-4.0</p>
<p>This Key-Value pair template is used for the data documentation during imaging experiments and the later data annotation in OMERO. It is tailored for the usage and image acquisition at the slide scanning system Zeiss AxioScan 7 in the Core Facility Cellular Imaging (CFCI). It contains important metadata of the imaging experiment, which are not saved in the corresponding imaging files. All users of the Core Facility Cellular Imaging are trained to use that file to document their imaging parameters directly during the data acquisition with the possibility for a later upload to OMERO. Furthermore, there is a corresponding public example image used in the publication “Setting up an institutional OMERO environment for bioimage data: perspectives from both facility staff and users” and is available here:
<a class="reference external" href="https://omero.med.tu-dresden.de/webclient/?show=image-33248">https://omero.med.tu-dresden.de/webclient/?show=image-33248</a>
This template was developed by the CFCI staff during the setup and usage of the AxioScan 7 and is based on the REMBI recommendations (<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8606015">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8606015</a>).
With this template it is possible to create a csv-file, that can be used to annotate an image or dataset in OMERO using the annotation script (<a class="github reference external" href="https://github.com/ome/omero-scripts/blob/develop/omero/annotation_scripts/">ome/omero-scripts</a>).
How to use:</p>
<p>fill the template sheet  with your metadata
select and copy the data range containing the Keys and Values
open a new excel sheet and paste transpose in cell A1 
Important: cell A1 contains always the name ‘dataset’ and cell A2 contains the exact name of the image/dataset, which should be annotated in OMERO
save the new excel sheet in csv-file (comma separated values) format</p>
<p>An example can be seen in sheet 3 ‘csv_AxioScan’.
Important note: The code has to be 8-Bit UCS transformation format (UTF-8) otherwise several characters (for example µ, %,°) might be not able to decode by the annotation script. We encountered this issue with old Microsoft-Office versions (MS Office 2016). 
Note: By filling the values in the excel sheet, avoid the usage of comma as decimal delimiter.
See cross reference:
10.5281/zenodo.12547566 Key-Value pair template for annotation of datasets in OMERO for light- and electron microscopy data within the research group of Prof. Mueller-Reichert
10.5281/zenodo.12546808 Key-Value pair template for annotation of datasets in OMERO (PERIKLES study)</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/12578084">https://zenodo.org/records/12578084</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12578084">https://doi.org/10.5281/zenodo.12578084</a></p>
</section>
<hr class="docutils" />
<section id="key-value-pair-template-for-annotation-of-datasets-in-omero-perikles-study">
<h2>Key-Value pair template for annotation of datasets in OMERO (PERIKLES study)<a class="headerlink" href="#key-value-pair-template-for-annotation-of-datasets-in-omero-perikles-study" title="Link to this heading">#</a></h2>
<p>Anett Jannasch, Silke Tulok, Vanessa Aphaia Fiona Fuchs, Tom Boissonnet, Christian Schmidt, Michele Bortolomeazzi, Gunar Fabig, Chukwuebuka Okafornta</p>
<p>Published 2024-06-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This is a Key-Value pair template used for the annotation of datasets in OMERO. It is tailored for a research study (PERIKLES project) on the biocompatibility of newly designed biomaterials out of pericardial tissue for cardiovascular substitutes (<a class="reference external" href="https://doi.org/10.1063/5.0182672">https://doi.org/10.1063/5.0182672</a>) conducted in the research department of Cardiac Surgery at the Faculty of Medicine Carl Gustav Carus at the Technische Universität Dresden . A corresponding public example dataset is used in the publication “Setting up an institutional OMERO environment for bioimage data: perspectives from both facility staff and users” and is available here
(<a class="reference external" href="https://omero.med.tu-dresden.de/webclient/?show=dataset-1557">https://omero.med.tu-dresden.de/webclient/?show=dataset-1557</a>).
The template is based on the REMBI recommendations (<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8606015">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8606015</a>) and it was developed during the PoL-Bio-Image Analysis Symposium in Dresden Aug 28th- Sept 1th 2023. 
With this template it is possible to create a csv-file, that can be used to annotate a dataset in OMERO using the annotation script (<a class="github reference external" href="https://github.com/ome/omero-scripts/blob/develop/omero/annotation_scripts/">ome/omero-scripts</a>).
How to use:
select and copy the data range containing Keys and Values
open a new excel sheet and paste transpose in column B1
type in A1 ‘dataset’
insert in A2 the exact name of the dataset, which should be annotated in OMERO
save the new excel sheet in csv- (comma seperated values) file format</p>
<p>Example can be seen in sheet 1 ‘csv import’. Important note; the code has to be 8-Bit UCS transformation format (UTF-8) otherwise several characters (for example µ, %,°) might not be able to decode by the annotation script. We encountered this issue with old Microsoft Office versions (e.g. MS Office 2016). 
Note: By filling the values in the excel sheet, avoid the usage of decimal delimiter.
 
See cross reference:
10.5281/zenodo.12547566 Key-Value pair template for annotation of datasets in OMERO (light- and electron microscopy data within the research group of Prof. Mueller-Reichert)
10.5281/zenodo.12578084 Key-Value pair template for annotation in OMERO for light microscopy data acquired with AxioScan7 - Core Facility Cellular Imaging (CFCI)</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/12546808">https://zenodo.org/records/12546808</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12546808">https://doi.org/10.5281/zenodo.12546808</a></p>
</section>
<hr class="docutils" />
<section id="key-value-pair-template-for-annotation-of-datasets-in-omero-for-light-and-electron-microscopy-data-within-the-research-group-of-prof-muller-reichert">
<h2>Key-Value pair template for annotation of datasets in OMERO for light- and electron microscopy data within the research group of Prof. Müller-Reichert<a class="headerlink" href="#key-value-pair-template-for-annotation-of-datasets-in-omero-for-light-and-electron-microscopy-data-within-the-research-group-of-prof-muller-reichert" title="Link to this heading">#</a></h2>
<p>Gunar Fabig, Anett Jannasch, Chukwuebuka Okafornta, Tom Boissonnet, Christian Schmidt, Michele Bortolomeazzi, Vanessa Aphaia Fiona Fuchs, Maria Koeckert, Aayush Poddar, Martin Vogel, Hanna-Margareta Schwarzbach, Andy Vogelsang, Michael Gerlach, Anja Nobst, Thomas Müller-Reichert, Silke Tulok</p>
<p>Published 2024-06-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This are a two Key-Value pair templates used for the annotation of datasets in OMERO. They are tailored for light- and electron microcopy data for all research projects of the research group of Prof. T. Mueller-Reichert.  All members of the Core Facility Cellular Imaging agreed for using these templates to annotate data in OMERO. Furthermore, there are a corresponding public example datasets used in the publication “Setting up an institutional OMERO environment for bioimage data: perspectives from both facility staff and users” and are available here:
<a class="reference external" href="https://omero.med.tu-dresden.de/webclient/?show=dataset-1552">https://omero.med.tu-dresden.de/webclient/?show=dataset-1552</a> –&gt; for lattice-light sheet microscopy
<a class="reference external" href="https://omero.med.tu-dresden.de/webclient/?show=dataset-1555--&amp;amp;gt">https://omero.med.tu-dresden.de/webclient/?show=dataset-1555–&amp;gt</a>; for electron microscopy data
That templates are based on the REMBI recommendations (<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8606015">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8606015</a>) and were developed during the PoL-Bio-Image Analysis Symposium in Dresden Aug 28th- Sept 1st in 2023 and further adapeted during the usage of OMERO. 
With every template it is possible to create a csv-file, that can be used to annotate a dataset in OMERO using the annotation script (<a class="github reference external" href="https://github.com/ome/omero-scripts/blob/develop/omero/annotation_scripts/">ome/omero-scripts</a>).
How to use:</p>
<p>fill the template with metadata
select and copy the data range containing the Keys and Values
open a new excel sheet and paste transpose in cell A1
Important: cell A1 contains always the name ‘dataset’ and cell A2 contains the exact name of the dataset, which should be annotated in OMERO
save the new excel sheet in csv-file (comma separated values) format</p>
<p>Examples can be seen in sheet 3 ‘csv_TOMO’ and sheet 5 csv_TEM’.
Important note: The code has to be 8-Bit UCS transformation format (UTF-8) otherwise several characters (for example µ, %,°) might be not able to decode by the annotation script. We encountered this issue with old Microsoft-Office versions (MS Office 2016). 
Note: By filling the values in the excel sheet, avoid the usage of comma as decimal delimiter.
See cross reference:
10.5281/zenodo.12546808 Key-Value pair template for annotation of datasets in OMERO (PERIKLES study)
10.5281/zenodo.12578084 Key-Value pair template for annotation in OMERO for light microscopy data acquired with AxioScan7 - Core Facility Cellular Imaging (CFCI)
 </p>
<p><a class="reference external" href="https://zenodo.org/records/12547566">https://zenodo.org/records/12547566</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12547566">https://doi.org/10.5281/zenodo.12547566</a></p>
</section>
<hr class="docutils" />
<section id="kollaboratives-arbeiten-und-versionskontrolle-mit-git">
<h2>Kollaboratives Arbeiten und Versionskontrolle mit Git<a class="headerlink" href="#kollaboratives-arbeiten-und-versionskontrolle-mit-git" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-04-15</p>
<p>Licensed CC-BY-4.0</p>
<p>Gemeinsames Arbeiten im Internet stellt uns vor neue Herausforderungen: Wer hat eine Datei wann hochgeladen? Wer hat zum Inhalt beigetragen? Wie kann man Inhalte zusammenfuehren, wenn mehrere Mitarbeiter gleichzeitig Aenderungen gemacht haben? Das Versionskontrollwerkzeug git stellt eine umfassende Loesung fuer solche Fragen bereit. Die Onlineplatform <a class="reference external" href="http://github.com">github.com</a> stellt nicht nur Softwareentwicklern weltweit eine git-getriebene Platform zur Verfuegung und erlaubt ihnen effektiv zusammen zu arbeiten. In diesem Workshop lernen wir:</p>
<p>Infuerung in FAIR-Prinzipien im Softwarecontext
Arbeiten mit git: Pull-requests
Aufloesen von Merge-Konflikten
Automatisiertes Archivieren von Inhalten nach <a class="reference external" href="http://Zenodo.org">Zenodo.org</a>
Eigene Webseiten auf <a class="reference external" href="http://github.io">github.io</a> publizieren</p>
<p>Tags: Research Data Management, FAIR-Principles, Git, Zenodo</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/10972692">https://zenodo.org/records/10972692</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10972692">https://doi.org/10.5281/zenodo.10972692</a></p>
</section>
<hr class="docutils" />
<section id="kriterienkatalog-fur-materialien-aus-dem-themenbereich-forschungsdatenmanagement">
<h2>Kriterienkatalog für Materialien aus dem Themenbereich Forschungsdatenmanagement<a class="headerlink" href="#kriterienkatalog-fur-materialien-aus-dem-themenbereich-forschungsdatenmanagement" title="Link to this heading">#</a></h2>
<p>Linda Zollitsch, Swantje Piotrowski</p>
<p>Published 2025-01-24</p>
<p>Licensed CC-BY-4.0</p>
<p>Im Rahmen von FDM-SH Kontor – einem Projekt, das im Kontext der AG Kompetenzentwicklung von der Landesinitiative FDM-SH durchgeführt wurde - haben wir zum Ziel, eine kuratierte Materialbasis für Fortbildungen und Schulungen zu schaffen. Dies stellte uns vor die Herausforderung, festzulegen, wie die Materialien ausgewählt werden sollen.
Dieser Kriterienkatalog ist ein Versuch, erste Qualitätskriterien (insbesondere hinsichtlich der Nachnutzbarkeit und den FAIR-Prinzipien) für Materialien auf Basis von Metadaten zu erstellen. Dabei wurde das Vorgehen des Open Science Learning Gates (<a class="reference external" href="https://zenodo.org/records/12772135">https://zenodo.org/records/12772135</a>), als Vorbild genommen. Neben dem Metadatenschema der RDA (<a class="reference external" href="https://zenodo.org/records/6769695#.YrrP9-xBybQ">https://zenodo.org/records/6769695#.YrrP9-xBybQ</a>) haben wir auf das Metadatenschema der DINI/nestor UAG Schulungen/Fortbildungen (<a class="reference external" href="https://zenodo.org/records/3760398">https://zenodo.org/records/3760398</a>) sowie das DALIA Interchange Format (<a class="reference external" href="https://zenodo.org/records/11521029">https://zenodo.org/records/11521029</a>) zurückgegriffen.</p>
<p><a class="reference external" href="https://zenodo.org/records/14729452">https://zenodo.org/records/14729452</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14729452">https://doi.org/10.5281/zenodo.14729452</a></p>
</section>
<hr class="docutils" />
<section id="leo-linking-eln-with-omero">
<h2>LEO: Linking ELN with OMERO<a class="headerlink" href="#leo-linking-eln-with-omero" title="Link to this heading">#</a></h2>
<p>Escobar Diaz Guerrero, Rodrigo</p>
<p>Published 2024-05-08</p>
<p>Licensed CC-BY-4.0</p>
<p>First updates of LEO (Linking ELN with OMERO)</p>
<p><a class="reference external" href="https://zenodo.org/records/11146807">https://zenodo.org/records/11146807</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11146807">https://doi.org/10.5281/zenodo.11146807</a></p>
</section>
<hr class="docutils" />
<section id="lmrg-image-analysis-study-fish-datasets">
<h2>LMRG Image Analysis Study - FISH datasets<a class="headerlink" href="#lmrg-image-analysis-study-fish-datasets" title="Link to this heading">#</a></h2>
<p>Kristopoher Kubow, Thomas Pengo</p>
<p>Published 2022-05-18</p>
<p>Licensed CC-BY-4.0</p>
<p>Original image files, label (ground truth) files, and PSF files used in the ABRF Light Microscopy Research Group (LMRG) image analysis study. Simulated 3D confocal fluorescence images of sub-diffraction punctate staining (fluorescence in situ hybridization (FISH) in C. elegans).</p>
<p>See <a class="github reference external" href="https://github.com/ABRFLMRG/image-analysis-study">ABRFLMRG/image-analysis-study</a> for more details.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/6560910">https://zenodo.org/records/6560910</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6560910">https://doi.org/10.5281/zenodo.6560910</a></p>
</section>
<hr class="docutils" />
<section id="lmrg-image-analysis-study-nuclei-datasets">
<h2>LMRG Image Analysis Study - nuclei datasets<a class="headerlink" href="#lmrg-image-analysis-study-nuclei-datasets" title="Link to this heading">#</a></h2>
<p>Kristopher Kubow, Thomas Pengo</p>
<p>Published 2022-05-18</p>
<p>Licensed CC-BY-4.0</p>
<p>Original image files, label (ground truth) files, and PSF files used in the ABRF Light Microscopy Research Group (LMRG) image analysis study. Simulated 3D widefield fluorescence images of nuclei.</p>
<p>See <a class="github reference external" href="https://github.com/ABRFLMRG/image-analysis-study">ABRFLMRG/image-analysis-study</a> for more details.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/6560759">https://zenodo.org/records/6560759</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6560759">https://doi.org/10.5281/zenodo.6560759</a></p>
</section>
<hr class="docutils" />
<section id="lsm-example-j-dubrulle">
<h2>LSM example J. Dubrulle<a class="headerlink" href="#lsm-example-j-dubrulle" title="Link to this heading">#</a></h2>
<p>Salama Lab Fred Hutchinson Cancer Center</p>
<p>Published 2024-12-17</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://zenodo.org/records/14510432">https://zenodo.org/records/14510432</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14510432">https://doi.org/10.5281/zenodo.14510432</a></p>
</section>
<hr class="docutils" />
<section id="lz4-compressed-imaris-ims-example-datasets">
<h2>LZ4-compressed Imaris ims example datasets.<a class="headerlink" href="#lz4-compressed-imaris-ims-example-datasets" title="Link to this heading">#</a></h2>
<p>Marco Stucchi</p>
<p>Published 2024-11-21</p>
<p>Licensed CC-BY-4.0</p>
<p>The files contained in this repository are cropped versions of Imaris demo images compressed with LZ4.</p>
<p><a class="reference external" href="https://zenodo.org/records/14197622">https://zenodo.org/records/14197622</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14197622">https://doi.org/10.5281/zenodo.14197622</a></p>
</section>
<hr class="docutils" />
<section id="large-language-models-an-introduction-for-life-scientists">
<h2>Large Language Models: An Introduction for Life Scientists<a class="headerlink" href="#large-language-models-an-introduction-for-life-scientists" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-12-12</p>
<p>Licensed CC-BY-4.0</p>
<p>This slide deck introduces Large Language Models to an audience of life-scientists. We first dive into terminology: Different kinds of Language Models and what they can be used for. The remaining slides are optional slides to allow us to dive deeper into topics such as tools for using LLMs in Science, Quality Assurance, Techniques such as Retrieval Augmented Generation and Prompt Engineering.</p>
<p>Tags: Globias, Artificial Intelligence</p>
<p><a class="reference external" href="https://zenodo.org/records/14418209">https://zenodo.org/records/14418209</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14418209">https://doi.org/10.5281/zenodo.14418209</a></p>
</section>
<hr class="docutils" />
<section id="large-tiling-confocal-acquisition-rat-brain">
<h2>Large tiling confocal acquisition (rat brain)<a class="headerlink" href="#large-tiling-confocal-acquisition-rat-brain" title="Link to this heading">#</a></h2>
<p>Julie Meystre</p>
<p>Published 2022-06-15</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: Large tiling confocal acquisition (rat brain)</p>
<p>Microscope: Zeiss LSM700</p>
<p>Microscopy data type: 108 tiles, each with 62 z-slices and 2 channels :
Channel 1: DAPI
Channel 2: cck staining</p>
<p>File format: .lsm (16-bit)</p>
<p>Image size: 1024x1024x62 (Pixel size: 0.152 x 0.152 x 1 micron), 2 channels.</p>
<p> </p>
<p>NOTE : Some tiles were annotated and used to train a StarDist3D model (<a class="reference external" href="https://doi.org/10.5281/zenodo.6645978">https://doi.org/10.5281/zenodo.6645978</a>   )</p>
<p><a class="reference external" href="https://zenodo.org/records/6646128">https://zenodo.org/records/6646128</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6646128">https://doi.org/10.5281/zenodo.6646128</a></p>
</section>
<hr class="docutils" />
<section id="laulauthom-maskfromrois-fiji-masks-from-rois-plugins-for-fiji-initial-release">
<h2>LauLauThom/MaskFromRois-Fiji: Masks from ROIs plugins for Fiji - initial release<a class="headerlink" href="#laulauthom-maskfromrois-fiji-masks-from-rois-plugins-for-fiji-initial-release" title="Link to this heading">#</a></h2>
<p>Laurent Thomas, Pierre Trehin</p>
<p>Published 2021-07-22</p>
<p>Licensed MIT</p>
<p>Fiji plugins for the creation of binary and semantic masks from ROIs in the RoiManager. Works with stacks too.</p>
<p>Installation in Fiji: activate the Rois from masks update site in Fiji.</p>
<p>See GitHub readme for the documentation.</p>
<p>Latest tested with Fiji 2.1.0/ImageJ 1.53j</p>
<p><a class="reference external" href="https://zenodo.org/records/5121890">https://zenodo.org/records/5121890</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5121890">https://doi.org/10.5281/zenodo.5121890</a></p>
</section>
<hr class="docutils" />
<section id="laulauthom-maskfromrois-fiji-v1-0-1-better-handle-cancel">
<h2>LauLauThom/MaskFromRois-Fiji: v1.0.1 - better handle “cancel”<a class="headerlink" href="#laulauthom-maskfromrois-fiji-v1-0-1-better-handle-cancel" title="Link to this heading">#</a></h2>
<p>Laurent Thomas, Pierre Trehin</p>
<p>Published 2025-02-24</p>
<p>Licensed MIT</p>
<p>Also re-uploaded the compiled FilenameGetter.py$class to the update site, to fix <a class="github reference external" href="https://github.com/LauLauThom/MaskFromRois-Fiji/issues/7">LauLauThom/MaskFromRois-Fiji#7</a></p>
<p><a class="reference external" href="https://zenodo.org/records/14917722">https://zenodo.org/records/14917722</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14917722">https://doi.org/10.5281/zenodo.14917722</a></p>
</section>
<hr class="docutils" />
<section id="learning-and-training-bio-image-analysis-in-the-age-of-ai">
<h2>Learning and Training Bio-image Analysis in the Age of AI<a class="headerlink" href="#learning-and-training-bio-image-analysis-in-the-age-of-ai" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2025-04-07</p>
<p>Licensed CC-BY-4.0</p>
<p>The advent of large language models (LLMs) such as ChatGPT changes the way we analyse images. We ask LLMs to generate code, apply it to images and spend less time on learning implementation details. This also has impact on how we learn image analysis. While coding skills are still required, we can use LLMs to explain code, make proposals how to analyse the images and yet still decide how the analysis is done.</p>
<p><a class="reference external" href="https://zenodo.org/records/15165424">https://zenodo.org/records/15165424</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15165424">https://doi.org/10.5281/zenodo.15165424</a></p>
</section>
<hr class="docutils" />
<section id="leica-lif-file-with-errors-in-channel-order-when-imported-with-bio-formats">
<h2>Leica (.lif) file with errors in channel order when imported with Bio-formats<a class="headerlink" href="#leica-lif-file-with-errors-in-channel-order-when-imported-with-bio-formats" title="Link to this heading">#</a></h2>
<p>Areli Rodriguez</p>
<p>Published 2025-02-26</p>
<p>The blue and red channels get swapped when imported with Bio-formats. Happens consistently with .lif imports in QuPath and ImageJ.</p>
<p><a class="reference external" href="https://zenodo.org/records/14933318">https://zenodo.org/records/14933318</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14933318">https://doi.org/10.5281/zenodo.14933318</a></p>
</section>
<hr class="docutils" />
<section id="leitfaden-zur-digitalen-datensparsamkeit-mit-praxisbeispielen">
<h2>Leitfaden zur digitalen Datensparsamkeit (mit Praxisbeispielen)<a class="headerlink" href="#leitfaden-zur-digitalen-datensparsamkeit-mit-praxisbeispielen" title="Link to this heading">#</a></h2>
<p>Maximilian Heber, Moritz Jakob, Matthias Landwehr, Jan Leendertse, Maximilian Müller, Gabriel Schneider, Dirk von Suchodoletz, Robert Ulrich</p>
<p>Published 2024-06-03</p>
<p>Licensed CC-BY-4.0</p>
<p>Im Zuge der stetig wachsenden Brisanz des Forschungsdatenmanagements fallen immer größere Mengen an Forschungsdaten an. Diese an sich begrüßenswerte Entwicklung führt zu technischen und organisatorischen Herausforderungen nicht nur im Bereich der Speicherung von Forschungsdaten, sondern in allen Phasen des Forschungsdatenlebenszyklus. Der vorliegende Beitrag erläutert vor diesem Hintergrund mögliche Motivationen hinter digitaler Datensparsamkeit mit Blick auf organisatorische, technische und ethische Kriterien, Datenschutz und Nachhaltigkeit. Anschließend werden vor dem Hintergrund zentraler Herausforderungen Umsetzungsvorschläge für das Vorfeld sowie den Verlauf eines Forschungsvorhabens gemacht. Zudem werden grundlegende Empfehlungen zur digitalen Datensparsamkeit ausgesprochen.
Eine kürzere Ausgabe des Leitfadens ist im Mai 2024 in der Zeitschrift o | bib erschienen: <a class="reference external" href="https://doi.org/10.5282/o-bib/6036">https://doi.org/10.5282/o-bib/6036</a>
Diese Ausgabe enthält ein zusätzliches Kapitel (4.2) mit konkreten Praxisbeispielen.
Dieser Artikel wurde ins Englische übersetzt:
Heber, M., Jakob, M., Landwehr, M., Leendertse, J., Müller, M., Schneider, G., von Suchodoletz, D., &amp; Ulrich, R. (2024). A Users’ Guide to Economical Digital Data Usage. Zenodo. <a class="reference external" href="https://doi.org/10.5281/zenodo.13752220">https://doi.org/10.5281/zenodo.13752220</a></p>
<p><a class="reference external" href="https://zenodo.org/records/11445843">https://zenodo.org/records/11445843</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11445843">https://doi.org/10.5281/zenodo.11445843</a></p>
</section>
<hr class="docutils" />
<section id="limeseg-test-datasets">
<h2>LimeSeg Test Datasets<a class="headerlink" href="#limeseg-test-datasets" title="Link to this heading">#</a></h2>
<p>Sarah Machado, Vincent Mercier, Nicolas Chiaruttini</p>
<p>Published 2018-10-27</p>
<p>Licensed CC-BY-4.0</p>
<p>Image datasets from the publication : LimeSeg: A coarse-grained lipid membrane simulation for 3D image segmentation</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Vesicles.tif: spinning-disc confocal images of giant unilamellar vesicles
HelaCell-FIBSEM.tif:&amp;nbsp;a 3D Electron&amp;nbsp;Microscopy (EM)&amp;nbsp;dataset of nearly isotropic sections of a Hela cell, acquired with a focused ion beam scanning electron microscope (FIB-SEM). Sections are aligned with TrackEm2 (doi: ), without additional preprocessing.
DrosophilaEggChamber.tif: point scanning confocal images of a Drosophila egg chamber. Channel&amp;nbsp;1: cell nuclei &amp;nbsp;stained with DAPI. Channel 2:&amp;nbsp;cell membranes visualized with fused membrane proteins Nrg::GFP and Bsg::GFP.&amp;nbsp;
</pre></div>
</div>
<p>Image metadata contains extra information including voxel sizes.</p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/1472859">https://zenodo.org/records/1472859</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.1472859">https://doi.org/10.5281/zenodo.1472859</a></p>
</section>
<hr class="docutils" />
<section id="linked-open-data-for-microbial-population-biology">
<h2>Linked (Open) Data for Microbial Population Biology<a class="headerlink" href="#linked-open-data-for-microbial-population-biology" title="Link to this heading">#</a></h2>
<p>Carsten Fortmann-Grote</p>
<p>Published 2024-03-12</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://zenodo.org/records/10808486">https://zenodo.org/records/10808486</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10808486">https://doi.org/10.5281/zenodo.10808486</a></p>
</section>
<hr class="docutils" />
<section id="liver-micrometastases-area-quantification-using-qupath-and-pixel-classifier">
<h2>Liver Micrometastases area quantification using QuPath and pixel classifier<a class="headerlink" href="#liver-micrometastases-area-quantification-using-qupath-and-pixel-classifier" title="Link to this heading">#</a></h2>
<p>Laia Simó-Riudalbas, Romain Guiet, Olivier Burri, Julien Duc, Didier Trono</p>
<p>Published 2022-05-06</p>
<p>Licensed CC-BY-4.0</p>
<p>Sample: Mouse (NSG) liver slices with human colorectal cancer cells metastases, stained with Hematoxylin &amp; Eosin. </p>
<p>Image Acquisition: Images were acquired on an Olympus VS120 Whole Slide Scanner, using a 20x objective (UPLSAPO, N.A. 0.75) and a color camera (Pike F505 Color) with an image pixel size of 0.345 microns.</p>
<p>Image Processing and Analysis: Obtained images were analyzed using the software QuPath [1] (version 0.3.2) using groovy scripts, making use of a pixel classifier to segment and measure cancer cell clusters.</p>
<p>Files :</p>
<p>Detailed_worflow.pdf : contains a detailed description of how pixel classifier was created</p>
<p>images_for_classifier_training.zip : contains all the vsi file obtained from the microscope and used for the training</p>
<p>project_for_classifier_training.zip : contains the QuPath project, with Training Image, annotations, classifiers and scripts for analysis</p>
<p>PythonCode.txt : code ran to transform output results from QuPath to final results</p>
<p> </p>
<p>[1] Bankhead, P. et al. QuPath: Open source software for digital pathology image analysis. Scientific Reports (2017). <a class="reference external" href="https://doi.org/10.1038/s41598-017-17204-5">https://doi.org/10.1038/s41598-017-17204-5</a></p>
<p><a class="reference external" href="https://zenodo.org/records/6523649">https://zenodo.org/records/6523649</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6523649">https://doi.org/10.5281/zenodo.6523649</a></p>
</section>
<hr class="docutils" />
<section id="lynsec-lymphoma-nuclear-segmentation-and-classification">
<h2>LyNSeC: Lymphoma Nuclear Segmentation and Classification<a class="headerlink" href="#lynsec-lymphoma-nuclear-segmentation-and-classification" title="Link to this heading">#</a></h2>
<p>Naji Hussein, Büttner Reinhard, Simon Adrian, Eich Marie-Lisa, Lohneis Philipp, Bozek Katarzyna</p>
<p>Published 2023-06-21</p>
<p>Licensed CC-BY-4.0</p>
<p>Over the last years, there has been large progress in automated segmentation and classification methods in histological whole slide images (WSIs) stained with hematoxylin and eosin (H&amp;E). Current state-of-the-art techniques are based on diverse datasets of H&amp;E-stained WSIs of different types of predominantly solid cancer. However, there is a lack of publicly available annotated datasets of lymphoma, which is why we generated a labeled diffuse large B-cell lymphoma dataset and denoted it LyNSeC (lymphoma nuclear segmentation and classification). LyNSeC comprises three subsets: LyNSeC 1 consists of 379 IHC images of size 512 x 512 pixels at 40x magnification. In the images, we annotated the contours of each cell nuclei and the cell class: marker-positive or marker-negative.</p>
<p>In total, LyNSeC 1 contains 87,316 annotated cell nuclei of four different cases, with 48,171 of them assigned the class negative and 39,145 positive. We included three markers in this dataset showing visually different staining patterns: cluster of differentiation 3 (CD3), Ki67 as a marker of proliferation, and erythroblast transformation-specific (EST)-related gene (ERG).</p>
<p>LyNSeC 2 and 3 contain H&amp;E-stained images of 70 different patients. LyNSeC 2 consists of 280 images and LyNSeC 3 of 40 images of size 512 x 512 pixels at 40x magnification. 65,479 and 8,452 nuclei were annotated in LyNSeC 2 and 3, respectively. In LyNSeC 3, the nuclei were also assigned a class label (tumor and non-tumor). 3,747 nuclei were identified as tumors and 4,705 as non-tumors.</p>
<p>In the annotation procedure, the contours of the H&amp;E images (LyNSeC 2 and LyNSeC 3) were annotated by two pathologists and by two students (trained by the pathologists). Annotation of the cell classes in LyNSeC 3 was done by the pathologists only. LyNSeC 1 was annotated by the two students who were additionally trained to annotate the contours and to distinguish marker-positive and marker-negative cells. The pathologists inspected and (if necessary) adjusted the LyNSeC 3 annotations.</p>
<p>The files are uploaded in ‘.npy’ format. The files of LyNSeC 1 (x_l1.npy) and LyNSeC 3 (x_l3.npy) contain five channels, respectively: the first three are the RGB channels of the images, channel 4 contains the instance maps, and channel 5 the class type maps (for LyNSeC 1 a pixel value of 1 corresponds to the class negative and 2 to the class positive, whereas in LyNSeC 3 1 corresponds to the class non-tumor and 2 to the class tumor). The files of LyNSeC 2 (x_l2.npy) have 4 channels (without the class type map).</p>
<p>Additionally, we also make our HoVer-Net-based pre-trained nuclei segmentation and classification models available (he.tar for H&amp;E images and ihc.tar for IHC images).</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8065174">https://zenodo.org/records/8065174</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8065174">https://doi.org/10.5281/zenodo.8065174</a></p>
</section>
<hr class="docutils" />
<section id="masterclasses-from-the-euro-bioimaging-evolve-mentoring-programme-2025">
<h2>Masterclasses from the Euro-Bioimaging EVOLVE Mentoring programme 2025<a class="headerlink" href="#masterclasses-from-the-euro-bioimaging-evolve-mentoring-programme-2025" title="Link to this heading">#</a></h2>
<p>Euro-BioImaging ERIC</p>
<p>Published 2025-06-26</p>
<p>Licensed CC-BY-4.0</p>
<p>EVOLVE Mentoring Masterclasses
Description:This series captures the class guides of the 2025 masterclasses from Euro‑BioImaging’s EVOLVE Mentoring Program.
Included Masterclasses:</p>
<p>Peter O’Toole – “Entrepreneurship &amp; Leadership in Imaging Core Facilities” Peter O’Toole, President of the Royal Microscopical Society and Director of the Bioscience Technology Facility (University of York), kicks off the series with a deep dive into entrepreneurial leadership. He highlights how to balance science, business, and technology, emphasizing stakeholder engagement, staff investment, cross-training, and using social media to boost visibility and unlock funding.
Ilaria Testa – “Interdisciplinary Science, SMART Microscopy &amp; Team Building” Professor Ilaria Testa (SciLifeLab &amp; KTH) reflects on her transition from physics to super-resolution microscopy and team leadership. Her session underscores the power of crossing disciplinary boundaries, mentorship, and innovation in live-cell imaging .
Daphna Link‑Sourani – “Leadership, Facility Management &amp; Work‑Life Balance” Dr. Daphna Link‑Sourani (Technion Human MRI Research Center) challenges hierarchical notions of leadership, advocating instead for integrity, empathy, and strategic vision. She draws on her experience establishing an MRI facility to discuss crisis management, user engagement, and balancing career demands.
Muriel Mari – “Women in Science: Normalizing, Supporting &amp; Leading”                                                                           Dr. Muriel Mari (Aarhus University) leads a powerful reflection on gender equity in science. Her masterclass goes beyond barriers—focusing on cultural shifts, inclusive leadership, and redefining success. She encourages institutions and individuals alike to move from tokenism to transformative support, and to recognize the diverse paths women take in STEM.
Sylvia E. Le Dévédec – “Image Data Management &amp; FAIR Core Facilities”                                                                     Dr. Sylvia Le Dévédec (Leiden University) discusses how to integrate FAIR data principles in imaging core facilities. Drawing on her experience with high-content imaging and Open Science advocacy, she outlines actionable steps toward sustainable, reusable, and accessible data workflows.</p>
<p>Why Archive These Sessions?These masterclasses offer invaluable insights for core facility managers, imaging scientists, and team leaders in life sciences. They blend hands-on leadership strategies, technical facility growth advice, and real-world experience—making them essential viewing for professionals and institutions aiming to build sustainable, people-centred imaging infrastructures.</p>
<p><a class="reference external" href="https://zenodo.org/records/15747344">https://zenodo.org/records/15747344</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15747344">https://doi.org/10.5281/zenodo.15747344</a></p>
</section>
<hr class="docutils" />
<section id="measuring-reporter-activity-domain-in-epi-aggregates-and-gastruloids-ijm">
<h2>Measuring reporter activity domain in EPI aggregates and Gastruloids.ijm<a class="headerlink" href="#measuring-reporter-activity-domain-in-epi-aggregates-and-gastruloids-ijm" title="Link to this heading">#</a></h2>
<p>Romain Guiet, Olivier Burri, Mehmet Girgin, Matthias Lutolf</p>
<p>Published 2022-12-07</p>
<p>Licensed CC-BY-4.0</p>
<p>This imagej macro analyses the reporter intensity activity and expression domain in EPI aggregates and Gastruloids.</p>
<p><a class="reference external" href="https://zenodo.org/records/7409423">https://zenodo.org/records/7409423</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7409423">https://doi.org/10.5281/zenodo.7409423</a></p>
</section>
<hr class="docutils" />
<section id="melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations">
<h2>Melanoma Histopathology Dataset with Tissue and Nuclei Annotations<a class="headerlink" href="#melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations" title="Link to this heading">#</a></h2>
<p>Mark Schuiveling</p>
<p>Published 2025-03-19</p>
<p>Licensed CC-ZERO</p>
<p>Description:
This dataset is designed for development of deep learning models for segmentation of nuclei and tissue in melanoma H&amp;E stained histopathology. Existing nuclei segmentation models that are trained on non-melanoma specific datasets have low performance due to the ability of melanocytes to mimic other cell types, whereas existing melanoma specific models utilize older, sub-optimal techniques. Moreover, these models do not provide tissue annotations necessary for determining the localization of tumor-infiltrating lymphocytes, which may hold value for predictive and prognostic tasks. To address this, we created a melanoma specific dataset with nuclei and tissue annotations. 
Methodology:
Sample Collection:
Regions of interest (ROIs) were sampled from H&amp;E stained slides of 103 primary melanoma specimens and 102 metastatic melanoma specimens, scanned using a Hamamatsu scanner at 40× magnification (0.23 μm per pixel). All slides were obtained from regular diagnostic procedures.From each specimen, a 40× magnified ROI of 1024×1024 pixels was selected for annotation. Additionally, a context ROI of 5120×5120 pixels was sampled to provide information about the broader context for the annotation process. Selection was performed by a trained medical expert (M.S.) and subsequently verified by a dermatopathologist (W.B.). Manual ROI selection ensured the inclusion of diverse tissue and nuclei types.
Annotation Process:</p>
<p>Nuclei segmentationNuclei segmentations were generated using Hover-Net pretrained on the PanNuke dataset. Manual annotation adjustments were performed by author M.S. using QuPath, with the following nuclei categories: tumor, stroma, vascular endothelium, histiocyte, melanophage, lymphocyte, plasma cell, neutrophil, apoptotic cell, and epithelium. All annotations were reviewed and corrected, where needed, by a dermatopathologist (W.B.).
Tissue segmentationTissue segmentations were created manually using QuPath by M.S., with the following categories: tumor, stroma, epidermis, necrosis, blood vessel, and background. Annotations were reviewed and corrected, where needed, by a dermatopathologist (W.B.).</p>
<p>Quality Control: To assess the reliability of the annotations, intra- and interobserver agreement (by pathologist G.B.) were determined on 12 randomly selected ROIs.</p>
<p>Nuclei segmentationThe intraobserver overall precision was 84.89%, with a recall of 86.45%, and an F1 score of 85.66%. Interobserver overall precision was 80.34%, with a recall of 80.62%, and an F1 score of 80.20%. These results are based on the sum of all true positive, false positive, and false negative counts for the 12 ROIs.
Tissue segmentationThe DICE score was determined on the same 12 randomly selected ROIs. The average intraobserver DICE score was 0.90, and the interobserver DICE score was also 0.90.</p>
<p> 
Version 3:Removed sample “training_set_metastatic_roi_103” due to inconsistencies in annotation file.
Version 4:Sample training_set_metastatic_roi_088 missed one color annotation for a nuclei_apoptosis in the geojson file rendering it qupath uncompatible. This is fixed in the new version. 
Version 5:Addition of correct sample of training_set_metastatic_roi_103” after deadline of panoptic segmentation of nuclei and tissue in advanced melanoma challenge test phase. </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/15050523">https://zenodo.org/records/15050523</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15050523">https://doi.org/10.5281/zenodo.15050523</a></p>
</section>
<hr class="docutils" />
<section id="membrain-seg-training-data">
<h2>MemBrain-seg training data<a class="headerlink" href="#membrain-seg-training-data" title="Link to this heading">#</a></h2>
<p>Lorenz Lamm</p>
<p>Published 2023-03-16</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset contains training data for segmenting membranes in cryo-electron tomograms.</p>
<p>More details will follow.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7739793">https://zenodo.org/records/7739793</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7739793">https://doi.org/10.5281/zenodo.7739793</a></p>
</section>
<hr class="docutils" />
<section id="memorandum-of-understanding-of-nfdi-consortia-from-earth-chemical-and-life-sciences-to-support-a-network-called-the-geo-chem-life-science-helpdesk-cluster">
<h2>Memorandum of Understanding of NFDI consortia from Earth-, Chemical and Life Sciences to support a network called the Geo-Chem-Life Science Helpdesk Cluster<a class="headerlink" href="#memorandum-of-understanding-of-nfdi-consortia-from-earth-chemical-and-life-sciences-to-support-a-network-called-the-geo-chem-life-science-helpdesk-cluster" title="Link to this heading">#</a></h2>
<p>Lars Bernard, Maike Brück, Christian Busse, Judith Engel, Jan Eufinger, Frank Ewert, Juliane Fluck, Konrad Förstner, Julia Fürst, Holger Gauza, Klaus Getzlaff, Glöckner, Frank Oliver, Johannes Hunold, Oliver Koepler, Ksenia Krooß, Birte Lindstädt, McHardy, Alice C., Hela Mehrtens, Elena Rey-Mazon, Marcus Schmidt, Isabel Schober, Annett Schröter, Oliver Stegle, Christoph Steinbeck, Feray Steinhart, von Suchodoletz, Dirk, Stefanie Weidtkamp-Peters, Jens Wendt, Conni Wetzker</p>
<p>Published 2025-04-02</p>
<p>Licensed CC-BY-4.0</p>
<p>In a Memorandum of Understanding, the undersigned consortia agree to work together to enhance their support capabilities (helpdesks) to meet the needs of interdisciplinary research in Earth-, Chemical and Life Sciences.</p>
<p><a class="reference external" href="https://zenodo.org/records/15065070">https://zenodo.org/records/15065070</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15065070">https://doi.org/10.5281/zenodo.15065070</a></p>
</section>
<hr class="docutils" />
<section id="metadata-annotation-workflow-for-omero-with-tabbles">
<h2>Metadata Annotation Workflow for OMERO with Tabbles<a class="headerlink" href="#metadata-annotation-workflow-for-omero-with-tabbles" title="Link to this heading">#</a></h2>
<p>Wendt Jens</p>
<p>Published 2023-09-04</p>
<p>Licensed CC-BY-4.0</p>
<p>Short presentation given at at PoL BioImage Analysis Symposium Dresden 2023</p>
<p><a class="reference external" href="https://zenodo.org/records/8314968">https://zenodo.org/records/8314968</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8314968">https://doi.org/10.5281/zenodo.8314968</a></p>
</section>
<hr class="docutils" />
<section id="metadata-in-bioimaging">
<h2>Metadata in Bioimaging<a class="headerlink" href="#metadata-in-bioimaging" title="Link to this heading">#</a></h2>
<p>Josh Moore, Susanne Kunis</p>
<p>Published 2025-03-25</p>
<p>Licensed CC-BY-4.0</p>
<p>Presentation given to the Search &amp; Harvesting workgroup of the Metadata section of NFDI on March 25th, 2025</p>
<p><a class="reference external" href="https://zenodo.org/records/15083018">https://zenodo.org/records/15083018</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15083018">https://doi.org/10.5281/zenodo.15083018</a></p>
</section>
<hr class="docutils" />
<section id="microsam-talks">
<h2>MicroSam-Talks<a class="headerlink" href="#microsam-talks" title="Link to this heading">#</a></h2>
<p>Constantin Pape</p>
<p>Published 2024-05-23</p>
<p>Licensed CC-BY-4.0</p>
<p>Talks about Segment Anything for Microscopy: <a class="github reference external" href="https://github.com/computational-cell-analytics/micro-sam">computational-cell-analytics/micro-sam</a>.
Currently contains slides for two talks:</p>
<p>Overview of Segment Anythign for Microscopy given at the SWISSBIAS online meeting in April 2024
Talk about vision foundation models and Segment Anything for Microscopy given at Human Technopole as part of the EMBO Deep Learning Course in May 2024</p>
<p>Tags: Bioimage Analysis, Artificial Intelligence</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/11265038">https://zenodo.org/records/11265038</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11265038">https://doi.org/10.5281/zenodo.11265038</a></p>
</section>
<hr class="docutils" />
<section id="modular-training-resources-for-bioimage-analysis">
<h2>Modular training resources for bioimage analysis<a class="headerlink" href="#modular-training-resources-for-bioimage-analysis" title="Link to this heading">#</a></h2>
<p>Christian Tischer, Antonio Politi, Tim-Oliver Buchholz, Elnaz Fazeli, Nicola Gritti, Aliaksandr Halavatyi, Gonzalez Tirado, Sebastian, Julian Hennies, Toby Hodges, Arif Khan, Dominik Kutra, Stefania Marcotti, Bugra Oezdemir, Felix Schneider, Martin Schorb, Anniek Stokkermans, Yi Sun, Nima Vakili</p>
<p>Published 2025-01-21</p>
<p>Licensed CC-BY-4.0</p>
<p>The newly developed image data formats course was taught for the first time: <a class="github reference external" href="https://github.com/NEUBIAS/training-resources/blob/master/courses/2025_01_EMBL_image_data_formats.md">NEUBIAS/training-resources</a></p>
<p><a class="reference external" href="https://zenodo.org/records/14710820">https://zenodo.org/records/14710820</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14710820">https://doi.org/10.5281/zenodo.14710820</a></p>
</section>
<hr class="docutils" />
<section id="morphological-analysis-of-neural-cells-with-weka-and-snt-fiji-plugins">
<h2>Morphological analysis of neural cells with WEKA and SNT Fiji plugins<a class="headerlink" href="#morphological-analysis-of-neural-cells-with-weka-and-snt-fiji-plugins" title="Link to this heading">#</a></h2>
<p>Daniel Waiger</p>
<p>Published 2022-07-14</p>
<p>Licensed CC-BY-4.0</p>
<p>A simple workflow to detect Soma and neurite paths, from light microscopy datasets.</p>
<p>Using open-source tools for beginners.</p>
<p><a class="reference external" href="https://zenodo.org/records/6834214">https://zenodo.org/records/6834214</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6834214">https://doi.org/10.5281/zenodo.6834214</a></p>
</section>
<hr class="docutils" />
<section id="multi-template-matching-for-object-detection-slides">
<h2>Multi-Template-Matching for object-detection (slides)<a class="headerlink" href="#multi-template-matching-for-object-detection-slides" title="Link to this heading">#</a></h2>
<p>Laurent Thomas</p>
<p>Published 2022-05-16</p>
<p>Licensed CC-BY-4.0</p>
<p>This presentations describes Multi-Template-Matching, a novel method extending on template-matching for object-detection in images.</p>
<p>The project was part of the PhD project of Laurent Thomas between 2017 and 2020, under supervision of Jochen Gehrig. The project was hosted at ACQUIFER Imaging with collaboration of the medical university of Heidelberg, and part of the ImageInLife Horizon2020 ITN (PhD program). </p>
<p><a class="reference external" href="https://zenodo.org/records/6554166">https://zenodo.org/records/6554166</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6554166">https://doi.org/10.5281/zenodo.6554166</a></p>
</section>
<hr class="docutils" />
<section id="multiplexed-histology-of-covid-19-post-mortem-lung-samples-control-case-1-fov1">
<h2>Multiplexed histology of COVID-19 post-mortem lung samples - CONTROL CASE 1 FOV1<a class="headerlink" href="#multiplexed-histology-of-covid-19-post-mortem-lung-samples-control-case-1-fov1" title="Link to this heading">#</a></h2>
<p>Anna Pascual Reguant, Ronja Mothes, Helena Radbruch, Anja E. Hauser</p>
<p>Published 2022-12-16</p>
<p>Licensed CC-BY-4.0</p>
<p>Image-based data set of a post-mortem lung sample from a non-COVID-related pneumonia donor (CONTROL CASE 1, FOV1)</p>
<p>Each image shows the same field of view (FOV), sequentially stained with the depicted fluorescence-labelled antibodies, including surface proteins, intracellular proteins and transcription factors. Images contain 2024 x 2024 pixels and are generated using an inverted wide-field fluorescence microscope with a 20x objective, a lateral resolution of 325 nm and an axial resolution above 5 µm. Images have been normalized and intensities adjusted.</p>
<p><a class="reference external" href="https://zenodo.org/records/7447491">https://zenodo.org/records/7447491</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7447491">https://doi.org/10.5281/zenodo.7447491</a></p>
</section>
<hr class="docutils" />
<section id="my-journey-through-bioimage-analysis-teaching-methods-from-classroom-to-cloud">
<h2>My Journey Through Bioimage Analysis Teaching Methods From Classroom to Cloud<a class="headerlink" href="#my-journey-through-bioimage-analysis-teaching-methods-from-classroom-to-cloud" title="Link to this heading">#</a></h2>
<p>Elnaz Fazeli</p>
<p>Published 2024-02-19</p>
<p>Licensed CC-BY-4.0</p>
<p>In these slides I introducemy journey through teaching bioimage analysis courses in different formats, from in person courses to online material. I have an overview of different training formats and comparing these for different audiences. </p>
<p>Tags: Teaching</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/10679054">https://zenodo.org/records/10679054</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10679054">https://doi.org/10.5281/zenodo.10679054</a></p>
</section>
<hr class="docutils" />
<section id="nfdi4bioimage">
<h2>NFDI4BIOIMAGE<a class="headerlink" href="#nfdi4bioimage" title="Link to this heading">#</a></h2>
<p>Carsten Fortmann-Grote</p>
<p>Published 2024-04-22</p>
<p>Licensed CC-BY-4.0</p>
<p>This presentation was given at the 2nd MPG-NFDI Workshop on April 18th.</p>
<p><a class="reference external" href="https://zenodo.org/records/11031747">https://zenodo.org/records/11031747</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11031747">https://doi.org/10.5281/zenodo.11031747</a></p>
</section>
<hr class="docutils" />
<section id="nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis-online-kick-off-2023">
<h2>NFDI4BIOIMAGE - National Research Data Infrastructure for Microscopy and BioImage Analysis - Online Kick-Off 2023<a class="headerlink" href="#nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis-online-kick-off-2023" title="Link to this heading">#</a></h2>
<p>Stefanie Weidtkamp-Peters</p>
<p>Licensed CC-BY-4.0</p>
<p>NFDI4BIOIMAGE core mission, bioimage data challenge, task areas, FAIR bioimage workflows.</p>
<p>Tags: Research Data Management, FAIR-Principles, Bioimage Analysis, Nfdi4Bioimage</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8070038">https://doi.org/10.5281/zenodo.8070038</a></p>
<p><a class="reference external" href="https://zenodo.org/records/8070038">https://zenodo.org/records/8070038</a></p>
</section>
<hr class="docutils" />
<section id="nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis-conference-talk-the-pelagic-imaging-consortium-meets-helmholtz-imaging-5-10-2023-hamburg">
<h2>NFDI4BIOIMAGE - National Research Data Infrastructure for Microscopy and BioImage Analysis [conference talk: The Pelagic Imaging Consortium meets Helmholtz Imaging, 5.10.2023, Hamburg]<a class="headerlink" href="#nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis-conference-talk-the-pelagic-imaging-consortium-meets-helmholtz-imaging-5-10-2023-hamburg" title="Link to this heading">#</a></h2>
<p>Riccardo Massei</p>
<p>Licensed CC-BY-4.0</p>
<p>NFDI4BIOIMAGE is a consortium within the framework of the National Research Data Infrastructure (NFDI) in Germany. In this talk, the consortium and the contribution to the work programme by the Helmholtz Centre for Environmental Research (UFZ) in Leipzig are outlined.</p>
<p>Tags: Research Data Management, Bioimage Analysis, Nfdi4Bioimage</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.8414318">https://zenodo.org/doi/10.5281/zenodo.8414318</a></p>
</section>
<hr class="docutils" />
<section id="nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis">
<h2>NFDI4BIOIMAGE - National Research Data Infrastructure for Microscopy and Bioimage Analysis<a class="headerlink" href="#nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis" title="Link to this heading">#</a></h2>
<p>NFDI4BIOIMAGE Consortium</p>
<p>Published 2024-08-07</p>
<p>Licensed CC-BY-4.0</p>
<p>Bioimaging refers to a collection of methods to visualize the internal structures and mechanisms of living organisms. The fundamental tool, the microscope, has enabled seminal discoveries like that of the cell as the smallest unit of life, and continues to expand our understanding of biological processes. Today, we can follow the interaction of single molecules within nanoseconds in a living cell, and the development of complete small organisms like fish and flies over several days starting from the fertilized egg. Each image pixel encodes multiple spatiotemporal and spectral dimensions, compounding the massive volume and complexity of bioimage data. Proper handling of this data is indispensable for analysis and its lack has become a growing hindrance for the many disciplines of the life and biomedical sciences relying on bioimaging. No single domain has the expertise to tackle this bottleneck alone.
As a method-specific consortium, NFDI4BIOMAGE seeks to address these issues, enabling bioimaging data to be shared and re-used like they are acquired, i.e., independently of disciplinary boundaries. We will provide solutions for exploiting the full information content of bioimage data and enable new discoveries through sharing and re-analysis. Our RDM strategy is based on a robust needs analysis that derives not only from a community survey but also from over a decade of experience in German BioImaging, the German Society for Microscopy and Image Analysis. It considers the entire lifecycle of bioimaging data, from acquisition to archiving, including analysis and enabling re-use. A foundational element of this strategy is the definition of a common, cloud-compatible, and interoperable digital object that bundles binary images with their descriptive and provenance metadata. With members from plant biology to neuroscience, NFDI4BIOIMAGE will champion the standardization of bioimage data to create a framework that answers discipline-specific needs while ensuring communication and interoperability with data types and RDM systems across domains. Integration of bioimage data with, e.g., omics data as the basis for spatial omics, holds great promise for fields such as cancer medicine. Unlocking the full potential of bioimage data will rely on the development and broad availability of exceptional analysis tools and training sets. NFDI4BIOIMAGE will make these accessible and usable including cutting-edge AI-based methods in scalable cloud environments. NFDI4BIOIMAGE intersects with multiple NFDI consortia, most prominently with GHGA for linking image and genomics data and with DataPLANT on the definition of FAIR data objects. Last but not least, NFDI4BIOIMAGE is internationally well connected and represents the opportunity for German scientists to keep path with and have a voice in several international initiatives focusing on the FAIRification of bioimage data as one of the main challenges for the advancement of knowledge in the life and biomedical sciences.</p>
<p><a class="reference external" href="https://zenodo.org/records/13168693">https://zenodo.org/records/13168693</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13168693">https://doi.org/10.5281/zenodo.13168693</a></p>
</section>
<hr class="docutils" />
<section id="nfdi4bioimage-data-management-illustrations-by-henning-falk">
<h2>NFDI4BIOIMAGE data management illustrations by Henning Falk<a class="headerlink" href="#nfdi4bioimage-data-management-illustrations-by-henning-falk" title="Link to this heading">#</a></h2>
<p>NFDI4BIOIMAGE Consortium</p>
<p>Published 2024-11-29</p>
<p>Licensed CC-BY-4.0</p>
<p>These illustrations were contracted by the Heinrich Heine University Düsseldorf in the frame of the consortium NFDI4BIOIMAGE from Henning Falk for the purpose of education and public outreach. The illustrations are free to use under a CC-BY 4.0 license.AttributionPlease include an attribution similar to: “Data annoation matters”, NFDI4BIOIMAGE Consortium (2024): NFDI4BIOIMAGE data management illustrations by Henning Falk, Zenodo, <a class="reference external" href="https://doi.org/10.5281/zenodo.14186100">https://doi.org/10.5281/zenodo.14186100</a>, is used under a CC-BY 4.0 license. Modifications to this illustration include cropping.
 </p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/14186101">https://zenodo.org/records/14186101</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14186101">https://doi.org/10.5281/zenodo.14186101</a></p>
</section>
<hr class="docutils" />
<section id="nfdi4bioimage-ta3-hackathon-uoc-2023-cologne-hackathon-2023-github-repository">
<h2>NFDI4Bioimage - TA3-Hackathon - UoC-2023 (Cologne-Hackathon-2023, GitHub repository)<a class="headerlink" href="#nfdi4bioimage-ta3-hackathon-uoc-2023-cologne-hackathon-2023-github-repository" title="Link to this heading">#</a></h2>
<p>Mohamed Abdrabbou, Mehrnaz Babaki, Tom Boissonnet, Michele Bortolomeazzi, Eik Dahms, Vanessa Fuchs, A. F. Moritz Hoevels, Niraj Kandpal, Christoph Möhl, Joshua A. Moore, Astrid Schauss, Andrea Schrader, Torsten Stöter, Julia Thönnißen, Monica Valencia-S., H. Lukas Weil, Jens Wendt, Peter Zentis</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository documents the first NFDI4Bioimage - TA3-Hackathon - UoC-2023 (Cologne Hackathon), where topics like ‘Interoperability’, ‘REMBI / Mapping’, and ‘Neuroglancer (OMERO / zarr)’ were explored through collaborative discussions and workflow sessions, culminating in reports that bridge NFDI4Bioimage to DataPLANT. Funded by various DFG initiatives, this event emphasized documentation and use cases, contributing preparatory work for future interoperability projects at the 2nd de.NBI BioHackathon in Bielefeld.</p>
<p>Tags: Research Data Management, FAIR-Principles, Bioimage Analysis, Nfdi4Bioimage</p>
<p>Content type: Github Repository</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.10609770">https://zenodo.org/doi/10.5281/zenodo.10609770</a></p>
</section>
<hr class="docutils" />
<section id="nfdi4bioimage-calendar-2024-october-original-image">
<h2>NFDI4Bioimage Calendar 2024 October; original image<a class="headerlink" href="#nfdi4bioimage-calendar-2024-october-original-image" title="Link to this heading">#</a></h2>
<p>Christian Jüngst, Peter Zentis</p>
<p>Published 2024-09-25</p>
<p>Licensed CC-BY-4.0</p>
<p>Raw microscopy image from the NFDI4Bioimage calendar October 2024</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/13837146">https://zenodo.org/records/13837146</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13837146">https://doi.org/10.5281/zenodo.13837146</a></p>
</section>
<hr class="docutils" />
<section id="nfdi4bioimage-calendar-2025-march-original-image">
<h2>NFDI4Bioimage Calendar 2025 March; original image<a class="headerlink" href="#nfdi4bioimage-calendar-2025-march-original-image" title="Link to this heading">#</a></h2>
<p>Sonja Schimmler, Reinhard Altenhöner, Lars Bernard, Juliane Fluck, Axel Klinger, Sören Lorenz, Brigitte Mathiak, Bernhard Miller, Raphael Ritz, Thomas Schörner-Sadenius, Alexander Sczyrba, Regine Stein</p>
<p>Published 2025-02-27</p>
<p>Licensed CC-BY-4.0</p>
<p>Raw microscopy image from the NFDI4Bioimage calendar March 2025.
The image shows 125x magnified microscopic details of a biofilm formed by Pseudomonas fluorescence on the surface of a liquid culture medium. The culture was inoculated with a cellulose-overexpressing and surface-colonizing mScarlet-tagged wild type and a GFP-tagged mutant that is unable to colonize the surface. The biofilm can collapse over time due to its own mass, so that new strategies have to be developed and thus a life cycle emerges.
Image Metadata (using REMBI template):</p>
<p>Study
 </p>
<p>Study description
Biofilm formation</p>
<p>Study Component
 </p>
<p>Imaging method
Stereo microscopy</p>
<p>Biosample
 </p>
<p>Biological entity
Bacteria</p>
<p>Organism
Pseudomonas fluorescence</p>
<p>Specimen
 </p>
<p>Signal/contrast mechanism
Relief, fluorescence</p>
<p>Channel 1 - content
Relief, grey</p>
<p>Channel 1 - biological entity
Details of the biofilm in transmitted light</p>
<p>Channel 2 - content
mScarlet, red</p>
<p>Channel 2 - biological entity
WT over-expressing cellulose and colonizing the surface</p>
<p>Channel 3 - content
GFP, green</p>
<p>Channel 3 - biological entity
∆wss mutant unable to colonize the surface</p>
<p>Image Acquisition
 </p>
<p>Microscope model
Zeiss Axio Zoom V16</p>
<p>Image Data
 </p>
<p>Magnification
125x</p>
<p>Objective
PlanNeoFluar Z 1.0x</p>
<p>Dimension extents
x: 2752, y: 2208</p>
<p>Pixel size description
0.91 µm x 0.91 µm</p>
<p>Image area
2500µm x 2500µm</p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/14937632">https://zenodo.org/records/14937632</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14937632">https://doi.org/10.5281/zenodo.14937632</a></p>
</section>
<hr class="docutils" />
<section id="navigating-the-bioimage-analysis-landscape-understanding-the-community-and-its-collaborative-dynamics">
<h2>Navigating the Bioimage Analysis Landscape: Understanding the Community  and its Collaborative Dynamics<a class="headerlink" href="#navigating-the-bioimage-analysis-landscape-understanding-the-community-and-its-collaborative-dynamics" title="Link to this heading">#</a></h2>
<p>Martin Schätz</p>
<p>Published 2024-06-28</p>
<p>Licensed CC-BY-4.0</p>
<p>Presented as a part of:
Mexican Bioimaging Workshops 9: Fundamentos de Microscopía “Microscopía en la Salud”
Workshop on light microscopyJune 26th to 28th, 2024
Outreach 9June 29th, 2024
Expanding Global Access to BioimagingConnecting the Mexican Bioimaging Community</p>
<p><a class="reference external" href="https://zenodo.org/records/12584729">https://zenodo.org/records/12584729</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12584729">https://doi.org/10.5281/zenodo.12584729</a></p>
</section>
<hr class="docutils" />
<section id="nd2-does-not-open-in-fiji-bio-formats-8-1-1">
<h2>Nd2 does not open in Fiji Bio_formats 8.1.1<a class="headerlink" href="#nd2-does-not-open-in-fiji-bio-formats-8-1-1" title="Link to this heading">#</a></h2>
<p>Jaramillo Carlos</p>
<p>Published 2025-06-02</p>
<p>Licensed CC-BY-4.0</p>
<p>this file is a .nd2 image of a pollen grain taken with a Nikon 80i.  It is in RGB and it is a stack of hundreds of Z layers</p>
<p><a class="reference external" href="https://zenodo.org/records/15579371">https://zenodo.org/records/15579371</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15579371">https://doi.org/10.5281/zenodo.15579371</a></p>
</section>
<hr class="docutils" />
<section id="nd2-does-not-open-in-fiji-bio-formats-8-1-1-additional-files">
<h2>Nd2 does not open in Fiji Bio_formats 8.1.1 (additional files)<a class="headerlink" href="#nd2-does-not-open-in-fiji-bio-formats-8-1-1-additional-files" title="Link to this heading">#</a></h2>
<p>Jonatan Bustos</p>
<p>Published 2025-05-23</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset contains 4 .nd2 image files of pollen grains captured using a Nikon 80i microscope. The files include both the original full-frame images and cropped Regions of Interest (ROIs) extracted from them. All images are in RGB format and include multiple Z-stack layers.</p>
<p><a class="reference external" href="https://zenodo.org/records/15493140">https://zenodo.org/records/15493140</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15493140">https://doi.org/10.5281/zenodo.15493140</a></p>
</section>
<hr class="docutils" />
<section id="neurips-2022-cell-segmentation-competition-dataset">
<h2>NeurIPS 2022 Cell Segmentation Competition Dataset<a class="headerlink" href="#neurips-2022-cell-segmentation-competition-dataset" title="Link to this heading">#</a></h2>
<p>Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, Wei Lou, Haofeng Li, Eric Upschulte, Timo Dickscheid, de Almeida, José Guilherme, Yixin Wang, Lin Han, Xin Yang, Marco Labagnara, Vojislav Gligorovski, Maxime Scheder, Rahi, Sahand Jamal, Carly Kempster, Alice Pollitt, Leon Espinosa, Tam Mignot, Middeke, Jan Moritz, Jan-Niklas Eckardt, Wangkai Li, Zhaoyang Li, Xiaochen Cai, Bizhe Bai, Greenwald, Noah F., Van Valen, David, Erin Weisbart, Cimini, Beth A, Trevor Cheung, Oscar Brück, Bader, Gary D., Bo Wang</p>
<p>Published 2024-02-27</p>
<p>Licensed CC-BY-NC-ND-4.0</p>
<p>The official data set for the NeurIPS 2022 competition: cell segmentation in multi-modality microscopy images.
<a class="reference external" href="https://neurips22-cellseg.grand-challenge.org/">https://neurips22-cellseg.grand-challenge.org/</a>
Please cite the following paper if this dataset is used in your research. 
 
&#64;article{NeurIPS-CellSeg,
title = {The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions},
author = {Jun Ma and Ronald Xie and Shamini Ayyadhury and Cheng Ge and Anubha Gupta and Ritu Gupta and Song Gu and Yao Zhang and Gihun Lee and Joonkee Kim and Wei Lou and Haofeng Li and Eric Upschulte and Timo Dickscheid and José Guilherme de Almeida and Yixin Wang and Lin Han and Xin Yang and Marco Labagnara and Vojislav Gligorovski and Maxime Scheder and Sahand Jamal Rahi and Carly Kempster and Alice Pollitt and Leon Espinosa and Tâm Mignot and Jan Moritz Middeke and Jan-Niklas Eckardt and Wangkai Li and Zhaoyang Li and Xiaochen Cai and Bizhe Bai and Noah F. Greenwald and David Van Valen and Erin Weisbart and Beth A. Cimini and Trevor Cheung and Oscar Brück and Gary D. Bader and Bo Wang},
journal = {Nature Methods},      volume={21},      pages={1103–1113},      year = {2024},
doi = {<a class="reference external" href="https://doi.org/10.1038/s41592-024-02233-6">https://doi.org/10.1038/s41592-024-02233-6</a>}
}
 
This is an instance segmentation task where each cell has an individual label under the same category (cells). The training set contains both labeled images and unlabeled images. You can only use the labeled images to develop your model but we encourage participants to try to explore the unlabeled images through weakly supervised learning, semi-supervised learning, and self-supervised learning.
 
The images are provided with original formats, including tiff, tif, png, jpg, bmp… The original formats contain the most amount of information for competitors and you have free choice over different normalization methods. For the ground truth, we standardize them as tiff formats.
 
We aim to maintain this challenge as a sustainable benchmark platform. If you find the top algorithms (<a class="reference external" href="https://neurips22-cellseg.grand-challenge.org/awards/">https://neurips22-cellseg.grand-challenge.org/awards/</a>) don’t perform well on your images, welcome to send us the dataset (<a class="reference external" href="mailto:neurips&#46;cellseg&#37;&#52;&#48;gmail&#46;com">neurips<span>&#46;</span>cellseg<span>&#64;</span>gmail<span>&#46;</span>com</a>)! We will include them in the new testing set and credit your contributions on the challenge website!
 
Dataset License: CC-BY-NC-ND</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10719375">https://zenodo.org/records/10719375</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10719375">https://doi.org/10.5281/zenodo.10719375</a></p>
</section>
<hr class="docutils" />
<section id="new-kid-on-the-nfdi-block-nfdi4bioimage-a-national-initiative-for-fair-data-management-in-bioimaging-and-bioimage-analysis">
<h2>New Kid on the (NFDI) Block: NFDI4BIOIMAGE  - A National Initiative for FAIR Data Management in Bioimaging and Bioimage Analysis<a class="headerlink" href="#new-kid-on-the-nfdi-block-nfdi4bioimage-a-national-initiative-for-fair-data-management-in-bioimaging-and-bioimage-analysis" title="Link to this heading">#</a></h2>
<p>Cornelia Wetzker</p>
<p>Published 2024-10-29</p>
<p>Licensed CC-BY-4.0</p>
<p>The poster introduces the consortium NFDI4BIOIMAGE with its central objectives, provides an overview of challenges in bioimage data handling, sharing and analysis and lists support options by the consortium through its data stewardship team.
It is part of the work of the German consortium NFDI4BIOIMAGE funded by the Deutsche Forschungsgemeinschaft (DFG grant number NFDI 46/1, project number 501864659) and has been presented at the conference FDM&#64;Campus held in Göttingen September 23-25, 2024.</p>
<p><a class="reference external" href="https://zenodo.org/records/14006558">https://zenodo.org/records/14006558</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14006558">https://doi.org/10.5281/zenodo.14006558</a></p>
</section>
<hr class="docutils" />
<section id="nextflow-scalable-and-reproducible-scientific-workflows">
<h2>Nextflow: Scalable and reproducible scientific workflows<a class="headerlink" href="#nextflow-scalable-and-reproducible-scientific-workflows" title="Link to this heading">#</a></h2>
<p>Floden Evan, Di Tommaso Paolo</p>
<p>Published 2020-12-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Nextflow is an open-source workflow management system that prioritizes portability and reproducibility. It enables users to develop and seamlessly scale genomics workflows locally, on HPC clusters, or in major cloud providers’ infrastructures. Developed since 2014 and backed by a fast-growing community, the Nextflow ecosystem is made up of users and developers across academia, government and industry. It counts over 1M downloads and over 10K users worldwide.</p>
<p>Tags: Workflow Engine</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/4334697">https://zenodo.org/records/4334697</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4334697">https://doi.org/10.5281/zenodo.4334697</a></p>
</section>
<hr class="docutils" />
<section id="ocelot-overlapped-cell-on-tissue-dataset-for-histopathology">
<h2>OCELOT: Overlapped Cell on Tissue Dataset for Histopathology<a class="headerlink" href="#ocelot-overlapped-cell-on-tissue-dataset-for-histopathology" title="Link to this heading">#</a></h2>
<p>Jeongun Ryu, Aaron Valero Puche, JaeWoong Shin, Seonwook Park, Biagio Brattoli, Mohammad Mostafavi, Jinhee Lee, Sérgio Pereira, Wonkyung Jung, Soo Ick Cho, Chan-Young Ock, Kyunghyun Paeng, Donggeun Yoo</p>
<p>Published 2023-03-23</p>
<p>The OCELOT dataset is a histopathology dataset designed to facilitate the development of methods that utilize cell and tissue relationships. The dataset comprises both small and large field-of-view (FoV) patches extracted from digitally scanned whole slide images (WSIs), with overlapping regions. The small and large FoV patches are accompanied by annotations of cells and tissues, respectively. The WSIs are sourced from the publicly available TCGA database and were stained using the H&amp;E method before being scanned with an Aperio scanner.</p>
<p>For more details, please check <a class="reference external" href="https://lunit-io.github.io/research/ocelot_dataset/">https://lunit-io.github.io/research/ocelot_dataset/</a>.</p>
<p> </p>
<p>Before downloading the dataset, please make sure to carefully read and agree to the Terms and Conditions at (<a class="reference external" href="https://lunit-io.github.io/research/ocelot_tc/">https://lunit-io.github.io/research/ocelot_tc/</a>).</p>
<p>Also, please provide 1. name, 2. e-mail address, 3. organization/company name.</p>
<p> </p>
<hr class="docutils" />
<p>Release note.</p>
<p>In version 1.0.1, we exclude four test cases (586, 589, 609, 615) due to under-annotated issue.
In version 1.0.0, we include images and annotations of validation and test splits.
In version 0.1.2, we modified the coordinates of cell labels to range from 0 to 1023 (-1 from the previous coordinates).
In version 0.1.1, we removed non-H&amp;E stained patches from the dataset.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8417503">https://zenodo.org/records/8417503</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8417503">https://doi.org/10.5281/zenodo.8417503</a></p>
</section>
<hr class="docutils" />
<section id="ome2024-ngff-challenge-results">
<h2>OME2024 NGFF Challenge Results<a class="headerlink" href="#ome2024-ngff-challenge-results" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Published 2024-11-01</p>
<p>Licensed CC-BY-4.0</p>
<p>Presented at the 2024 FoundingGIDE event in Okazaki, Japan: <a class="reference external" href="https://founding-gide.eurobioimaging.eu/event/foundinggide-community-event-2024/">https://founding-gide.eurobioimaging.eu/event/foundinggide-community-event-2024/</a>
Note: much of the presentation was a demonstration of the OME2024-NGFF-Challenge – <a class="reference external" href="https://ome.github.io/ome2024-ngff-challenge/">https://ome.github.io/ome2024-ngff-challenge/</a> especially of querying an extraction of the metadata (<a class="github reference external" href="https://github.com/ome/ome2024-ngff-challenge-metadata">ome/ome2024-ngff-challenge-metadata</a>)
 </p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/14234608">https://zenodo.org/records/14234608</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14234608">https://doi.org/10.5281/zenodo.14234608</a></p>
</section>
<hr class="docutils" />
<section id="omexcavator-a-tool-for-exporting-and-connecting-bioimaging-specific-metadata-in-wider-knowledge-graphs">
<h2>OMExcavator: a tool for exporting and connecting  Bioimaging-specific metadata in wider knowledge graphs<a class="headerlink" href="#omexcavator-a-tool-for-exporting-and-connecting-bioimaging-specific-metadata-in-wider-knowledge-graphs" title="Link to this heading">#</a></h2>
<p>Stefan Dvoretskii, Klaus Maier-Hein, Marco Nolden, Christian Schmidt, Michele Bortolomeazzi, Josh Moore</p>
<p>Published 2025-05-15</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://zenodo.org/records/15423904">https://zenodo.org/records/15423904</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15423904">https://doi.org/10.5281/zenodo.15423904</a></p>
</section>
<hr class="docutils" />
<section id="omexcavator-a-tool-for-exporting-and-connecting-domain-specific-metadata-in-a-wider-knowledge-graph">
<h2>OMExcavator: a tool for exporting and connecting domain-specific metadata in a wider knowledge graph<a class="headerlink" href="#omexcavator-a-tool-for-exporting-and-connecting-domain-specific-metadata-in-a-wider-knowledge-graph" title="Link to this heading">#</a></h2>
<p>Stefan Dvoretskii, Michele Bortolomeazzi, Marco Nolden, Christian Schmidt, Klaus Maier-Hein, Josh Moore</p>
<p>Published 2025-02-21</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://zenodo.org/records/15268798">https://zenodo.org/records/15268798</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15268798">https://doi.org/10.5281/zenodo.15268798</a></p>
</section>
<hr class="docutils" />
<section id="open-science-sharing-licensing">
<h2>Open Science, Sharing &amp; Licensing<a class="headerlink" href="#open-science-sharing-licensing" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-04-18</p>
<p>Licensed CC-BY-4.0</p>
<p>Wir tauchen ein in die Welt der Open Science und definieren Begriffe wie Open Source, Open Access und die FAIR-Prinzipien (Findable, Accessible, Interoperable and Reuasable). Wir diskutieren, wie diese Methoden der [wissenschaftlichen] Kommunikation und des Datenmanagements die Welt verändern und wie wir sie praktisch in unsere Arbeit integrieren können. Dabei spielen Aspekte wie Copyright und Lizenzierung eine wichtige Rolle.</p>
<p>Tags: Research Data Management, Open Access, FAIR-Principles, Licensing</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/10990107">https://zenodo.org/records/10990107</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10990107">https://doi.org/10.5281/zenodo.10990107</a></p>
</section>
<hr class="docutils" />
<section id="optimisation-and-validation-of-a-swarm-intelligence-based-segmentation-algorithm-for-low-contrast-positron-emission-tomography">
<h2>Optimisation and Validation of a Swarm Intelligence based Segmentation Algorithm for low Contrast Positron Emission Tomography<a class="headerlink" href="#optimisation-and-validation-of-a-swarm-intelligence-based-segmentation-algorithm-for-low-contrast-positron-emission-tomography" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2014-04-01</p>
<p>Licensed CC-BY-4.0</p>
<p>In the field of radiooncological research, individualised therapy is one of the hot topics at the moment. As a key aspect biologically-adapted therapy is discussed. Therapy adaption based on biological parameters may include tomographic imaging to determine biological properties of the tumour. One often invoked imaging modality is positron emission tomography (PET) using the tracer [18F]-fluoromisonidazole (FMISO) for hypoxia imaging. Hypoxia imaging is of interest, because hypoxic tumours are known to be radiorestistant. Even further, patients with hypoxic tumours have worse prognosis compared to patients with normoxic tumours. Thus, hypoxia imaging appears promising for radiotherapy treatment adaption. For example, volumetric analysis of FMISO PET could deliver additional hypoxia target volumes, which may be irradiated with higher radiation doses to improve the therapeutic effect. However, limited contrast between target volume and background in FMISO PET images interferes image analysis.Established methods for target volume delineation in PET do not allow determination of reliable contours in FMISO PET. To tackle this aspect, this thesis focusses on an earlier developed swarm intelligence based segmentation algorithm for FMISO PET and rather, its optimisation and validation in a clinically relevant setting. In this setting, clinical FMISO PET images were used which were acquired as part of a clinical trial performed at the Clinic and Policlinic for Radiation Therapy and Radiooncology of the University Hospital Carl Gustav Carus Dresden. The segmentation algorithm was applied to these imaging data sets and optimised using a cross-validation approach incorporating reference contours from experienced observers who outlined FMISO PET positive volumes manually. Afterwards, the performance of the algorithm and the properties of the resulting contours were studied in more detail. The algorithm was shown to deliver contours which were similar to manually-created contours to a degree like manually-created contours were similar to each other. Thus, the application of the algorithm in clinical research is recommended to eliminate inter-observer-variabilities. Finally, it was shown that repeated FMISO PET imaging before and shortly after the beginning of combined radiochemotherapy lead to manually-created contours with significantly higher variations than the variations of automatically-created contours using the proposed algorithm. Increased contour similarity in subsequently acquired imaging data highlights the observer-independence of the algorithm. While several observers outline different volumes, in identical data sets as well as in subsequent imaging data sets, the algorithm outlines more stable volumes in both cases. Thus, increased contour reproducibility is reached by automation of the delineation process by the proposed algorithm. </p>
<p><a class="reference external" href="https://zenodo.org/records/7209862">https://zenodo.org/records/7209862</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7209862">https://doi.org/10.5281/zenodo.7209862</a></p>
</section>
<hr class="docutils" />
<section id="optimized-cranial-window-implantation-for-subcellular-and-functional-imaging-in-vivo">
<h2>Optimized cranial window implantation for subcellular and functional imaging in vivo<a class="headerlink" href="#optimized-cranial-window-implantation-for-subcellular-and-functional-imaging-in-vivo" title="Link to this heading">#</a></h2>
<p>Ben Vermaercke</p>
<p>Published 2025-01-13</p>
<p>Licensed CC-BY-4.0</p>
<p>Intravital workshop 15/11/2024</p>
<p><a class="reference external" href="https://zenodo.org/records/14641777">https://zenodo.org/records/14641777</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14641777">https://doi.org/10.5281/zenodo.14641777</a></p>
</section>
<hr class="docutils" />
<section id="parhyale-3d-segmentation-dataset">
<h2>Parhyale 3D segmentation dataset<a class="headerlink" href="#parhyale-3d-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Frederike Alwes, Ko Sugawara, Michalis Averof</p>
<p>Published 2023-08-11</p>
<p>Licensed CC-BY-4.0</p>
<p>The Parhyale 3D Segmentation dataset consists of 50 timepoints (TP01-TP50) of 3D images (512x512x34), where the manual annotations can be found at discrete 6 timepoints (at TP01, TP11, TP21, TP31, TP41 and TP50).</p>
<p>For further details, see README file.</p>
<p>This version fixes the duplicated label IDs found in the previous version of label files. This version ensures that each instance has a unique ID. Thanks to Jackson Borchardt for reporting that error.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8252039">https://zenodo.org/records/8252039</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8252039">https://doi.org/10.5281/zenodo.8252039</a></p>
</section>
<hr class="docutils" />
<section id="platynereis-em-training-data">
<h2>Platynereis EM training data<a class="headerlink" href="#platynereis-em-training-data" title="Link to this heading">#</a></h2>
<p>Constantin Pape</p>
<p>Published 2020-02-19</p>
<p>Licensed CC-BY-4.0</p>
<p>Training data for Convolutional Neural Networks used in the publication Whole-body integration of gene expression and single-cell morphology. We provide training data for segmenting structures in the SerialBlockface Electron Microscopy data-set containing a complete 6 day old Platynereis dumerilii larva, in particular for:</p>
<ul class="simple">
<li><p>cell membranes: 9 training blocks &#64; resolution 20x20x25 nm. Based on initial training data provided by <a class="reference external" href="https://ariadne.ai/">https://ariadne.ai/</a>.</p></li>
<li><p>cilia: 3 training and 2 validation blocks &#64; resolution 20x20x25 nm.</p></li>
<li><p>cuticle: 5 training blocks &#64; resolution 40x40x50 nm.</p></li>
<li><p>nuclei: 12 training blocks &#64; resolution 80x80x100 nm. Based on initial training data provided by <a class="reference external" href="https://ariadne.ai/">https://ariadne.ai/</a>.</p></li>
</ul>
<p>For details on how to use this data for training, see <a class="github reference external" href="https://github.com/platybrowser/platybrowser-backend/tree/master/segmentation">platybrowser/platybrowser-backend</a>.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/3675220">https://zenodo.org/records/3675220</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.3675220">https://doi.org/10.5281/zenodo.3675220</a></p>
</section>
<hr class="docutils" />
<section id="preprint-be-sustainable-recommendations-for-fair-resources-in-life-sciences-research-eosc-life-s-lessons">
<h2>Preprint: “Be Sustainable”, Recommendations for FAIR Resources in Life Sciences research: EOSC-Life’s Lessons<a class="headerlink" href="#preprint-be-sustainable-recommendations-for-fair-resources-in-life-sciences-research-eosc-life-s-lessons" title="Link to this heading">#</a></h2>
<p>Romain David, Arina Rybina, Jean-Marie Burel, Jean-Karim Heriche, Pauline Audergon, Jan-Willem Boiten, Frederik Coppens, Sara Crockett, Exter Katrina, Sven Fahrener, Maddalena Fratelli, Carole Goble, Philipp Gormanns, Tobias Grantner, Bjorn Gruning, Kim Tamara Gurwitz, John Hancock, Henriette Harmse, Petr Holub, Nick Juty, Geoffrey Karnbach, Emma Karoune, Antje Keppler, Jessica Klemeier, Carla Lancelotti, Jean-Luc Legras, L. Allyson Lister, Dario Livio Longo, Rebecca Ludwig, Benedicte Madon, Marzia Massimi, Vera Matser, Rafaele Matteoni, Mayrhofer Michaela Th., Christian Ohmann, Maria Panagiotopoulou, Helen Parkinson, Isabelle Perseil, Claudia Pfander, Roland Pieruschka, Michael Raess, Andreas Rauber, Audrey S. Richard, Paolo Romano, Antonio Rosato, Alex Sanchez-Pla, Susanna-Assunta Sansone, Ugis Sarkans, Beatriz Serrano-Solano, Jing Tang, Ziaurrehman Tanoli, Jonathan Tedds, Harald Wagener, Martin Weise, Hans V. Westerhoff, Rudolf Wittner, Jonathan Ewbank, Niklas Blomberg, Philip Gribbon</p>
<p>Published 2023-09-12</p>
<p>Licensed CC-BY-4.0</p>
<p>“Be SURE - Be SUstainable REcommendations”The main goals and challenges for the Life Science (LS) communities in the Open Science framework are to increase reuse and sustainability of data resources, software tools, and workflows, especially in large-scale data-driven research and computational analyses. Here, we present key findings, procedures, effective measures and recommendations for generating and establishing sustainable LS resources based on the collaborative, cross-disciplinary work done within the EOSC-Life (European Open Science Cloud for Life Sciences) consortium. Bringing together 13 European LS Research Infrastructures (RIs), it has laid the foundation for an open, digital space to support biological and medical research. Using lessons learned from 27 selected projects, we describe the organisational, technical, financial and legal/ethical challenges that represent the main barriers to sustainability in the life sciences. We show how EOSC-Life provides a model for sustainable FAIR data management, including solutions for sensitive- and industry-related resources, by means of cross-disciplinary training and best practices sharing. Finally, we illustrate how data harmonisation and collaborative work facilitate interoperability of tools, data, solutions and lead to a better understanding of concepts, semantics and functionalities in the life <a class="reference external" href="http://sciences.IN">sciences.IN</a> PRESS EMBO Journal: <a class="reference external" href="https://www.embopress.org/journal/14602075&amp;amp;nbsp;AVAILABLE">https://www.embopress.org/journal/14602075&amp;nbsp;AVAILABLE</a> SOON at : <a class="reference external" href="https://doi.org/10.15252/embj.2023115008">https://doi.org/10.15252/embj.2023115008</a> </p>
<p><a class="reference external" href="https://zenodo.org/records/8338931">https://zenodo.org/records/8338931</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8338931">https://doi.org/10.5281/zenodo.8338931</a></p>
</section>
<hr class="docutils" />
<section id="prodgerlab-stardist-hiv-target-cell-training-set">
<h2>ProdgerLab-StarDist-HIV Target Cell Training Set<a class="headerlink" href="#prodgerlab-stardist-hiv-target-cell-training-set" title="Link to this heading">#</a></h2>
<p>Zhongtian Shao</p>
<p>Published 2023-06-28</p>
<p>Licensed CC-BY-4.0</p>
<p>40 annotated immunofluorescence microscopy images (600 microns x 600 microns) of foreskin tissue stained for CD3/CD4/CCR5/Nuclei. These images were used to train StarDist models used for the identification of HIV Target Cells in foreskin tissue section scans. </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8091914">https://zenodo.org/records/8091914</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8091914">https://doi.org/10.5281/zenodo.8091914</a></p>
</section>
<hr class="docutils" />
<section id="prompt-engineering-agentic-workflows-and-multi-modal-large-language-models">
<h2>Prompt Engineering, Agentic Workflows and Multi-modal Large Language Models<a class="headerlink" href="#prompt-engineering-agentic-workflows-and-multi-modal-large-language-models" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2025-01-19</p>
<p>Licensed CC-BY-4.0</p>
<p>In these two slide-decks we explore applications of large language models. In the first slide deck we dive into prompt engineering, function calling and how to build agentic workflows. In the second slide-deck we explore multi-modal large language models focusing on vision language models and image generation models. </p>
<p><a class="reference external" href="https://zenodo.org/records/14692037">https://zenodo.org/records/14692037</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14692037">https://doi.org/10.5281/zenodo.14692037</a></p>
</section>
<hr class="docutils" />
<section id="qupath-open-source-software-for-analysing-awkward-images">
<h2>QuPath: Open source software for analysing (awkward) images<a class="headerlink" href="#qupath-open-source-software-for-analysing-awkward-images" title="Link to this heading">#</a></h2>
<p>Peter Bankhead</p>
<p>Published 2020-12-16</p>
<p>Licensed CC-BY-4.0</p>
<p>Slides from the CZI/EOSS online meeting in December 2020.</p>
<p>Tags: Bioimage Analysis</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/4328911">https://zenodo.org/records/4328911</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4328911">https://doi.org/10.5281/zenodo.4328911</a></p>
</section>
<hr class="docutils" />
<section id="rdf-as-a-bridge-to-domain-platforms-like-omero-or-there-and-back-again">
<h2>RDF as a bridge to domain-platforms like OMERO, or There and back again.<a class="headerlink" href="#rdf-as-a-bridge-to-domain-platforms-like-omero-or-there-and-back-again" title="Link to this heading">#</a></h2>
<p>Josh Moore, Andra Waagmeester, Kristina Hettne, Katherine Wolstencroft, Susanne Kunis</p>
<p>Licensed CC-BY-4.0</p>
<p>In 2005, the first version of OMERO stored RDF natively. However, just a year after the 1.0 release of RDF, performance considerations led to the development of a more traditional SQL approach for OMERO. A binary protocol makes it possible to query and retrieve metadata but the resulting information cannot immediately be combined with other sources. This is the adventure of rediscovering the benefit of RDF triples as a – if not the – common exchange mechanism.</p>
<p>Tags: Research Data Management, FAIR-Principles, Bioimage Analysis</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.10687658">https://zenodo.org/doi/10.5281/zenodo.10687658</a></p>
</section>
<hr class="docutils" />
<section id="research-data-management-on-campus-and-in-nfdi4bioimage">
<h2>RESEARCH DATA MANAGEMENT on Campus and in NFDI4BIOIMAGE<a class="headerlink" href="#research-data-management-on-campus-and-in-nfdi4bioimage" title="Link to this heading">#</a></h2>
<p>Cornelia Wetzker, Michael Schlierf</p>
<p>Published 2024-08-29</p>
<p>Licensed CC-BY-4.0</p>
<p>The poster is part of the work of the German consortium NFDI4BIOIMAGE funded by the Deutsche Forschungsgemeinschaft (DFG grant number NFDI 46/1, project number 501864659).</p>
<p><a class="reference external" href="https://zenodo.org/records/13684187">https://zenodo.org/records/13684187</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13684187">https://doi.org/10.5281/zenodo.13684187</a></p>
</section>
<hr class="docutils" />
<section id="reconstructed-images-of-a-2dsim-multiposition-acquisition">
<h2>Reconstructed images of a 2DSIM multiposition acquisition.<a class="headerlink" href="#reconstructed-images-of-a-2dsim-multiposition-acquisition" title="Link to this heading">#</a></h2>
<p>Louis Romette</p>
<p>Published 2025-02-19</p>
<p>Licensed CC-BY-4.0</p>
<p>Acquired with an Nikon SIM, in 2D-SIM mode at 488nm of excitation with 30% laser power and 200ms of exposition.  Fluorescence is a knocked-in mStayGold-β2Spectrin. Cells are rat hippocampal neurons à DIV 3. The file is a reconstructed multiposition acquisition (10 positions). Uploaded to show a probable issue with Bio-Formats in Fiji, where SIM reconstrcuted multipositions files open like static noise. </p>
<p><a class="reference external" href="https://zenodo.org/records/14893791">https://zenodo.org/records/14893791</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14893791">https://doi.org/10.5281/zenodo.14893791</a></p>
</section>
<hr class="docutils" />
<section id="report-on-a-pilot-study-implementation-of-omero-for-microscopy-data-management">
<h2>Report on a pilot study:  Implementation of OMERO for  microscopy data management<a class="headerlink" href="#report-on-a-pilot-study-implementation-of-omero-for-microscopy-data-management" title="Link to this heading">#</a></h2>
<p>Silke Tulok, Gunar Fabig, Andy Vogelsang, Thomas Kugel, Thomas Müller-Reichert</p>
<p>Published 2023-11-10</p>
<p>Licensed CC-BY-4.0</p>
<p>The Core Facility Cellular Imaging (CFCI) at the Faculty of Medicine Carl Gustav Carus (TU Dresden) is currently running a pilot project for testing the use and handling of the OMERO software. This is done together with interested users of the imaging facility and a research group. Currently, we are pushing forward this pilot study on a small scale without any data steward. Our experiences argue so far for giving data management issues into the hands of dedicated personnel not fully involved in research projects. As funding agencies will ask for higher and higher standards for implementing FAIRdata principles in the future, this will be a releva</p>
<p><a class="reference external" href="https://zenodo.org/records/10103316">https://zenodo.org/records/10103316</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10103316">https://doi.org/10.5281/zenodo.10103316</a></p>
</section>
<hr class="docutils" />
<section id="research-data-management-seminar-slides">
<h2>Research Data Management Seminar - Slides<a class="headerlink" href="#research-data-management-seminar-slides" title="Link to this heading">#</a></h2>
<p>Stefano Della Chiesa</p>
<p>Published 2022-05-18</p>
<p>Licensed CC-BY-4.0</p>
<p>This Research Data Management (RDM) Slides introduce to the multidisciplinary knowledge and competencies required to address policy compliance and research data management best practices throughout a project lifecycle, and beyond it.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Module 1 - Introduces the RDM giving its context in the Research Data Governance
Module 2 - Illustrates the most important RDM policies and principles
Module 3 - Provides the most relevant RDM knowledge bricks
Module 4 - Discuss the Data Management Plans (DMPs), examples, templates and guidance
</pre></div>
</div>
<p> </p>
<p>Tags: Research Data Management</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/record/6602101">https://zenodo.org/record/6602101</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6602101">https://doi.org/10.5281/zenodo.6602101</a></p>
</section>
<hr class="docutils" />
<section id="research-data-managemet-and-how-not-to-get-overwhelmed-with-data">
<h2>Research Data Managemet and how not to get overwhelmed with data<a class="headerlink" href="#research-data-managemet-and-how-not-to-get-overwhelmed-with-data" title="Link to this heading">#</a></h2>
<p>Martin Schätz</p>
<p>Published 2023-09-23</p>
<p>Licensed CC-BY-4.0</p>
<p>Research data management and how not to get overwhelmed with data presentation is an overview of bioimage analysis with a focus on the basics for data management planning, FAIR principles, and how to practically organize folders and prepares naming convention. The presentation includes an overview of metadata, Creative Common licenses, and a sum up of electronic laboratory notebooks. The last two slides go through how all of that works in practice in open access core microscopy facility.</p>
<p><a class="reference external" href="https://zenodo.org/records/8372703">https://zenodo.org/records/8372703</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8372703">https://doi.org/10.5281/zenodo.8372703</a></p>
</section>
<hr class="docutils" />
<section id="root-tissue-segmentation-dataset">
<h2>Root tissue segmentation dataset<a class="headerlink" href="#root-tissue-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Julian Wanner, Kuhn Cuellar, Luis, Friederike Wanke</p>
<p>Published 2022-01-12</p>
<p>Licensed CC-BY-4.0</p>
<p>The PHDFM dataset is composed of fluorescence microscopy images of root tissue samples from A. thaliana, using the ratiometric fluorescent indicator 8‐hydroxypyrene‐1,3,6‐trisulfonic acid trisodium salt (HPTS). This semantic segmentation training dataset consists of 2D microscopy images (the brightfield channel for excitation at 405 nm), each containing a segmentation mask as an additional image channel (manually annotated by plant biologists). The segmentation masks classify pixels into the following 5 labels with the corresponding IDs: background (0), root tissue (1), early elongation zone (2), late elongation zone (3), and meristematic zone (4).</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5841376">https://zenodo.org/records/5841376</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5841376">https://doi.org/10.5281/zenodo.5841376</a></p>
</section>
<hr class="docutils" />
<section id="round-table-workshop-1-sample-stabilization-in-intravital-imaging">
<h2>Round Table Workshop 1 - Sample Stabilization in intravital Imaging<a class="headerlink" href="#round-table-workshop-1-sample-stabilization-in-intravital-imaging" title="Link to this heading">#</a></h2>
<p>Michael Gerlach, Hans-Ulrich Fried, Christiane Peuckert</p>
<p>Published 2024-11-14</p>
<p>Licensed CC-BY-4.0</p>
<p>Notes from a round table workshop on the 4th Day of Intravital Microscopy in Leuven, Belgium.
Contains hands-on tips, tricks and schemes to improve sample stability during various models of Intravital Miroscopy.</p>
<p><a class="reference external" href="https://zenodo.org/records/14161289">https://zenodo.org/records/14161289</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14161289">https://doi.org/10.5281/zenodo.14161289</a></p>
</section>
<hr class="docutils" />
<section id="round-table-workshop-2-correction-of-drift-and-movement">
<h2>Round Table Workshop 2 - Correction of Drift and Movement<a class="headerlink" href="#round-table-workshop-2-correction-of-drift-and-movement" title="Link to this heading">#</a></h2>
<p>Dr. Hellen Ishikawa-Ankerhold, Max Nobis</p>
<p>Published 2024-11-14</p>
<p>Licensed CC-BY-4.0</p>
<p>Session 2 of a round table workshop. Features description of image processing methods useful in intravital imaging to compensate for the motion of living tissue.</p>
<p><a class="reference external" href="https://zenodo.org/records/14161633">https://zenodo.org/records/14161633</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14161633">https://doi.org/10.5281/zenodo.14161633</a></p>
</section>
<hr class="docutils" />
<section id="sample-data-for-pr-4284-https-github-com-ome-bioformats-pull-4284">
<h2>Sample data for PR#4284 (<a class="github reference external" href="https://github.com/ome/bioformats/pull/4284">ome/bioformats#4284</a>)<a class="headerlink" href="#sample-data-for-pr-4284-https-github-com-ome-bioformats-pull-4284" title="Link to this heading">#</a></h2>
<p>Jürgen Bohl</p>
<p>Published 2025-03-04</p>
<p>Licensed CC-BY-4.0</p>
<p>With this file the problem addressed in PR#4284 can be reproduced: when runningbfconvert -series 4 -channel 0 2025_01_27__0007_offline_Zen_3_9_5.czi output.png
the result is garbled.</p>
<p><a class="reference external" href="https://zenodo.org/records/14968770">https://zenodo.org/records/14968770</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14968770">https://doi.org/10.5281/zenodo.14968770</a></p>
</section>
<hr class="docutils" />
<section id="sciaugment">
<h2>SciAugment<a class="headerlink" href="#sciaugment" title="Link to this heading">#</a></h2>
<p>Martin Schätz</p>
<p>Published 2022-07-29</p>
<p>Licensed OTHER-OPEN</p>
<p>SciAugment v0.2.0 has pip installable version, channel-wise augmentation was added, and an option for all augmentations or no augmentation. Examples of how to use the tool are in README and in Google Colab notebooks. Practical examples of how to use results with YOLOv5 on scientific data can be found in the SciCount project.</p>
<p>SciAugment aims to provide an option to create an augmented image set with similar changes in data as the imaging sensor and technique would do.</p>
<p><a class="reference external" href="https://zenodo.org/records/6991106">https://zenodo.org/records/6991106</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6991106">https://doi.org/10.5281/zenodo.6991106</a></p>
</section>
<hr class="docutils" />
<section id="segmentation-of-nuclei-in-histopathology-images-by-deep-regression-of-the-distance-map">
<h2>Segmentation of Nuclei in Histopathology Images by deep regression of the distance map<a class="headerlink" href="#segmentation-of-nuclei-in-histopathology-images-by-deep-regression-of-the-distance-map" title="Link to this heading">#</a></h2>
<p>Naylor Peter Jack, Walter Thomas, Laé Marick, Reyal Fabien</p>
<p>Published 2018-02-16</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset has been annonced in our accepted paper “Segmentation of Nuclei in Histopathology Images by deep regression of the distance map” in Transcation on Medical Imaging on the 13th of August.
This dataset consists of 50 annotated images, divided into 11 patients.</p>
<p> </p>
<p>v1.1 (27/02/19): Small corrections to a few pixel that were labelled nuclei but weren’t.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/2579118">https://zenodo.org/records/2579118</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.2579118">https://doi.org/10.5281/zenodo.2579118</a></p>
</section>
<hr class="docutils" />
<section id="segmenting-cells-in-a-spheroid-in-3d-using-2d-stardist-within-trackmate">
<h2>Segmenting cells in a spheroid in 3D using 2D StarDist within TrackMate<a class="headerlink" href="#segmenting-cells-in-a-spheroid-in-3d-using-2d-stardist-within-trackmate" title="Link to this heading">#</a></h2>
<p>Jean-Yves Tinevez, Joanna W. Pylvänäinen, Guillaume Jacquemet</p>
<p>Published 2021-08-19</p>
<p>Licensed CC-BY-4.0</p>
<p>3D image of cells in a spheroid, imaged on a confocal microscope, used in a tutorial to demonstrate how to hack TrackMate to segment cells in 3D using the 2D segmentation algorithms it ships.</p>
<p>Image by Guillaume Jacquemet.</p>
<p>For more details see <a class="reference external" href="https://imagej.net/plugins/trackmate/trackmate-stardist#generation-of-3d-labels-by-tracking-2d-labels-using-trackmate">https://imagej.net/plugins/trackmate/trackmate-stardist#generation-of-3d-labels-by-tracking-2d-labels-using-trackmate</a></p>
<p> </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5220610">https://zenodo.org/records/5220610</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5220610">https://doi.org/10.5281/zenodo.5220610</a></p>
</section>
<hr class="docutils" />
<section id="single-cell-approach-dissecting-agr-quorum-sensing-dynamics-in-staphylococcus-aureus">
<h2>Single-cell approach dissecting agr quorum sensing dynamics in Staphylococcus aureus<a class="headerlink" href="#single-cell-approach-dissecting-agr-quorum-sensing-dynamics-in-staphylococcus-aureus" title="Link to this heading">#</a></h2>
<p>Julian Bär</p>
<p>Published 2024-02-28</p>
<p>Licensed CC-BY-4.0</p>
<p>Training data for the two StarDist2D models and the DeLTA 2.0 2D tracking model used in the publication on bioarxiv. The trained stardist models are included in the respective zip files of the training data. mm: mother-machine; cc: connected chamber. Each of them contains two folders, img and seg_label. They contain matching pairs of phasecontrast images (img) and label images (seg_label). 
 
tracking_set_subset.zip contains the training data for the DeLTA tracking model following the default folder structure. We used custom weight functions to create the training weight maps in the folder wei. The folder wei_bck contains weights generated with the original function.
The unet_pads_tracking.hdf5 is the retrained tracking model used in the associated publication.
See associated GitHub repository for example code on how to use the models for segmentation and tracking.
The four numbered zip files contain the data used to create all figures displaying image analysis output.
Abstract:
Staphylococcus aureus both colonizes humans and causes severe virulent infections. Virulence is regulated by the agr quorum sensing system and its autoinducing peptide (AIP), with dynamics at the single-cell level across four agr-types – each defined by distinct AIP sequences and capable of cross-inhibition – remaining elusive. Employing microfluidics, time-lapse microscopy, and deep-learning image analysis, we uncovered significant differences in AIP sensitivity among agr-types. We observed bimodal agr activation, attributed to intergenerational phenotypic stability and influenced by AIP concentration. Upon AIP stimulation, agr‑III showed AIP insensitivity, while agr‑II exhibited increased sensitivity and prolonged generation time. Beyond expected cross-inhibition of agr‑I by heterologous AIP‑II and ‑III, the presumably cross-activating AIP‑IV also inhibited agr‑I. Community interactions across different agr-type pairings revealed four main patterns: stable or switched dominance, and delayed or stable dual activation, influenced by community characteristics. These insights underscore the potential of personalized treatment strategies considering virulence and genetic diversity.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10720439">https://zenodo.org/records/10720439</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10720439">https://doi.org/10.5281/zenodo.10720439</a></p>
</section>
<hr class="docutils" />
<section id="slides-about-flute-a-python-gui-for-interactive-phasor-analysis-of-flim-data">
<h2>Slides about FLUTE: a Python GUI for interactive phasor analysis of FLIM data<a class="headerlink" href="#slides-about-flute-a-python-gui-for-interactive-phasor-analysis-of-flim-data" title="Link to this heading">#</a></h2>
<p>Chiara Stringari</p>
<p>Published 2024-03-19</p>
<p>Licensed CC-BY-4.0</p>
<p>This presentation introduces the open source software to analyze FLIM data:
FLUTE – (F)luorescence (L)ifetime (U)ltima(T)e (E)xplorer:
a Python GUI for interactive phasor analysis of FLIM data
 
The software is available on GitHub: <a class="github reference external" href="https://github.com/LaboratoryOpticsBiosciences/FLUTE">LaboratoryOpticsBiosciences/FLUTE</a>
and it is published on Biological imaging Journal: Gottlieb, D., Asadipour, B., Kostina, P., Ung, T., &amp; Stringari, C. (2023). FLUTE: A Python GUI for interactive phasor analysis of FLIM data. Biological Imaging, 1-22. doi:10.1017/S2633903X23000211
The lecture was part of the short talks on community developed FLIM-software at the German BioImaging workshop on FLIM in Munich.</p>
<p><a class="reference external" href="https://zenodo.org/records/10839310">https://zenodo.org/records/10839310</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10839310">https://doi.org/10.5281/zenodo.10839310</a></p>
</section>
<hr class="docutils" />
<section id="so-geschlossen-wie-notig-so-offen-wie-moglich-datenschutz-beim-umgang-mit-forschungsdaten">
<h2>So geschlossen wie nötig, so offen wie möglich - Datenschutz beim Umgang mit Forschungsdaten<a class="headerlink" href="#so-geschlossen-wie-notig-so-offen-wie-moglich-datenschutz-beim-umgang-mit-forschungsdaten" title="Link to this heading">#</a></h2>
<p>Pia Voigt</p>
<p>Published 2024-05-30</p>
<p>Licensed CC-BY-4.0</p>
<p>Der Umgang mit personenbezogenen Daten stellt Forschende oft vor rechtliche Herausforderungen: Unter welchen Bedingungen dürfen personenbezogene Daten verarbeitet werden? Welche Voraussetzungen müssen erfüllt sein und welche Strategien können angewendet werden, um Daten sicher speichern, verarbeiten, teilen und aufbewahren zu können? Mit Hilfe dieses Foliensatzes erhalten Sie Einblicke in datenschutzrechtliche Aspekte beim Umgang mit Ihren Forschungsdaten. </p>
<p>Tags: Research Data Management, Data Protection, FAIR-Principles</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/11396199">https://zenodo.org/records/11396199</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11396199">https://doi.org/10.5281/zenodo.11396199</a></p>
</section>
<hr class="docutils" />
<section id="stackview-sliceplot-example-data">
<h2>Stackview sliceplot example data<a class="headerlink" href="#stackview-sliceplot-example-data" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-11-03</p>
<p>Licensed CC-BY-4.0</p>
<p>This is a dataset of PNG images of <a class="reference external" href="https://zenodo.org/records/12623730">Bio-Image Data Science teaching slides</a>. The png_umap.yml file contains a list of all images and a dimensionality reduced embedding (Uniform Manifold Approximation Projection, UMAP) made using OpenAI’s text-embedding-ada-002 model.
A notebook for visualizing this data is published here: <a class="github reference external" href="https://github.com/haesleinhuepf/stackview/blob/main/docs/sliceplot.ipynb">haesleinhuepf/stackview</a></p>
<p><a class="reference external" href="https://zenodo.org/records/14030307">https://zenodo.org/records/14030307</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14030307">https://doi.org/10.5281/zenodo.14030307</a></p>
</section>
<hr class="docutils" />
<section id="stardist-adipocyte-segmentation-training-data-training-notebook-and-model">
<h2>StarDist Adipocyte Segmentation Training data, Training Notebook and Model<a class="headerlink" href="#stardist-adipocyte-segmentation-training-data-training-notebook-and-model" title="Link to this heading">#</a></h2>
<p>Sarkis Rita, Naveiras Olaia, Burri Olivier, Weigert Martin, De Leval Laurence</p>
<p>Published 2022-08-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Data from H&amp;E human bone marrow whole slide scanner images used in the paper: “MarrowQuant 2.0: a digital pathology workflow assisting bone marrow evaluation in clinical and experimental hematology” (<a class="reference external" href="https://doi.org/10.21203/rs.3.rs-1860140/v1">https://doi.org/10.21203/rs.3.rs-1860140/v1</a>)</p>
<p> </p>
<p>292 image patches</p>
<p>Ground truth were manually annotated using QuPath and split into 263 images for training and 29 for validation.</p>
<p>Training in StarDist was done on a Windows 10 PC with an RTX 2080 GPU. The requirements file for installing a Python 3.7 environment to run the attached notebooks is provided (stardist-val.txt).</p>
<p>The StarDist model configuration can be found in the Jupyter Notebook :</p>
<p>Adipocyte Training.ipynb</p>
<p>Model validation and metrics can be performed by running the notebook after finishing the Adipocyte Training notebook.</p>
<p>Quality Control.ipynb</p>
<p> </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7003909">https://zenodo.org/records/7003909</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7003909">https://doi.org/10.5281/zenodo.7003909</a></p>
</section>
<hr class="docutils" />
<section id="stardist-model-and-data-for-the-segmentation-of-yersinia-enterocolitica-cells-in-widefield-images">
<h2>StarDist model and data for the segmentation of Yersinia enterocolitica cells in widefield images<a class="headerlink" href="#stardist-model-and-data-for-the-segmentation-of-yersinia-enterocolitica-cells-in-widefield-images" title="Link to this heading">#</a></h2>
<p>Christoph Spahn, Andreas Diepold, Francesca Ermoli</p>
<p>Published 2024-05-02</p>
<p>Licensed CC-BY-4.0</p>
<p>Dataset and StarDist model for the segmentation of Yersinia enterocolitica cells
This dataset and StarDist model are part of the publication “Active downregulation of the type III secretion system at higher local cell densities promotes Yersinia replication and dissemination”.
It contains the dataset that was used for training the provided StarDist model using ZeroCostDL4Mic.
Data:
Yersinia enterocolitica cells were spotted on an agarose pad (1.5% low melting agarose (Sigma-Aldrich) in minimal medium, 1% Casamino acids, 5 mM EGTA,  glass depression slides (Marienfeld)). For imaging, a Deltavision Elite Optical Sectioning Microscope equipped with a UPlanSApo 100×/1.40 oil objective (Olympus) and an EDGE sCMOS_5.5 camera (Photometrics) was used. Z-stacks with 9 slices (∆z = 0.15 µm) per fluorescence channel were acquired and  5 slices were selected for network training. Images were annotated in Fiji using the Freehand selection tool, and brightlight and mask images were quartered to obtain the final dataset of 300 paired images. 260 images were used for training, while 40 images were used to test model performance.
Model:
The StarDist 2D model was trained from scratch for 100 epochs on 300 paired image patches (image dimensions: (480 x 480 px²), patch size: (480 x 480 px²)) with a batch size of 4 and a mae loss function, using the StarDist 2D ZeroCostDL4Mic notebook (v 1) (von Chamier &amp; Laine et al., 2020). Grid parameter was set to 2 and the number of rays to 120. The model was trained with an initial learning rate of 0.0003 using a 80/20 train/test split. The dataset was augmented 4-fold by flipping and rotation.
Key python packages used include tensorflow (v 0.1.12), Keras (v2.3.1), csbdeep (v 0.7.2), numpy (v 1.21.6), cuda (v 11.1.105Build cuda_11.1.TC455_06.29190527_0). The training was accelerated using a Tesla T4 GPU.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/11105050">https://zenodo.org/records/11105050</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11105050">https://doi.org/10.5281/zenodo.11105050</a></p>
</section>
<hr class="docutils" />
<section id="stardist-aspc1-lifeact">
<h2>StarDist_AsPC1_Lifeact<a class="headerlink" href="#stardist-aspc1-lifeact" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-08-29</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model designed for segmenting AsPC1 cells labeled with Lifeact from fluorescence microscopy images. The model distinguishes individual AsPC1 cells within clusters and separates them from the background. The model was trained on a small dataset and achieved an Intersection over Union (IoU) score of 0.884 and an F1 Score of 0.967, indicating high accuracy in cell segmentation.
Specifications</p>
<p>Model: StarDist for segmenting AsPC1 cells in fluorescence microscopy images</p>
<p>Training Dataset:</p>
<p>Number of Images: 10 paired fluorescence microscopy images and label masks</p>
<p>Microscope: Spinning disk confocal microscope (3i CSU-W1) with a 20x objective, NA 0.8</p>
<p>Data Type: Fluorescence microscopy images of the AsPC1 Lifeact channel with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Fluorescence Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1024 pixels (Pixel size: 0.6337 x 0.6337 µm²)</p>
<p>Model Capabilities:</p>
<p>Segment AsPC1 Cells: Detects individual AsPC1 cells from a cluster and separates them from the background</p>
<p>Measure Intensity: Enables measurement of CD44, ICAM1, ICAM2, or Fibronectin intensity under individual cells in respective channels</p>
<p>Performance:</p>
<p>Average IoU: 0.884</p>
<p>Average F1 Score: 0.967</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a>
 </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/13442128">https://zenodo.org/records/13442128</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13442128">https://doi.org/10.5281/zenodo.13442128</a></p>
</section>
<hr class="docutils" />
<section id="stardist-bf-monocytes-dataset">
<h2>StarDist_BF_Monocytes_dataset<a class="headerlink" href="#stardist-bf-monocytes-dataset" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-01-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model and its training and validation datasets for detecting mononucleated cells perfused over an endothelial cell monolayer. The model was trained on 27 manually annotated images and achieved an average F1 Score of 0.941. The dataset and model are helpful for biomedical research, especially in studying interactions between mononucleated and endothelial cells.
Specifications</p>
<p>Model: StarDist for mononucleated cell detection on endothelial cells</p>
<p>Training Dataset:</p>
<p>Number of Images: 27 paired brightfield microscopy images and label masks</p>
<p>Microscope: Nikon Eclipse Ti2-E, 20x objective</p>
<p>Data Type: Brightfield microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1022 pixels (Pixel size: 650 nm)</p>
<p>Training Parameters:</p>
<p>Epochs: 400</p>
<p>Patch Size: 992 x 992 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.941</p>
<p>Average IoU: 0.831</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers
Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10572200">https://zenodo.org/records/10572200</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10572200">https://doi.org/10.5281/zenodo.10572200</a></p>
</section>
<hr class="docutils" />
<section id="stardist-bf-neutrophil-dataset">
<h2>StarDist_BF_Neutrophil_dataset<a class="headerlink" href="#stardist-bf-neutrophil-dataset" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-01-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model and its training and validation datasets for detecting neutrophils perfused over an endothelial cell monolayer. The model was trained on 36 manually annotated images, achieving an average F1 Score of 0.969. The dataset and model are intended for use in biomedical research, particularly for analyzing interactions between neutrophils and endothelial cells.
Specifications</p>
<p>Model: StarDist for neutrophil detection on endothelial cells</p>
<p>Training Dataset:</p>
<p>Number of Images: 36 paired brightfield microscopy images and label masks</p>
<p>Microscope: Nikon Eclipse Ti2-E, 20x objective</p>
<p>Data Type: Brightfield microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1022 pixels (Pixel size: 650 nm)</p>
<p>Training Parameters:</p>
<p>Epochs: 400</p>
<p>Patch Size: 992 x 992 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.969</p>
<p>Average IoU: 0.914</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10572231">https://zenodo.org/records/10572231</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10572231">https://doi.org/10.5281/zenodo.10572231</a></p>
</section>
<hr class="docutils" />
<section id="stardist-bf-cancer-cell-dataset-10x">
<h2>StarDist_BF_cancer_cell_dataset_10x<a class="headerlink" href="#stardist-bf-cancer-cell-dataset-10x" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-08-12</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model and its training dataset designed for segmenting cancer cells perfused over an endothelial cell monolayer captured at 10x magnification. The model was trained on 77 manually annotated images, with the dataset being computationally augmented during training by a factor of 8. The model was trained for 500 epochs and achieved an average F1 Score of 0.968, indicating high accuracy in segmenting cancer cells on endothelial cells.
Specifications</p>
<p>Model: StarDist for cancer cell segmentation on endothelial cells (10x magnification)</p>
<p>Training Dataset:</p>
<p>Number of Images: 77 paired brightfield microscopy images and label masks</p>
<p>Augmented Dataset: Computational augmentation by a factor of 8 during training</p>
<p>Microscope: Nikon Eclipse Ti2-E, 10x objective</p>
<p>Data Type: Brightfield microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit or 16-bit</p>
<p>Image Size: 1024 x 1022 pixels (pixel size: 1.3148 μm)</p>
<p>Training Parameters:</p>
<p>Epochs: 500</p>
<p>Patch Size: 992 x 992 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.968</p>
<p>Average IoU: 0.882</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/13304399">https://zenodo.org/records/13304399</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13304399">https://doi.org/10.5281/zenodo.13304399</a></p>
</section>
<hr class="docutils" />
<section id="stardist-bf-cancer-cell-dataset-20x">
<h2>StarDist_BF_cancer_cell_dataset_20x<a class="headerlink" href="#stardist-bf-cancer-cell-dataset-20x" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-01-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository contains a StarDist deep learning model and its training and validation datasets designed for segmenting cancer cells perfused over an endothelial cell monolayer captured at 20x magnification. Using computational methods, the initial dataset of 20 manually annotated images was augmented to 160 paired images. The model was trained over 400 epochs and achieved an average F1 Score of 0.921, demonstrating high accuracy in cell segmentation tasks.
Specifications</p>
<p>Model: StarDist for cancer cell segmentation on endothelial cells (20x magnification)</p>
<p>Training Dataset:</p>
<p>Number of Original Images: 20 paired brightfield microscopy images and label masks</p>
<p>Microscope: Nikon Eclipse Ti2-E, 20x objective</p>
<p>Data Type: Brightfield microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1022 pixels (Pixel size: 650 nm)</p>
<p>Training Parameters:</p>
<p>Epochs: 400</p>
<p>Patch Size: 992 x 992 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.921</p>
<p>Average IoU: 0.793</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p> </p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10572122">https://zenodo.org/records/10572122</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10572122">https://doi.org/10.5281/zenodo.10572122</a></p>
</section>
<hr class="docutils" />
<section id="stardist-fluorescent-cells">
<h2>StarDist_Fluorescent_cells<a class="headerlink" href="#stardist-fluorescent-cells" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-01-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model and its training and validation datasets for detecting fluorescently labeled cancer cells perfused over an endothelial cell monolayer. The model was trained on 66 images labeled with CellTrace and demonstrated high accuracy, achieving an average F1 Score of 0.877. The dataset and the trained model can be used for biomedical image analysis, particularly in cancer research.
Specifications</p>
<p>Model: StarDist for cancer cell detection</p>
<p>Training Dataset:</p>
<p>Number of Images: 66 paired fluorescent microscopy images and label masks</p>
<p>Microscope: Nikon Eclipse Ti2-E, 10x objective</p>
<p>Data Type: Fluorescent microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1024 pixels (Pixel size: 1.3205 μm)</p>
<p>Training Parameters:</p>
<p>Epochs: 200</p>
<p>Patch Size: 1024 x 1024 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.877</p>
<p>Average IoU: 0.646</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10572310">https://zenodo.org/records/10572310</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10572310">https://doi.org/10.5281/zenodo.10572310</a></p>
</section>
<hr class="docutils" />
<section id="stardist-huvec-nuclei-dataset">
<h2>StarDist_HUVEC_nuclei_dataset<a class="headerlink" href="#stardist-huvec-nuclei-dataset" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-02-05</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository contains a StarDist deep learning model and its training and validation datasets for segmenting endothelial nuclei while ignoring cancer cells. The cancer cells were perfused over an endothelial cell monolayer. The initial dataset consisted of 17 images, where cancer cell nuclei were manually removed after segmentation with the StarDist Versatile Nuclei model. This dataset was augmented to 68 paired images using computational techniques like rotation and flipping. The model was trained for 200 epochs, achieving an average F1 Score of 0.976, demonstrating high accuracy in segmenting endothelial nuclei while excluding cancer cells.
Specifications</p>
<p>Model: StarDist for segmenting endothelial nuclei while ignoring cancer cells</p>
<p>Training Dataset:</p>
<p>Number of Original Images: 17 paired predictions of nuclei and label images</p>
<p>Augmented Dataset: Expanded to 68 paired images using rotation and flipping</p>
<p>Source Image Generation: Generated using a pix2pix model trained to predict nuclei from brightfield images of cancer cells on top of an endothelium (DOI: 10.5281/zenodo.10617532)</p>
<p>Target Image Generation: Masks obtained via manual segmentation</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 8-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1022 pixels (uncalibrated)</p>
<p>Training Parameters:</p>
<p>Epochs: 200</p>
<p>Patch Size: 1024 x 1024 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.976</p>
<p>Average IoU: 0.927</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10617532">https://zenodo.org/records/10617532</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10617532">https://doi.org/10.5281/zenodo.10617532</a></p>
</section>
<hr class="docutils" />
<section id="stardist-tumorcell-nuclei">
<h2>StarDist_TumorCell_nuclei<a class="headerlink" href="#stardist-tumorcell-nuclei" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-08-29</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository contains a StarDist deep learning model designed for segmenting tumor cell nuclei from the DAPI channel in fluorescence microscopy images while excluding HUVEC nuclei. The model was trained to accurately detect individual tumor cell nuclei for subsequent measurement of CD44, ICAM1, ICAM2, or Fibronectin intensity around or under the nuclei. The model achieved an Intersection over Union (IoU) score of 0.558 and an F1 Score of 0.793, reflecting its capability to distinguish tumor cell nuclei from HUVEC nuclei.
Specifications</p>
<p>Model: StarDist for segmenting tumor cell nuclei from the DAPI fluorescence channel</p>
<p>Training Dataset:</p>
<p>Number of Images: 48 paired fluorescence microscopy images and label masks</p>
<p>Microscope: Spinning disk confocal microscope (3i CSU-W1) with a 20x objective, NA 0.8</p>
<p>Data Type: Fluorescence microscopy images of the DAPI channel with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Fluorescence Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 920 x 920 pixels (Pixel size: 0.6337 x 0.6337 µm²)</p>
<p>Model Capabilities:</p>
<p>Segment Tumor Cell Nuclei: Detects individual tumor cell nuclei in the DAPI channel while distinguishing them from HUVEC nuclei</p>
<p>Measure Intensity: Enables measurement of CD44, ICAM1, ICAM2, or Fibronectin intensity around or under tumor cell nuclei in respective channels</p>
<p>Performance:</p>
<p>Average IoU: 0.558</p>
<p>Average F1 Score: 0.793</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a>
 </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/13443221">https://zenodo.org/records/13443221</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13443221">https://doi.org/10.5281/zenodo.13443221</a></p>
</section>
<hr class="docutils" />
<section id="stardist-model-and-training-dataset-for-automated-tracking-of-mda-mb-231-and-bt20-cells">
<h2>Stardist model and training dataset for automated tracking of MDA-MB-231 and BT20 cells<a class="headerlink" href="#stardist-model-and-training-dataset-for-automated-tracking-of-mda-mb-231-and-bt20-cells" title="Link to this heading">#</a></h2>
<p>Hussein Al-Akhrass, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2021-05-26</p>
<p>Licensed CC-BY-4.0</p>
<p>StarDist Model:
The StarDist model was generated using the ZeroCostDL4Mic platform (Chamier et al., 2021). This custom StarDist model was trained for 300 epochs using 46 manually annotated paired images (image dimensions: (1024, 1024)) with a batch size of 2, an augmentation factor of 4 and a mae loss function. The StarDist “Versatile fluorescent nuclei” model was used as a training starting point. Key python packages used include TensorFlow (v 0.1.12), Keras (v 2.3.1), CSBdeep (v 0.6.1), NumPy (v 1.19.5), Cuda (v 11.0.221). The training was accelerated using a Tesla P100GPU.
The model weights can be used in the ZeroCostDL4Mic StarDist 2D notebook or in the StarDist Fiji plugin.</p>
<p>StarDist Training dataset:
Paired microscopy images (fluorescence) and corresponding masks</p>
<p>Microscopy data type: Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Cells were imaged using a 20x Nikon CFI Plan Apo Lambda objective (NA 0.75) one frame every 10 minutes for 16h.</p>
<p>Cell type: MDA-MB-231 cells and BT20 cells</p>
<p>File format: .tif (16-bit for fluorescence and 8 and 16-bit for the masks)</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4811213">https://zenodo.org/records/4811213</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4811213">https://doi.org/10.5281/zenodo.4811213</a></p>
</section>
<hr class="docutils" />
<section id="stardist-miapaca2-from-cd44">
<h2>Stardist_MiaPaCa2_from_CD44<a class="headerlink" href="#stardist-miapaca2-from-cd44" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-08-29</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository contains a StarDist deep learning model designed for segmenting MiaPaCa2 cells from the CD44 channel in fluorescence microscopy images. The model is capable of accurately segmenting individual MiaPaCa2 cells while excluding HUVECs. Trained on a small dataset, the model achieved an Intersection over Union (IoU) score of 0.884 and an F1 Score of 0.950, indicating high precision in cell segmentation.
Specifications</p>
<p>Model: StarDist for segmenting MiaPaCa2 cells from the CD44 fluorescence channel</p>
<p>Training Dataset:</p>
<p>Number of Images: 8 paired fluorescence microscopy images and label masks</p>
<p>Microscope: Spinning disk confocal microscope (3i CSU-W1) with a 20x objective, NA 0.8</p>
<p>Data Type: Fluorescence microscopy images of the CD44 channel, obtained after immunofluorescence staining with primary and secondary antibodies and manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Fluorescence Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 920 x 920 pixels (Pixel size: 0.6337 x 0.6337 µm²)</p>
<p>Model Capabilities:</p>
<p>Segment MiaPaCa2 Cells: Accurately detects individual MiaPaCa2 cells while ignoring HUVECs</p>
<p>Measure CD44 Intensity: Allows for the measurement of CD44 intensity around MiaPaCa2 cells, specifically from the CD44 channel</p>
<p>Performance:</p>
<p>Average IoU: 0.884</p>
<p>Average F1 Score: 0.950</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/13442877">https://zenodo.org/records/13442877</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13442877">https://doi.org/10.5281/zenodo.13442877</a></p>
</section>
<hr class="docutils" />
<section id="structuring-of-data-and-metadata-in-bioimaging-concepts-and-technical-solutions-in-the-context-of-linked-data">
<h2>Structuring of Data and Metadata in Bioimaging: Concepts and technical Solutions in the Context of Linked Data<a class="headerlink" href="#structuring-of-data-and-metadata-in-bioimaging-concepts-and-technical-solutions-in-the-context-of-linked-data" title="Link to this heading">#</a></h2>
<p>Sarah Weischer, Jens Wendt, Thomas Zobel</p>
<p>Published 2022-07-12</p>
<p>Licensed CC-BY-4.0</p>
<p>Provides an overview of contexts, frameworks, and models from the world of bioimage data as well as metadata. Visualizes the techniques for structuring this data as Linked Data. (Walkthrough Video: <a class="reference external" href="https://doi.org/10.5281/zenodo.7018928">https://doi.org/10.5281/zenodo.7018928</a> )</p>
<p>Content:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Types of metadata
Data formats
Data Models Microscopy Data
Tools to edit/gather metadata
ISA Framework
FDO Framework
Ontology
RDF
JSON-LD
SPARQL
Knowledge Graph
Linked Data
Smart Data
...
</pre></div>
</div>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/7018750">https://zenodo.org/records/7018750</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7018750">https://doi.org/10.5281/zenodo.7018750</a></p>
</section>
<hr class="docutils" />
<section id="sustainable-data-stewardship">
<h2>Sustainable Data Stewardship<a class="headerlink" href="#sustainable-data-stewardship" title="Link to this heading">#</a></h2>
<p>Stefano Della Chiesa</p>
<p>Published 2024-03-25</p>
<p>Licensed CC-BY-4.0</p>
<p>These slides were presented at the 2. SaxFDM-Beratungsstammtisch and delve into the strategic integration of Research Data Management (RDM) within research organizations. The Leibniz IOER presented an insightful overview of RDM activities and approaches, emphasizing the criticality of embedding RDM strategically within research institutions. The presentation showcases some best practices in RDM implementation through practical examples, offering valuable insights for optimizing data stewardship processes.</p>
<p>Tags: Research Data Management, Data Stewardship</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/records/10942559">https://zenodo.org/records/10942559</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10942559">https://doi.org/10.5281/zenodo.10942559</a></p>
</section>
<hr class="docutils" />
<section id="synapsenet-training-data">
<h2>SynapseNet Training Data<a class="headerlink" href="#synapsenet-training-data" title="Link to this heading">#</a></h2>
<p>Constantin Pape</p>
<p>Published 2024-12-01</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset contains room-temperature single-axis TEM tomograms from Schaffer collateral and mossy fiber synapses in organotypic hippocampal slices. The tomograms were published in the two studies [1, 2]. The data was re-used for training deep neural networks to segment different synaptic structures in electron micrographs in [3]. For the tomograms, organotypic slices were prepared from the hippocampi of neonatal mice according to the interface protocol55 and vitrified after 28 days in vitro in culture medium supplemented with 20% (w/v) bovine serum albumin using an HPM100 (Leica) high-pressure freezing device. The dataset also contains 23 tomograms resulting from chemically-fixed material, which were also published in (Maus et al., 2020). For these tomograms, wild-type animals at postnatal day 28 were transcardially perfused under deep anesthesia, first with 0.9% sodium chloride solution, and then one of two fixatives (Fixative 1: Ice-cold 4% paraformaldehyde, 2.5% glutaraldehyde in 0.1 M phosphate buffer16; Fixative 2: 37° C 2% paraformaldehyde, 2.5% glutaraldehyde, 2 mM CaCl2, in 0.1 M cacodylate buffer56). Brains were rinsed and sectioned coronally through the dorsal hippocampus in an ice-cold 0.1 M phosphate buffer using a VT 1200S vibratome (Leica) (step size 100 µm; amplitude 1.5 mm, speed 0.1 mm/sec). Hippocampal CA3 subregions were excised using a 1.5 mm diameter biopsy punch and high-pressure frozen on the same day in 20% (w/v) bovine serum albumin using an HPM100 (Leica) high-pressure freezing device. For both sample preparations, automated freeze-substitution was performed. Tomograms were collected using a 200 kV JEM-2100 (JEOL) transmission electron microscope equipped with an 11 MP Orius SC1000 CCD camera (Gatan). Tilt-series (tilt range +/- 60°; 1° angular increments) were acquired at 30 000x magnification using SerialEM58. Tomographic reconstructions were generated using weighted back-projection with etomo.The data is organized into two different subfolders for data with annotations for “vesicles” and “active_zones”. Each of these subfolders is further subdivided into “train” and “test” folders, which containtomograms for the two different sample preparations in “chemical_fixation” and “single_axis_tem”.Each tomogram and the corresponding annotation is stored as a hdf5 file, containing the following internal datasets:- raw: The tomogram data.- labels/vesicles: Annotations for the synaptic vesicles, annotated with IMOD, further postprocessed and then exported to instance masks. (for tomograms in “vesicles”)- labels/AZ: Annotations for the active zone, annotated with IMOD and exported to binary masks.
[1] Imig et al., The Morphological and Molecular Nature of Synaptic Vesicle Priming at Presynaptic Active Zones, Neuron, 2014, DOI:10.1016/j.neuron.2014.10.009[2] Maus et al., Ultrastructural Correlates of Presynaptic Functional Heterogeneity in Hippocampal Synapses, Cell Reports, 2020, DOI: 10.1016/j.celrep.2020.02.083[3] Muth, Moschref et al., 2024, Preprint to be published</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/14330011">https://zenodo.org/records/14330011</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14330011">https://doi.org/10.5281/zenodo.14330011</a></p>
</section>
<hr class="docutils" />
<section id="terminology-service-for-research-data-management-and-knowledge-discovery-in-low-temperature-plasma-physics">
<h2>Terminology service for research data management and knowledge discovery in low-temperature plasma physics<a class="headerlink" href="#terminology-service-for-research-data-management-and-knowledge-discovery-in-low-temperature-plasma-physics" title="Link to this heading">#</a></h2>
<p>Markus M. Becker, Ihda Chaerony Siffa, Roman Baum</p>
<p>Published 2024-12-11</p>
<p>Licensed CC-BY-4.0</p>
<p>Abstract:
Terminology services (TS) [1,2] play a pivotal role in achieving structured metadata by providing controlled vocabularies and ontologies that standardize the description of data. This is a crucial aspect of research data management (RDM) in all scientific disciplines. In addition, TS facilitate the use of a common vocabulary within a scientific community also in a more general context, e.g. to annotate scientific papers, patents or other content for better discoverability, as envisaged by the Open Research Knowledge Graph (ORKG) [3] or the Patents4Science project [4]. 
To make use of these opportunities, terminologies, ontologies and knowledge graphs must be developed and made available as TS where they do not yet exist. This step is currently being taken by the research community in low-temperature plasma (LTP) physics. LTP physics explores partially ionized gases and its technological applications. This vibrant field offers innovative solutions for societal challenges, ranging from developing efficient lighting and solar cells to revolutionizing healthcare through plasma medicine. Various activities and projects have been started in the past years to support the RDM in LTP research and development and to facilitate the application of data-driven research methods. These activities are supported in parts by the NFDI4BIOIMAGE consortium, active work in the NFDI section “(Meta)data, Terminologies, Provenance”, and the basic service Terminology Services 4 NFDI (TS4NFDI) funded by Base4NFDI. 
Recently, the ontology Plasma-O [5–7] for LTP physics has been developed at INP in collaboration with FIZ Karlsruhe – Leibniz Institute for Information Infrastructure, providing a framework for structuring metadata and building a knowledge graph for scientific information within the field. The present contribution will show how a TS utilizing this resource can support different aspects of RDM and knowledge discovery using concrete examples. The application cases include (i) standardizing data annotation: By providing researchers with a controlled vocabulary of LTP-specific terms and their relationships, ensuring consistent and unambiguous data descriptions; (ii) enabling semantic search: Moving beyond keyword-based searches, TS allow for complex queries based on the relationships between concepts, significantly improving data discoverability; (iii) facilitating data integration: By mapping data from different sources to a common ontology, TS enable seamless integration and analysis of heterogeneous datasets, which is crucial for data-driven research and development. The TS Suite of TS4NFDI with the provided widgets [8] fits perfectly to the requirements of these three application cases and will support the harmonization of metadata in LTP physics. The implementation of a public TS is required to provide the domain-specific metadata in a standardized format and will be instrumental in unlocking the full potential of the TS widgets for RDM and knowledge discovery by LTP researchers. Furthermore, the results can provide insights to other domains on how to apply TS to their specific needs.
 The work was supported in parts by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the National Research Data Infrastructure – [NFDI46/1] – 501864659 and project number 496963457 as well as by the Federal Ministry of Education and Research (BMBF), project number 16KOA013A.
References:</p>
<p>[1]</p>
<p>S. Jupp, T. Burdett, C. Leroy, H. Parkinson, “A new Ontology Lookup Service at EMBL-EBI”, Workshop on Semantic Web Applications and Tools for Life Sciences (2015), <a class="reference external" href="https://ceur-ws.org/Vol-1546/paper_29.pdf">https://ceur-ws.org/Vol-1546/paper_29.pdf</a> (accessed: 2024-09-20).</p>
<p>[2]</p>
<p>P. L. Whetzel, N. F. Noy, N. H. Shah, P. R. Alexander, C. Nyulas, T. Tudorache, M. A. Musen, “BioPortal: enhanced functionality via new Web services from the National Center for Biomedical Ontology to access and use ontologies in software applications”, Nucleic Acids Res. 39 (2011) W541–W545, <a class="reference external" href="https://doi.org/10.1093/nar/gkr469">https://doi.org/10.1093/nar/gkr469</a>.</p>
<p>[3]</p>
<p>Open Research Knowledge Graph, <a class="reference external" href="https://orkg.org/">https://orkg.org/</a> (accessed: 2024-09-20).</p>
<p>[4]</p>
<p>Patents4Science – Establishing an Information Infrastructure for the Use of Patent Knowledge in Science, <a class="reference external" href="https://www.patents4science.org/">https://www.patents4science.org/</a> (accessed: 2024-09-20).</p>
<p>[5]</p>
<p>H. Sack, F. Hoppe, “Verbundprojekt: Qualitätssicherung und Vernetzung von Forschungsdaten in der Plasmatechnologie - QPTDat; Teilvorhaben: Wissensgraph und Ontologieentwicklung zur Vernetzung von Metadaten : Schlussbericht des Teilvorhabens”, 2023, <a class="reference external" href="https://doi.org/10.2314/KXP:1883436974">https://doi.org/10.2314/KXP:1883436974</a>.</p>
<p>[6]</p>
<p>I. Chaerony Siffa, R. Wagner, L. Vilardell Scholten, M. M. Becker, “Semantic Information Management in Low-Temperature Plasma Science and Technology with VIVO”, 2024, preprint, <a class="reference external" href="https://doi.org/10.48550/arXiv.2409.11065">https://doi.org/10.48550/arXiv.2409.11065</a>.</p>
<p>[7]</p>
<p>I. Chaerony Siffa, R. Wagner, L. Vilardell Scholten, M. M. Becker, “Plasma Ontology and Knowledge Graph Initial Release v0.5.0”, 2024, Zenodo, <a class="reference external" href="https://doi.org/10.5281/zenodo.13325226">https://doi.org/10.5281/zenodo.13325226</a>.</p>
<p>[8]</p>
<p>J. Sasse, V. Kneip, R. Baum, P. Zimmermann, J. Darms, J. Schneider, V. Clemens, P. Oladazimi, L. Kühnel, “ts4nfdi/terminology-service-suite: v2.6.0”, 2024, Zenodo, <a class="reference external" href="https://doi.org/10.5281/zenodo.13692297">https://doi.org/10.5281/zenodo.13692297</a>.</p>
<p><a class="reference external" href="https://zenodo.org/records/14381522">https://zenodo.org/records/14381522</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14381522">https://doi.org/10.5281/zenodo.14381522</a></p>
</section>
<hr class="docutils" />
<section id="test-dataset-for-whole-slide-image-registration">
<h2>Test Dataset for Whole Slide Image Registration<a class="headerlink" href="#test-dataset-for-whole-slide-image-registration" title="Link to this heading">#</a></h2>
<p>Romain Guiet, Nicolas Chiaruttini</p>
<p>Published 2021-04-12</p>
<p>Licensed CC-BY-4.0</p>
<p>Mouse duodenum fixed in 4% PFA overnight at 4°C, processed for paraffin infiltration using a standard histology procedure and cut at 4 microns were dewaxed, rehydrated, permeabilized with 0.5% Triton X-100 in PBS 1x and stained with Azide - Alexa Fluor 555 (Thermo Fisher) to detect EdU and DAPI for nuclei. The images were taken using a Leica DM5500 microscope with a 40X N.A.1 objective (black&amp;white camera: DFC350FXR2, pixel dimension: 0.161 microns). Next, the slide was unmounted and stained using the fully automated Ventana Discovery xT autostainer (Roche Diagnostics, Rotkreuz, Switzerland). All steps were performed on automate with Ventana solutions. Sections were pretreated with heat using the CC1 solution under mild conditions. The primary rat anti BrDU (clone: BU1/75 (ICR1), Serotec, diluted 1:300) was incubated 1 hour at 37°C. After incubation with a donkey anti rat biotin diluted 1:200 (Jackson ImmunoResearch Laboratories), chromogenic revelation was performed with DabMap kit. The section was counterstained with Harris hematoxylin (J.T. Baker) before a second round of imaging on DM5500 PL Fluotar 40X N.A.1.0 oil (color camera: DFC 320 R2, pixel dimension: 0.1725 microns). Before acquisition, a white-balance as well as a shading correction is performed according to Leica LAS software wizard. The fluorescence and DAB images were converted in ome.tiff multiresolution file with the kheops Fiji Plugin.</p>
<p>Sampled prepared in the EPFL histology core facility by Nathalie Müller and Gian-Filippo Mancini.</p>
<p>Associated documents:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>https://c4science.ch/w/bioimaging_and_optics_platform_biop/teaching/dab-intensity/
https://imagej.net/plugins/bdv/warpy/warpy
</pre></div>
</div>
<p>This document contains a full QuPath project with an example of registered image.</p>
<p> </p>
<p><a class="reference external" href="https://zenodo.org/records/5675686">https://zenodo.org/records/5675686</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5675686">https://doi.org/10.5281/zenodo.5675686</a></p>
</section>
<hr class="docutils" />
<section id="the-information-infrastructure-for-bioimage-data-i3d-bio-project-to-advance-fair-microscopy-data-management-for-the-community">
<h2>The Information Infrastructure for BioImage Data (I3D:bio) project to advance FAIR microscopy data management for the community<a class="headerlink" href="#the-information-infrastructure-for-bioimage-data-i3d-bio-project-to-advance-fair-microscopy-data-management-for-the-community" title="Link to this heading">#</a></h2>
<p>Christian Schmidt, Michele Bortolomeazzi, Tom Boissonnet, Julia Dohle, Tobias Wernet, Janina Hanne, Roland Nitschke, Susanne Kunis, Karen Bernhardt, Stefanie Weidtkamp-Peters, Elisa Ferrando-May</p>
<p>Published 2024-03-04</p>
<p>Licensed CC-BY-4.0</p>
<p>Research data management (RDM) in microscopy and image analysis is a challenging task. Large files in proprietary formats, complex N-dimensional array structures, and various metadata models and formats can make image data handling inconvenient and difficult. For data organization, annotation, and sharing, researchers need solutions that fit everyday practice and comply with the FAIR (Findable, Accessible, Interoperable, Reusable) principles. International community-based efforts have begun creating open data models (OME), an open file format and translation library (OME-TIFF, Bio-Formats), data management software platforms, and microscopy metadata recommendations and annotation tools. Bringing these developments into practice requires support and training. Iterative feedback and tool improvement is needed to foster practical adoption by the scientific community. The Information Infrastructure for BioImage Data (I3D:bio) project works on guidelines, training resources, and practical assistance for FAIR microscopy RDM adoption with a focus on the management platform OMERO and metadata annotations.</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/10805204">https://zenodo.org/records/10805204</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10805204">https://doi.org/10.5281/zenodo.10805204</a></p>
</section>
<hr class="docutils" />
<section id="the-role-of-helmholtz-centers-in-nfdi4bioimage-a-national-consortium-enhancing-fair-data-management-for-microscopy-and-bioimage-analysis">
<h2>The role of Helmholtz Centers in NFDI4BIOIMAGE - A national consortium enhancing FAIR data management for microscopy and bioimage analysis<a class="headerlink" href="#the-role-of-helmholtz-centers-in-nfdi4bioimage-a-national-consortium-enhancing-fair-data-management-for-microscopy-and-bioimage-analysis" title="Link to this heading">#</a></h2>
<p>Riccardo Massei, Christian Schmidt, Michele Bortolomeazzi, Julia Thoennissen, Jan Bumberger, Timo Dickscheid, Jan-Philipp Mallm, Elisa Ferrando-May</p>
<p>Published 2024-06-06</p>
<p>Licensed CC-BY-4.0</p>
<p>Germany’s National Research Data Infrastructure (NFDI) aims to establish a sustained, cross-disciplinary research data management (RDM) infrastructure that enables researchers to handle FAIR (findable, accessible, interoperable, reusable) data. While FAIR principles have been adopted by funders, policymakers, and publishers, their practical implementation remains an ongoing effort. In the field of bio-imaging, harmonization of data formats, metadata ontologies, and open data repositories is necessary to achieve FAIR data. The NFDI4BIOIMAGE was established to address these issues and develop tools and best practices to facilitate FAIR microscopy and image analysis data in alignment with international community activities. The consortium operates through its Data Stewards team to provide expertise and direct support to help overcome RDM challenges. The three Helmholtz Centers in NFDI4BIOIMAGE aim to collaborate closely with other centers and initiatives, such as HMC, Helmholtz AI, and HIP. Here we present NFDI4BIOIMAGE’s work and its significance for research in Helmholtz and beyond</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/11501662">https://zenodo.org/records/11501662</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11501662">https://doi.org/10.5281/zenodo.11501662</a></p>
</section>
<hr class="docutils" />
<section id="thinking-data-management-on-different-scales">
<h2>Thinking data management on different scales<a class="headerlink" href="#thinking-data-management-on-different-scales" title="Link to this heading">#</a></h2>
<p>Susanne Kunis</p>
<p>Licensed CC-BY-4.0</p>
<p>Presentation given at PoL BioImage Analysis Symposium Dresden 2023</p>
<p>Tags: Research Data Management, Nfdi4Bioimage</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.8329305">https://zenodo.org/doi/10.5281/zenodo.8329305</a></p>
</section>
<hr class="docutils" />
<section id="towards-preservation-of-life-science-data-with-nfdi4bioimage">
<h2>Towards Preservation of Life Science Data with NFDI4BIOIMAGE<a class="headerlink" href="#towards-preservation-of-life-science-data-with-nfdi4bioimage" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-09-03</p>
<p>Licensed CC-BY-4.0</p>
<p>This talk will present the initiatives of the NFDI4BioImage consortium aimed at the long-term preservation of life science data. We will discuss our efforts to establish metadata standards, which are crucial for ensuring data reusability and integrity. The development of sustainable infrastructure is another key focus, enabling seamless data integration and analysis in the cloud. We will take a look at how we manage training materials and communicate with our community. Through these actions, NFDI4BioImage seeks to enable FAIR bioimage data management for German researchers, across disciplines and embedded in the international framework.</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/13640979">https://zenodo.org/records/13640979</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13640979">https://doi.org/10.5281/zenodo.13640979</a></p>
</section>
<hr class="docutils" />
<section id="towards-transparency-and-knowledge-exchange-in-ai-assisted-data-analysis-code-generation">
<h2>Towards Transparency and Knowledge Exchange in AI-assisted Data Analysis Code Generation<a class="headerlink" href="#towards-transparency-and-knowledge-exchange-in-ai-assisted-data-analysis-code-generation" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-10-14</p>
<p>Licensed CC-BY-4.0</p>
<p>The integration of Large Language Models (LLMs) in scientific research presents both opportunities and challenges for life scientists. Key challenges include ensuring transparency in AI-generated content and facilitating efficient knowledge exchange among researchers. These issues arise from the in-transparent nature of AI-driven code generation and the informal sharing of AI insights, which may hinder reproducibility and collaboration. This paper introduces git-bob, an innovative AI-assistant designed to address these challenges by fostering an interactive and transparent collaboration platform within GitHub. By enabling seamless dialogue between humans and AI, git-bob ensures that AI contributions are transparent and reproducible. Moreover, it supports collaborative knowledge exchange, enhancing the interdisciplinary dialogue necessary for cutting-edge life sciences research. The open-source nature of git-bob further promotes accessibility and customization, positioning it as a vital tool in employing LLMs responsibly and effectively within scientific communities.</p>
<p><a class="reference external" href="https://zenodo.org/records/13928832">https://zenodo.org/records/13928832</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13928832">https://doi.org/10.5281/zenodo.13928832</a></p>
</section>
<hr class="docutils" />
<section id="towards-open-and-standardised-imaging-data-an-introduction-to-bio-formats-ome-tiff-and-ome-zarr">
<h2>Towards open and standardised imaging data: an introduction to Bio-Formats, OME-TIFF, and OME-Zarr<a class="headerlink" href="#towards-open-and-standardised-imaging-data-an-introduction-to-bio-formats-ome-tiff-and-ome-zarr" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Published 2025-05-28</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://www.ebi.ac.uk/training/events/towards-open-and-standardised-imaging-data-introduction-bio-formats-ome-tiff-and-ome-zarr/">https://www.ebi.ac.uk/training/events/towards-open-and-standardised-imaging-data-introduction-bio-formats-ome-tiff-and-ome-zarr/</a>
Microscopy and bioimaging technologies are fundamental tools for exploring biological systems, generating large, multidimensional datasets rich in experimental detail. However, the bioimaging community has historically faced major challenges around data handling: vendor-specific proprietary formats, fragmented metadata storage, and increasingly large dataset sizes that outstrip traditional storage and computing solutions.
In this webinar, key open technologies developed by the Open Microscopy Environment (OME) to address these challenges were presented. Specifically, the Bio-Formats library for accessing diverse proprietary file formats, the OME-TIFF standard for archival data storage, and the OME-Zarr format for cloud-native, scalable bioimaging workflows were presented.</p>
<p><a class="reference external" href="https://zenodo.org/records/15479606">https://zenodo.org/records/15479606</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15479606">https://doi.org/10.5281/zenodo.15479606</a></p>
</section>
<hr class="docutils" />
<section id="train-the-trainer-concept-on-research-data-management">
<h2>Train-the-Trainer Concept on Research Data Management<a class="headerlink" href="#train-the-trainer-concept-on-research-data-management" title="Link to this heading">#</a></h2>
<p>Katarzyna Biernacka, Maik Bierwirth, Petra Buchholz, Dominika Dolzycka, Kerstin Helbig, Janna Neumann, Carolin Odebrecht, Cord Wiljes, Ulrike Wuttke</p>
<p>Published 2020-11-04</p>
<p>Licensed CC-BY-4.0</p>
<p>Within the project FDMentor, a German Train-the-Trainer Programme on Research Data Management (RDM) was developed and piloted in a series of workshops. The topics cover many aspects of research data management, such as data management plans and the publication of research data, as well as didactic units on learning concepts, workshop design and a range of didactic methods.</p>
<p>After the end of the project, the concept was supplemented and updated by members of the Sub-Working Group Training/Further Education (UAG Schulungen/Fortbildungen) of the DINI/nestor Working Group Research Data (DINI/nestor-AG Forschungsdaten). The newly published English version of the Train-the-Trainer Concept contains the translated concept, the materials and all methods of the Train-the-Trainer Programme. Furthermore, additional English references and materials complement this version.</p>
<p>Tags: Research Data Management</p>
<p>Content type: Book</p>
<p><a class="reference external" href="https://zenodo.org/record/4071471">https://zenodo.org/record/4071471</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4071471">https://doi.org/10.5281/zenodo.4071471</a></p>
</section>
<hr class="docutils" />
<section id="training-computational-skills-in-the-age-of-ai">
<h2>Training Computational Skills in the Age of AI<a class="headerlink" href="#training-computational-skills-in-the-age-of-ai" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2024-11-06</p>
<p>Licensed CC-BY-4.0</p>
<p>Artificial intelligence (AI) and large language models (LLMs) are changing the way we use computers in science. This slide deck introduces ways for using AI and LLMs for making training materials and for exchanging knowledge about how to use AI in joint discussions between humans and LLM-based AI-systems.</p>
<p><a class="reference external" href="https://zenodo.org/records/14043615">https://zenodo.org/records/14043615</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14043615">https://doi.org/10.5281/zenodo.14043615</a></p>
</section>
<hr class="docutils" />
<section id="training-set-of-microscopy-images-for-dietler-et-al-nature-communications-2020">
<h2>Training set of microscopy images for Dietler et al. Nature Communications 2020<a class="headerlink" href="#training-set-of-microscopy-images-for-dietler-et-al-nature-communications-2020" title="Link to this heading">#</a></h2>
<p>Nicola Dietler, Matthias Minder, Vojislav Gligorovski, Economou, Augoustina Maria, Joly, Denis Alain Henri Lucien, Ahmad Sadeghi, Chan, Chun Hei Michael, Mateusz Kozinski, Martin Weigert, Anne-Florence Bitbol, Rahi, Sahand Jamal</p>
<p>Published 2021-12-07</p>
<p>Licensed CC-BY-4.0</p>
<p>Training set of microscopy images for Dietler et al. Nature Communications 2020</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5765648">https://zenodo.org/records/5765648</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5765648">https://doi.org/10.5281/zenodo.5765648</a></p>
</section>
<hr class="docutils" />
<section id="vision-language-models-for-bio-image-data-science">
<h2>Vision Language Models for Bio-image Data Science<a class="headerlink" href="#vision-language-models-for-bio-image-data-science" title="Link to this heading">#</a></h2>
<p>Robert Haase</p>
<p>Published 2025-06-25</p>
<p>Licensed CC-BY-4.0</p>
<p>In this talk, I demonstrate potential use-cases for vision-language models (VLM) in bio-image data science, focusing on how to analyse microscopy image data. It covers these use-cases:</p>
<p>cell counting
bounding-box segmentation
image descriptions
VLMs guessing which algorithm to use for processing
Data analysis code generation
Answering github issues </p>
<p>The talk also points at a number of VLM-based open-source tools which start reshaping the scientific bio-image data science domain:</p>
<p>bia-bob
unprompted
git-bob
napari-chatgpt
<a class="reference external" href="http://bioimage.io">bioimage.io</a> chatbot</p>
<p><a class="reference external" href="https://zenodo.org/records/15735577">https://zenodo.org/records/15735577</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15735577">https://doi.org/10.5281/zenodo.15735577</a></p>
</section>
<hr class="docutils" />
<section id="volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset">
<h2>Volumetric segmentation of biological cells and subcellular structures for optical diffraction tomography images - dataset<a class="headerlink" href="#volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset" title="Link to this heading">#</a></h2>
<p>Martyna Mazur, Wojciech Krauze</p>
<p>Published 2023-06-16</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset includes 4 files with segmentation results for 4 different ODT reconstructions of SH-SY5Y neuroblastoma cell. The segmentation results contain:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>3D binary masks of biological cells obtained through Cellpose [1] and ODT-SAS;
3D binary masks of organelles: nucleoli and lipid structures (LS) obtained through slice-by-slice manual segmentation&amp;nbsp;and ODT-SAS.
</pre></div>
</div>
<p>All files are .*mat files.</p>
<p>The files REC_SH-SY5Y_1.mat, REC_SH-SY5Y_2.mat and REC_SH-SY5Y_3.mat consist of 7 variables:</p>
<p>RECON – tomographic reconstruction of SH-SY5Y neuroblastoma cell;
n_imm – refractive index of object immersion medium;
dx – object space sample size in XY [(\mu m)];
rayXY – xy-coordinates of illumination vectors;</p>
<p>maskManual – table with manually determined 3D binary masks of organelles;
maskCellpose – 3D binary mask of biological cell obtained through Cellpose;
maskODTSAS – table with 3D binary masks of biological cell and their organelles obtained through ODT-SAS.</p>
<p>File REC_SH-SY5Y_4.mat includes masks for the ODT-SAS and Cellpose segmentation of three closely packed cells and consists of 5 variables: RECON, n_imm, dx, maskCellpose and maskODTSAS.</p>
<p>Access a particular 3D binary mask from ‘maskManual’ and ‘maskODTSAS’ tables, using the following names: ‘Cell’, ‘Nucleoli’, ‘LS’.
For example:</p>
<p>cellMask = maskODTSAS.Cell{1};</p>
<p>[1] Stringer, C., Wang, T., Michaelos, M., &amp; Pachitariu, M. (2021). Cellpose: a generalist algorithm for cellular segmentation. Nature methods, 18(1), 100-106.</p>
<p> </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8188948">https://zenodo.org/records/8188948</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8188948">https://doi.org/10.5281/zenodo.8188948</a></p>
</section>
<hr class="docutils" />
<section id="what-not-to-do-when-creating-a-data-management-plan-dmp">
<h2>WHAT NOT TO DO WHEN CREATING A DATA MANAGEMENT PLAN (DMP)<a class="headerlink" href="#what-not-to-do-when-creating-a-data-management-plan-dmp" title="Link to this heading">#</a></h2>
<p>Georgia Koutentaki, Martin Schätz, Jan Vališ</p>
<p>Published 2025-05-14</p>
<p>Licensed CC-BY-4.0</p>
<p><a class="reference external" href="https://zenodo.org/records/15402904">https://zenodo.org/records/15402904</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15402904">https://doi.org/10.5281/zenodo.15402904</a></p>
</section>
<hr class="docutils" />
<section id="welcome-to-bioimage-town">
<h2>Welcome to BioImage Town<a class="headerlink" href="#welcome-to-bioimage-town" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Licensed CC-BY-4.0</p>
<p>Welcome at NFDI4BIOIMAGE All-Hands Meeting in Düsseldorf, Germany, October 16, 2023</p>
<p>Tags: OMERO, Bioimage Analysis, Nfdi4Bioimage</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.10008464">https://zenodo.org/doi/10.5281/zenodo.10008464</a></p>
</section>
<hr class="docutils" />
<section id="who-you-gonna-call-data-stewards-to-the-rescue">
<h2>Who you gonna call? - Data Stewards to the rescue<a class="headerlink" href="#who-you-gonna-call-data-stewards-to-the-rescue" title="Link to this heading">#</a></h2>
<p>Vanessa Aphaia Fiona Fuchs, Jens Wendt, Maximilian Müller, Mohsen Ahmadi, Riccardo Massei, Cornelia Wetzker</p>
<p>Published 2024-03-01</p>
<p>Licensed CC-BY-4.0</p>
<p>The Data Steward Team of the NFDI4BIOIMAGE consortium presents themselves and the services (including the Helpdesk) that we offer.</p>
<p><a class="reference external" href="https://zenodo.org/records/10730424">https://zenodo.org/records/10730424</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10730424">https://doi.org/10.5281/zenodo.10730424</a></p>
</section>
<hr class="docutils" />
<section id="workflow-for-user-introduction-into-microscopy-omero-and-data-management-at-center-for-advanced-imaging">
<h2>Workflow for user introduction into microscopy, OMERO and data management at Center for Advanced imaging<a class="headerlink" href="#workflow-for-user-introduction-into-microscopy-omero-and-data-management-at-center-for-advanced-imaging" title="Link to this heading">#</a></h2>
<p>Ksenia Krooß, Fuchs, Vanessa Aphaia Fiona, Tom Boissonnet, Stefanie Weidtkamp-Peters</p>
<p>Published 2025-03-07</p>
<p>Licensed CC-BY-4.0</p>
<p>At the Center for Advanced Imaging (CAi) at the Heinrich Heine University Düsseldorf, Germany, we have established a workflow to guide users through all aspects of bioimaging. The process begins with an initial consultation with our imaging specialists regarding microscopy techniques for their specific project. Users then receive training in microscope operation, ensuring they can handle the equipment effectively. If needed, our specialists also provide support in image analysis. Next, we introduce users to OMERO, highlighting its features and the advantages of using a bioimage data management system. They are then trained to structure and annotate their data within OMERO according to the Recommended Metadata for Biological Images (REMBI), taking their specific research topics into account. As users prepare for data publication, we assist with data organization and repository uploads. Our goal is to educate researchers in managing bioimage data throughout its entire lifecycle, with a strong emphasis on the FAIR (findable, accessible, interoperable, reusable) principles.</p>
<p><a class="reference external" href="https://zenodo.org/records/14988921">https://zenodo.org/records/14988921</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14988921">https://doi.org/10.5281/zenodo.14988921</a></p>
</section>
<hr class="docutils" />
<section id="working-group-charter-rdm-helpdesk-network">
<h2>Working Group Charter. RDM Helpdesk Network<a class="headerlink" href="#working-group-charter-rdm-helpdesk-network" title="Link to this heading">#</a></h2>
<p>Judith Engel, Patrick Helling, Robert Herrenbrück, MarinaLemaire, Hela Mehrtens, Marcus Schmidt, Martha Stellmacher, Lukas Weimer, Cord Wiljes, Wolf Zinke</p>
<p>Published 2024-11-04</p>
<p>Licensed CC-BY-4.0</p>
<p>Support is an essential component of an efficient infrastructure for research data management (RDM). Helpdesks guide researchers through this complex landscape and provide reliable support about all questions regarding research data management, including support for technical services, best practices, requirements of funding organizations and legal topics. In NFDI, most consortia have already established or are planning to establish helpdesks to support their specific communities. On a local level, many institutions have set up RDM helpdesks that provide support for the researchers of their own institution. Additional RDM support services are offered by RDM federal state initiatives, by research data centers, by specialist libraries, by the EOSC, and by providers of RDM-relevant tools. Helpdesks cover a wide range of institutions, disciplines, topics, methodologies and target audiences. However, the individual helpdesks are not yet interconnected and therefore cannot complement one another in an efficient way: Given the wide and constantly increasing complexity of RDM, no single helpdesk can provide the expertise for all potential support requests. Therefore, we see great potential in combining the efforts and resources of the existing RDM helpdesks into an efficient and comprehensive national RDM support network in order to provide optimal and tailored RDM support to all researchers and research-related institutions in Germany and in an international context.</p>
<p><a class="reference external" href="https://zenodo.org/records/14035822">https://zenodo.org/records/14035822</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14035822">https://doi.org/10.5281/zenodo.14035822</a></p>
</section>
<hr class="docutils" />
<section id="zeiss-axiozoom-stage-adapter">
<h2>Zeiss AxioZoom Stage Adapter<a class="headerlink" href="#zeiss-axiozoom-stage-adapter" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-06-20</p>
<p>Licensed CC-BY-4.0</p>
<p>A 3D- printable microscope stage adapter for the reproducible accomodation of samples at a Zeiss AxioZoom stereomicroscope.
4 cylindrical anchors are fixed to the glass plate of the stage. The stage adapter is reversibly placed on these anchors.
 </p>
<p><a class="reference external" href="https://zenodo.org/records/7963020">https://zenodo.org/records/7963020</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7963020">https://doi.org/10.5281/zenodo.7963020</a></p>
</section>
<hr class="docutils" />
<section id="zeiss-axiozoom-stage-adapter-12-6well-plate">
<h2>Zeiss AxioZoom Stage Adapter - 12/6Well Plate<a class="headerlink" href="#zeiss-axiozoom-stage-adapter-12-6well-plate" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-06-20</p>
<p>Licensed CC-BY-4.0</p>
<p>A 3D- printable microscope stage adapter for the reproducible accomodation of 6 or 12-well plates at a Zeiss AxioZoom microscope.
4 cylindrical anchors are fixed to the glass plate of the stage. The stage adapter is reversibly placed on these anchors and acommodates a standard Greiner 6- or 12-well plate.</p>
<p><a class="reference external" href="https://zenodo.org/records/7944877">https://zenodo.org/records/7944877</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7944877">https://doi.org/10.5281/zenodo.7944877</a></p>
</section>
<hr class="docutils" />
<section id="zeiss-axiozoom-stage-adapter-em-block-holder">
<h2>Zeiss AxioZoom Stage Adapter - EM block holder<a class="headerlink" href="#zeiss-axiozoom-stage-adapter-em-block-holder" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-06-20</p>
<p>Licensed CC-BY-4.0</p>
<p>A 3D- printable microscope stage adapter for the reproducible accomodation of EM Blocks at a Zeiss AxioZoom microscope.</p>
<p>4 cylindrical anchors are fixed to the glass plate of the stage. The stage adapter is reversibly placed on these anchors and acommodates 70 standard resin EM blocks.</p>
<p><a class="reference external" href="https://zenodo.org/records/7963006">https://zenodo.org/records/7963006</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7963006">https://doi.org/10.5281/zenodo.7963006</a></p>
</section>
<hr class="docutils" />
<section id="zeiss-axiozoom-stage-adapter-microscope-slides">
<h2>Zeiss AxioZoom Stage Adapter - Microscope slides<a class="headerlink" href="#zeiss-axiozoom-stage-adapter-microscope-slides" title="Link to this heading">#</a></h2>
<p>Michael Gerlach</p>
<p>Published 2024-06-21</p>
<p>Licensed CC-BY-4.0</p>
<p>A 3D- printable microscope stage adapter for the reproducible accomodation of microscopic slides at a Zeiss AxioZoom microscope.
4 cylindrical anchors are fixed to the glass plate of the stage. The stage adapter is reversibly placed on these anchors and acommodates 4 standard glass slides.</p>
<p><a class="reference external" href="https://zenodo.org/records/7945018">https://zenodo.org/records/7945018</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7945018">https://doi.org/10.5281/zenodo.7945018</a></p>
</section>
<hr class="docutils" />
<section id="zerocostdl4mic-stardist-2d-example-training-and-test-dataset-light">
<h2>ZeroCostDL4Mic - Stardist 2D example training and test dataset (light)<a class="headerlink" href="#zerocostdl4mic-stardist-2d-example-training-and-test-dataset-light" title="Link to this heading">#</a></h2>
<p>Johanna Jukkala, Guillaume Jacquemet</p>
<p>Published 2023-05-19</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: ZeroCostDL4Mic - Stardist 2D example training and test dataset (light)</p>
<p>(see our Wiki for details)</p>
<p>Data type: Paired microscopy images (fluorescence) and corresponding masks</p>
<p>Microscopy data type: Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation (see <a class="reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist&amp;amp;nbsp;for">https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist&amp;nbsp;for</a> details about the segmentation)</p>
<p>Microscope: Spinning disk confocal microscope with a 20x 0.8 NA objective</p>
<p>Cell type: <a class="reference external" href="http://DCIS.COM">DCIS.COM</a> LifeAct-RFP cells</p>
<p>File format: .tif (16-bit for fluorescence and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 634 nm)</p>
<p> </p>
<p>Author(s): Johanna Jukkala1,2 and Guillaume Jacquemet1,2</p>
<p>Contact email: <a class="reference external" href="mailto:guillaume&#46;jacquemet&#37;&#52;&#48;abo&#46;fi">guillaume<span>&#46;</span>jacquemet<span>&#64;</span>abo<span>&#46;</span>fi</a></p>
<p>Affiliation : </p>
<ol class="arabic simple">
<li><p>Faculty of Science and Engineering, Cell Biology, Åbo Akademi University, 20520 Turku, Finland</p></li>
<li><p>Turku Bioscience Centre, University of Turku and Åbo Akademi University, FI-20520 Turku, Finland</p></li>
</ol>
<p>Funding bodies: G.J. was supported by grants awarded by the Academy of Finland, the Sigrid Juselius Foundation and Åbo Akademi University Research Foundation (CoE CellMech) and by Drug Discovery and Diagnostics strategic funding to Åbo Akademi University.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7949940">https://zenodo.org/records/7949940</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7949940">https://doi.org/10.5281/zenodo.7949940</a></p>
</section>
<hr class="docutils" />
<section id="zerocostdl4mic-stardist-example-training-and-test-dataset">
<h2>ZeroCostDL4Mic - Stardist example training and test dataset<a class="headerlink" href="#zerocostdl4mic-stardist-example-training-and-test-dataset" title="Link to this heading">#</a></h2>
<p>Johanna Jukkala, Guillaume Jacquemet</p>
<p>Published 2020-03-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: ZeroCostDL4Mic - Stardist example training and test dataset</p>
<p>(see our Wiki for details)</p>
<p> </p>
<p>Data type: Paired microscopy images (fluorescence) and corresponding masks</p>
<p>Microscopy data type: Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Microscope: Spinning disk confocal microscope with a 20x 0.8 NA objective</p>
<p>Cell type: <a class="reference external" href="http://DCIS.COM">DCIS.COM</a> LifeAct-RFP cells</p>
<p>File format: .tif (16-bit for fluorescence and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 634 nm)</p>
<p> </p>
<p>Author(s): Johanna Jukkala1,2 and Guillaume Jacquemet1,2</p>
<p>Contact email: <a class="reference external" href="mailto:guillaume&#46;jacquemet&#37;&#52;&#48;abo&#46;fi">guillaume<span>&#46;</span>jacquemet<span>&#64;</span>abo<span>&#46;</span>fi</a></p>
<p>Affiliation : </p>
<ol class="arabic simple">
<li><p>Faculty of Science and Engineering, Cell Biology, Åbo Akademi University, 20520 Turku, Finland</p></li>
<li><p>Turku Bioscience Centre, University of Turku and Åbo Akademi University, FI-20520 Turku, Finland</p></li>
</ol>
<p> </p>
<p>Associated publications: Unpublished</p>
<p>Funding bodies: G.J. was supported by grants awarded by the Academy of Finland, the Sigrid Juselius Foundation and Åbo Akademi University Research Foundation (CoE CellMech) and by Drug Discovery and Diagnostics strategic funding to Åbo Akademi University.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/3715492">https://zenodo.org/records/3715492</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.3715492">https://doi.org/10.5281/zenodo.3715492</a></p>
</section>
<hr class="docutils" />
<section id="bina-cc-scalable-strategies-for-a-next-generation-of-fair-bioimaging">
<h2>[BINA CC] Scalable strategies for a next-generation of FAIR bioimaging<a class="headerlink" href="#bina-cc-scalable-strategies-for-a-next-generation-of-fair-bioimaging" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Published 2024-09-24</p>
<p>Licensed CC-BY-4.0</p>
<p>Presented at <a class="reference external" href="https://www.bioimagingnorthamerica.org/events/bina-2024-community-congress/">https://www.bioimagingnorthamerica.org/events/bina-2024-community-congress/</a></p>
<p><a class="reference external" href="https://zenodo.org/records/13831274">https://zenodo.org/records/13831274</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13831274">https://doi.org/10.5281/zenodo.13831274</a></p>
</section>
<hr class="docutils" />
<section id="cidas-scalable-strategies-for-a-next-generation-of-fair-bioimaging">
<h2>[CIDAS] Scalable strategies for a next-generation of FAIR bioimaging<a class="headerlink" href="#cidas-scalable-strategies-for-a-next-generation-of-fair-bioimaging" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Published 2025-01-23</p>
<p>Licensed CC-BY-4.0</p>
<p>Talk given at Georg-August-Universität Göttingen Campus Institute Data Science23rd January 2025
<a class="reference external" href="https://www.uni-goettingen.de/en/653203.html">https://www.uni-goettingen.de/en/653203.html</a></p>
<p><a class="reference external" href="https://zenodo.org/records/14845059">https://zenodo.org/records/14845059</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14845059">https://doi.org/10.5281/zenodo.14845059</a></p>
</section>
<hr class="docutils" />
<section id="cmcb-scalable-strategies-for-a-next-generation-of-fair-bioimaging">
<h2>[CMCB] Scalable strategies for a next-generation of FAIR bioimaging<a class="headerlink" href="#cmcb-scalable-strategies-for-a-next-generation-of-fair-bioimaging" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Published 2025-01-16</p>
<p>Licensed CC-BY-4.0</p>
<p>CMCB LIFE SCIENCES SEMINARSTechnische Universität Dresden16th January 2025
<a class="reference external" href="https://tu-dresden.de/cmcb/crtd/news-termine/termine/cmcb-life-sciences-seminar-josh-moore-german-bioimaging-e-v-society-for-microscopy-and-image-analysis-constance">https://tu-dresden.de/cmcb/crtd/news-termine/termine/cmcb-life-sciences-seminar-josh-moore-german-bioimaging-e-v-society-for-microscopy-and-image-analysis-constance</a>
 </p>
<p>Tags: Nfdi4Bioimage</p>
<p><a class="reference external" href="https://zenodo.org/records/14650434">https://zenodo.org/records/14650434</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14650434">https://doi.org/10.5281/zenodo.14650434</a></p>
</section>
<hr class="docutils" />
<section id="cordi-2023-zarr-a-cloud-optimized-storage-for-interactive-access-of-large-arrays">
<h2>[CORDI 2023] Zarr: A Cloud-Optimized Storage for Interactive Access of Large Arrays<a class="headerlink" href="#cordi-2023-zarr-a-cloud-optimized-storage-for-interactive-access-of-large-arrays" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Licensed CC-BY-4.0</p>
<p>For decades, the sharing of large N-dimensional datasets has posed issues across multiple domains. Interactively accessing terabyte-scale data has previously required significant server resources to properly prepare cropped or down-sampled representations on the fly. Now, a cloud-native chunked format easing this burden has been adopted in the bioimaging domain for standardization. The format — Zarr — is potentially of interest for other consortia and sections of NFDI.</p>
<p>Tags: Research Data Management, Bioimage Analysis, Data Science</p>
<p>Content type: Poster</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.8340247">https://zenodo.org/doi/10.5281/zenodo.8340247</a></p>
</section>
<hr class="docutils" />
<section id="community-meeting-2024-overview-team-image-data-analysis-and-management">
<h2>[Community Meeting 2024] Overview Team Image Data Analysis and Management<a class="headerlink" href="#community-meeting-2024-overview-team-image-data-analysis-and-management" title="Link to this heading">#</a></h2>
<p>Susanne Kunis, Thomas Zobel</p>
<p>Published 2024-03-08</p>
<p>Licensed CC-BY-4.0</p>
<p>Overview of Activities of the Team Image Data Analysis and Management of German BioImaging e.V.
 </p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/10796364">https://zenodo.org/records/10796364</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10796364">https://doi.org/10.5281/zenodo.10796364</a></p>
</section>
<hr class="docutils" />
<section id="community-meeting-2024-supporting-and-financing-rdm-projects-within-gerbi">
<h2>[Community Meeting 2024] Supporting and financing RDM projects within GerBI<a class="headerlink" href="#community-meeting-2024-supporting-and-financing-rdm-projects-within-gerbi" title="Link to this heading">#</a></h2>
<p>Stefanie Weidtkamp-Peters, Josh Moore, Christian Schmidt, Roland Nitschke, Susanne Kunis, Thomas Zobel</p>
<p>Published 2024-03-28</p>
<p>Licensed CC-BY-4.0</p>
<p>Overview of GerBI RDM projects: why and how?</p>
<p><a class="reference external" href="https://zenodo.org/records/10889694">https://zenodo.org/records/10889694</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10889694">https://doi.org/10.5281/zenodo.10889694</a></p>
</section>
<hr class="docutils" />
<section id="elmi-2024-ai-s-dirty-little-secret-without">
<h2>[ELMI 2024]  AI’s Dirty Little Secret: Without<a class="headerlink" href="#elmi-2024-ai-s-dirty-little-secret-without" title="Link to this heading">#</a></h2>
<p>FAIR Data, It’s Just Fancy Math</p>
<p>Josh Moore, Susanne Kunis</p>
<p>Published 2024-05-21</p>
<p>Licensed CC-BY-4.0</p>
<p>Poster presented at the European Light Microscopy Initiative meeting in Liverpool (<a class="reference external" href="https://www.elmi2024.org/">https://www.elmi2024.org/</a>)</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/11235513">https://zenodo.org/records/11235513</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11235513">https://doi.org/10.5281/zenodo.11235513</a></p>
</section>
<hr class="docutils" />
<section id="elmi-2024-ai-s-dirty-little-secret-without-fair-data-it-s-just-fancy-math">
<h2>[ELMI 2024] AI’s Dirty Little Secret: Without FAIR Data, It’s Just Fancy Math<a class="headerlink" href="#elmi-2024-ai-s-dirty-little-secret-without-fair-data-it-s-just-fancy-math" title="Link to this heading">#</a></h2>
<p>Josh Moore, Susanne Kunis</p>
<p>Licensed CC-BY-4.0</p>
<p>Poster presented at the European Light Microscopy Initiative meeting in Liverpool (<a class="reference external" href="https://www.elmi2024.org/">https://www.elmi2024.org/</a>)</p>
<p>Tags: Research Data Management, FAIR-Principles, Bioimage Analysis, Nfdi4Bioimage</p>
<p>Content type: Poster</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.11235512">https://zenodo.org/doi/10.5281/zenodo.11235512</a></p>
</section>
<hr class="docutils" />
<section id="elmi2025-bridging-communities-with-ome-zarr">
<h2>[ELMI2025] Bridging communities with OME-Zarr<a class="headerlink" href="#elmi2025-bridging-communities-with-ome-zarr" title="Link to this heading">#</a></h2>
<p>Christian Schmidt, Aastha Mathur, Josh Moore</p>
<p>Published 2025-06-04</p>
<p>Licensed CC-BY-4.0</p>
<p>Presented at ELMI2025
 
<a class="reference external" href="https://www.embl.org/about/info/course-and-conference-office/events/elmi2025/">https://www.embl.org/about/info/course-and-conference-office/events/elmi2025/</a></p>
<p><a class="reference external" href="https://zenodo.org/records/15393592">https://zenodo.org/records/15393592</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15393592">https://doi.org/10.5281/zenodo.15393592</a></p>
</section>
<hr class="docutils" />
<section id="elmi2025-the-road-to-ome-zarr-1-0">
<h2>[ELMI2025] The Road to OME-Zarr 1.0<a class="headerlink" href="#elmi2025-the-road-to-ome-zarr-1-0" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Published 2025-06-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Presented at <a class="reference external" href="https://www.embl.org/about/info/course-and-conference-office/events/elmi2025/">https://www.embl.org/about/info/course-and-conference-office/events/elmi2025/</a>
 
Abstract
For over 20 years, the Open Microscopy Environment (OME) has developed tools and specifications to support bioimaging data sharing. Technologies such as Bio-Formats, OMERO, and OME-TIFF have helped researchers manage the growing size, complexity, and acquisition rates of imaging datasets. However, with increasing mandates for research data management, such as the Nelson memo in the United States, and the shift toward cloud-native workflows, the bioimaging community faces new challenges in ensuring scalable and FAIR data infrastructure.
In 2024, following expanding community engagement, the focus of the Next-Generation File Format (NGFF) community was on building consensus around a Request for Comments (RFC) process. This collaborative effort has laid the foundation for future refinements and wider adoption. In parallel, we hosted the “OME2024 NGFF Challenge,” bringing together over the course of just four months hundreds of terabytes of data in a first prototype of federated image hosting, showcasing the power of OME-Zarr for handling large-scale, distributed datasets.
In 2025, we are set to take a major step toward a stable FAIR solution with OME-Zarr 1.0. This milestone marks a crucial phase towards an international standard, providing an open, cloud-optimized, and scalable solution for handling terabyte- and petabyte-scale imaging data. The 1.0 release will introduce long-awaited transforms, enabling robust support for multimodal datasets, followed by collections and an extensibility mechanism to accommodate evolving scientific needs. These additions emphasize a solid foundation on which future capabilities can be built while providing the stability needed for broader adoption of the format. This presentation will outline the path to 1.0, including community-driven refinements, vendor engagement to ensure complete metadata representation, and alignment with global bioimaging initiatives. As imaging data continues to grow in scale and complexity, consensus-driven evolution of infrastructure will be key to ensuring a truly FAIR future for bioimaging.
 </p>
<p><a class="reference external" href="https://zenodo.org/records/15597856">https://zenodo.org/records/15597856</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15597856">https://doi.org/10.5281/zenodo.15597856</a></p>
</section>
<hr class="docutils" />
<section id="elmi2025-workshop-fair101-navigating-fair-data-from-principles-to-practice">
<h2>[ELMI2025] Workshop: FAIR101 - Navigating FAIR data from principles to practice<a class="headerlink" href="#elmi2025-workshop-fair101-navigating-fair-data-from-principles-to-practice" title="Link to this heading">#</a></h2>
<p>Isabel Kemmer, Euro-BioImaging ERIC</p>
<p>Published 2025-06-12</p>
<p>Licensed CC-BY-4.0</p>
<p> This workshop was held at the ELMI Meeting 2025 in Heidelberg (<a class="reference external" href="https://www.embl.org/about/info/course-and-conference-office/events/elmi2025/">https://www.embl.org/about/info/course-and-conference-office/events/elmi2025/</a>).
Abstract
FAIR 101 - Navigating FAIR data from principles to practice
Isabel Kemmer, Euro-BioImaging ERIC
This workshop will introduce the FAIR principles in the context of bioimaging data. Designed for researchers working across scales and technologies of biological and biomedical imaging, the session will address the unique challenges posed by complex, multidimensional bioimaging datasets. With the aim of providing simple yet impactful steps for a smooth start to the FAIR journey we will explore the features and benefits of FAIR data through interactive exercises and discussions - from metadata annotation and data management planning to repository selection. By the end of the workshop, you will feel more confident in applying the FAIR concepts and be prepared to improve your imaging workflows to make your precious data even more valuable.</p>
<p><a class="reference external" href="https://zenodo.org/records/15647102">https://zenodo.org/records/15647102</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15647102">https://doi.org/10.5281/zenodo.15647102</a></p>
</section>
<hr class="docutils" />
<section id="gbi-eoe-vii-five-or-ten-must-have-items-for-making-it-infrastructure-for-managing-bioimage-data">
<h2>[GBI EOE VII] Five (or ten) must-have items for making IT infrastructure for managing bioimage data<a class="headerlink" href="#gbi-eoe-vii-five-or-ten-must-have-items-for-making-it-infrastructure-for-managing-bioimage-data" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Published 2024-05-26</p>
<p>Licensed CC-BY-4.0</p>
<p>Presentation made to the GBI Image Data Management Working Group during the 7th Exchange of Experience in Uruguay.</p>
<p><a class="reference external" href="https://zenodo.org/records/11318151">https://zenodo.org/records/11318151</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11318151">https://doi.org/10.5281/zenodo.11318151</a></p>
</section>
<hr class="docutils" />
<section id="gbi-eoe-ix-nfdi4bioimage">
<h2>[GBI EoE IX] NFDI4BIOIMAGE<a class="headerlink" href="#gbi-eoe-ix-nfdi4bioimage" title="Link to this heading">#</a></h2>
<p>National Research Data Infrastructure
for Microscopy and BioImage Analysis</p>
<p>Josh Moore</p>
<p>Published 2024-10-29</p>
<p>Licensed CC-BY-4.0</p>
<p>Presented at <a class="reference external" href="https://globalbioimaging.org/exchange-of-experience/exchange-of-experience-ix">https://globalbioimaging.org/exchange-of-experience/exchange-of-experience-ix</a> in Okazaki, Japan.</p>
<p><a class="reference external" href="https://zenodo.org/records/14001388">https://zenodo.org/records/14001388</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14001388">https://doi.org/10.5281/zenodo.14001388</a></p>
</section>
<hr class="docutils" />
<section id="i2k-scalable-strategies-for-a-next-generation-of-fair-bioimaging">
<h2>[I2K] Scalable strategies for a next-generation of FAIR bioimaging<a class="headerlink" href="#i2k-scalable-strategies-for-a-next-generation-of-fair-bioimaging" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Published 2024-10-25</p>
<p>Licensed CC-BY-4.0</p>
<p>or, “OME-Zarr: ‘even a talk on formats [can be] interesting’”
Presented at <a class="reference external" href="https://events.humantechnopole.it/event/1/">https://events.humantechnopole.it/event/1/</a></p>
<p><a class="reference external" href="https://zenodo.org/records/13991322">https://zenodo.org/records/13991322</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13991322">https://doi.org/10.5281/zenodo.13991322</a></p>
</section>
<hr class="docutils" />
<section id="n4bi-ahm-welcome-to-bioimage-town">
<h2>[N4BI AHM] Welcome to BioImage Town<a class="headerlink" href="#n4bi-ahm-welcome-to-bioimage-town" title="Link to this heading">#</a></h2>
<p>Josh Moore</p>
<p>Published 2023-10-16</p>
<p>Licensed CC-BY-4.0</p>
<p>Keynote at the NFDI4BIOIMAGE All-Hands Meeting in Düsseldorf, Germany, October 16, 2023.</p>
<p><a class="reference external" href="https://zenodo.org/records/15031842">https://zenodo.org/records/15031842</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15031842">https://doi.org/10.5281/zenodo.15031842</a></p>
</section>
<hr class="docutils" />
<section id="nfdi-tech-talk-cloud-based-image-science">
<h2>[NFDI Tech Talk] Cloud Based Image Science<a class="headerlink" href="#nfdi-tech-talk-cloud-based-image-science" title="Link to this heading">#</a></h2>
<p>Josh Moore, Yi Sun</p>
<p>Published 2025-06-02</p>
<p>Licensed CC-BY-4.0</p>
<p>Slides for the NFDI Tech Talk live streamed to <a class="reference external" href="https://www.youtube.com/live/bzfmE29S270">https://www.youtube.com/live/bzfmE29S270</a>
See <a class="reference external" href="http://nfdi.de/talks">http://nfdi.de/talks</a> for more information.</p>
<p><a class="reference external" href="https://zenodo.org/records/15575379">https://zenodo.org/records/15575379</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15575379">https://doi.org/10.5281/zenodo.15575379</a></p>
</section>
<hr class="docutils" />
<section id="swat4hcls-2023-nfdi4bioimage-perspective-for-a-national-bioimage-standard">
<h2>[SWAT4HCLS 2023] NFDI4BIOIMAGE: Perspective for a national bioimage standard<a class="headerlink" href="#swat4hcls-2023-nfdi4bioimage-perspective-for-a-national-bioimage-standard" title="Link to this heading">#</a></h2>
<p>Josh Moore, Susanne Kunis</p>
<p>Licensed CC-BY-4.0</p>
<p>Poster presented at Semantic Web Applications and Tools for Health Care and Life Sciences (SWAT4HCLS 2023), Feb 13–16, 2023, Basel, Switzerland. NFDI4BIOIMAGE is a newly established German consortium dedicated to the FAIR representation of biological imaging data. A key deliverable is the definition of a semantically-compatible FAIR image object integrating RDF metadata with web-compatible storage of large n-dimensional binary data in OME-Zarr. We invite feedback from and collaboration with other endeavors during the soon-to-begin 5 year funding period.</p>
<p>Tags: Research Data Management, FAIR-Principles, Nfdi4Bioimage</p>
<p>Content type: Poster</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.7928332">https://zenodo.org/doi/10.5281/zenodo.7928332</a></p>
</section>
<hr class="docutils" />
<section id="short-talk-nfdi4bioimage-a-consortium-in-the-national-research-data-infrastructure">
<h2>[Short Talk] NFDI4BIOIMAGE - A consortium in the National Research Data Infrastructure<a class="headerlink" href="#short-talk-nfdi4bioimage-a-consortium-in-the-national-research-data-infrastructure" title="Link to this heading">#</a></h2>
<p>Christian Schmidt</p>
<p>Published 2024-04-10</p>
<p>Licensed CC-BY-4.0</p>
<p>Short Talk about the NFDI4BIOIMAGE consortium presented at the RDM in (Bio-)Medicine Information Event on April 10th, 2024, organized C³RDM &amp; ZB MED.</p>
<p><a class="reference external" href="https://zenodo.org/records/10939520">https://zenodo.org/records/10939520</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10939520">https://doi.org/10.5281/zenodo.10939520</a></p>
</section>
<hr class="docutils" />
<section id="webinar-a-journey-to-fair-bioimage-data">
<h2>[Webinar] A journey to FAIR bioimage data<a class="headerlink" href="#webinar-a-journey-to-fair-bioimage-data" title="Link to this heading">#</a></h2>
<p>Stefanie Weidtkamp-Peters, Tom Boissonnet, Christian Schmidt</p>
<p>Published 2025-07-03</p>
<p>Licensed CC-BY-4.0</p>
<p>Presentation slides from an EMBL-EBI Webinar Talk within the webinar series:
“How to organise and share my imaging data? - Multimodal data management for marine biologists, environmental scientists and imaging specialists”
 
Abstract / Description
Bioimaging is a pervasive and indispensable methodological approach in the life and biomedical sciences. Due to the development of new technologies and the easier access to compute resources, bioimaging experiments have become a big data discipline, facing the same challenges as other omics technologies within the life sciences. However, to fully exploit the potential of bioimage data, it is necessary to make the data FAIR. In this webinar we will present viable solutions for storing, processing, analysing, and, first and foremost, sharing bioimaging data. We will introduce services provided to the scientific community, that are dealing with various aspects of the bioimage data life cycle such as:</p>
<ul class="simple">
<li><p>Where to get support for bioimage data management- Local bioimage data management: OMERO and beyond- Annotation of bioimage data: metadata, ontologies, REMBI etc- Linking your image data with experimental protocols and analysis results- Large data living in the cloud: ome.zarr- Publication of bioimage data
Who is this course for?
This webinar is suitable for marine biologists and environmental scientists collecting samples from the natural environment, generating, visualising, annotating and analysing large, multimodal datasets such as imaging data, and sharing their data by submitting them to public data repositories. The webinar will support you to set up an efficient data flow that is aligned with FAIR principles.
This event is part of a webinar series organised by the STANDFLOW project, an initiative supported by EMBL’s Planetary biology Transversal Theme. STANDFLOW is about a collaborative effort towards creating a standardised data management workflow. The project primarily utilises imaging data derived from samples collected through the TREC (Traversing European Coastlines) and the Roscoff Culture Collection. For details on all topics covered in this series and registration information, please visit the following link: How to organise and share my imaging data?: Multimodal data management for marine biologists, and environmental scientists and imaging specialists
Outcomes
By the end of the webinar you will be able to: </p></li>
</ul>
<p>Find resources and support for bioimage data management
Get started with bioimage data annotation
Identify the dos and don’ts for bioimage data publication</p>
<p> 
(taken from: <a class="reference external" href="https://www.ebi.ac.uk/training/events/journey-fair-bioimage-data/">https://www.ebi.ac.uk/training/events/journey-fair-bioimage-data/</a>)</p>
<p><a class="reference external" href="https://zenodo.org/records/15796252">https://zenodo.org/records/15796252</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15796252">https://doi.org/10.5281/zenodo.15796252</a></p>
</section>
<hr class="docutils" />
<section id="workshop-material-fit-for-omero-how-imaging-facilities-and-it-departments-work-together-to-enable-rdm-for-bioimaging-october-16-17-2024-heidelberg">
<h2>[Workshop Material] Fit for OMERO - How imaging facilities and IT departments work together to enable RDM for bioimaging, October 16-17, 2024, Heidelberg<a class="headerlink" href="#workshop-material-fit-for-omero-how-imaging-facilities-and-it-departments-work-together-to-enable-rdm-for-bioimaging-october-16-17-2024-heidelberg" title="Link to this heading">#</a></h2>
<p>Tom Boissonnet, Bettina Hagen, Susanne Kunis, Christian Schmidt, Stefanie Weidtkamp-Peters</p>
<p>Published 2024-11-18</p>
<p>Licensed CC-BY-4.0</p>
<p>Fit for OMERO: How imaging facilities and IT departments work together to enable RDM for bioimaging
Description:
Research data management (RDM) in bioimaging is challenging because of large file sizes, heterogeneous file formats and the variability of imaging methods. The image data management system OMERO (OME Remote Objects) allows for centralized and secure storage, organization, annotation, and interrogation of microscopy data by researchers. It is an internationally well-supported open-source software tool that has become one of the best-known image data management tools among bioimaging scientists. Nevertheless, the de novo setup of OMERO at an institute is a multi-stakeholder process that demands time, funds, organization and iterative implementation. In this workshop, participants learn how to begin setting up OMERO-based image data management at their institution. The topics include:</p>
<p>Stakeholder identification at the university / research institute
Process management, time line expectations, and resources planning
Learning about each other‘s perspectives on chances and challenges for RDM
Funding opportunities and strategies for IT and imaging core facilities
Hands-on: Setting up an OMERO server in a virtual machine environment</p>
<p>Target audience:
This workshop was directed at universities and research institutions who consider or plan to implement OMERO, or are in an early phase of implementation. This workshop was intended for teams from IT departments and imaging facilities to participate together with one person from the IT department, and one person from the imaging core facility at the same institution.
The trainers:</p>
<p>Prof. Dr. Stefanie Weidtkamp-Peters (Imaging Core Facility Head, Center for Advanced Imaging, Heinrich Heine University of Düsseldorf)
Dr. Susanne Kunis (Software architect, OMERO administrator, metadata specialist, University of Osnabrück)
Dr. Tom Boissonnet (OMERO admin and image metadata specialist, Center for Advanced Imaging, Heinrich Heine University of Düsseldorf)
Dr. Bettina Hagen (IT Administration and service specialist, Max Planck Institute for the Biology of Ageing, Cologne) 
Dr. Christian Schmidt (Science Manager for Research Data Management in Bioimaging, German Cancer Research Center (DKFZ), Heidelberg)</p>
<p>Time and place
The format was a two-day, in-person workshop (October 16-17, 2024). Location: Heidelberg, Germany
Workshop learning goals</p>
<p>Learn the steps to establish a local RDM environment fit for bioimaging data
Create a network of IT experts and bioimaging specialists for bioimage RDM across institutions
Establish a stakeholder process management for installing OMERO-based RDM
Learn from each other, leverage different expertise
Learn how to train users, establish sustainability strategies, and foster FAIR RDM for bioimaging at your institution</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/14178789">https://zenodo.org/records/14178789</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14178789">https://doi.org/10.5281/zenodo.14178789</a></p>
</section>
<hr class="docutils" />
<section id="workshop-bioimage-data-management-and-analysis-with-omero">
<h2>[Workshop] Bioimage data management and analysis with OMERO<a class="headerlink" href="#workshop-bioimage-data-management-and-analysis-with-omero" title="Link to this heading">#</a></h2>
<p>Riccardo Massei, Michele Bortolomeazzi, Christian Schmidt</p>
<p>Published 2024-05-13</p>
<p>Licensed CC-BY-4.0</p>
<p>Here we share the material used in a workshop held on May 13th, 2024, at the German Cancer Research Center in Heidelberg (on-premise)
Description:Microscopy experiments generate information-rich, multi-dimensional data, allowing us to investigate biological processes at high spatial and temporal resolution. Image processing and analysis is a standard procedure to retrieve quantitative information from biological imaging. Due to the complex nature of bioimaging files that often come in proprietary formats, it can be challenging to organize, structure, and annotate bioimaging data throughout a project. Data often needs to be moved between collaboration partners, transformed into open formats, processed with a variety of software tools, and exported to smaller-sized images for presentation. The path from image acquisition to final publication figures with quantitative results must be documented and reproducible.
In this workshop, participants learn how to use OMERO to organize their data and enrich the bioimage data with structured metadata annotations.We also focus on image analysis workflows in combination with OMERO based on the Fiji/ImageJ software and using Jupyter Notebooks. In the last part, we explore how OMERO can be used to create publication figures and prepare bioimage data for publication in a suitable repository such as the Bioimage Archive.
Module 1 (9 am - 10.15 am): Basics of OMERO, data structuring and annotation
Module 2 (10.45 am - 12.45 pm): OMERO and Fiji
Module 3 (1.45 pm - 3.45 pm): OMERO and Jupyter Notebooks
Module 4 (4.15 pm - 6. pm): Publication-ready figures and data with OMERO
The target group for this workshopThis workshop is directed at researchers at all career levels who plan to or have started to use OMERO for their microscopy research data management. We encourage the workshop participants to bring example data from their research to discuss suitable metadata annotation for their everyday practice.
Prerequisites:Users should bring their laptops and have access to the internet through one of the following options:- eduroam- institutional WiFi- VPN connection to their institutional networks to access OMERO
Who are the trainers?
Dr. Riccardo Massei (Helmholtz-Center for Environmental Research, UFZ, Leipzig) - Data Steward for Bioimaging Data in NFDI4BIOIMAGE
Dr. Michele Bortolomeazzi (DKFZ, Single cell Open Lab, bioimage data specialist, bioinformatician, staff scientist in the NFDI4BIOIMAGE project)
Dr. Christian Schmidt (Science Manager for Research Data Management in Bioimaging, German Cancer Research Center, Heidelberg, Project Coordinator of the NFDI4BIOIMAGE project)</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/11350689">https://zenodo.org/records/11350689</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11350689">https://doi.org/10.5281/zenodo.11350689</a></p>
</section>
<hr class="docutils" />
<section id="workshop-fair-data-handling-for-microscopy-structured-metadata-annotation-in-omero">
<h2>[Workshop] FAIR data handling for microscopy: Structured metadata annotation in OMERO<a class="headerlink" href="#workshop-fair-data-handling-for-microscopy-structured-metadata-annotation-in-omero" title="Link to this heading">#</a></h2>
<p>Vanessa Fiona Aphaia Fuchs, Christian Schmidt, Tom Boissonnet</p>
<p>Published 2024-05-06</p>
<p>Licensed CC-BY-4.0</p>
<p>Description
Microscopy experiments generate information-rich, multi-dimensional data, allowing us to investigate biological processes at high spatial and temporal resolution. Image processing and analysis is a standard procedure to retrieve quantitative information from biological imaging. Due to the complex nature of bioimaging files that often come in proprietary formats, it can be challenging to organize, structure, and annotate bioimaging data throughout a project. Data often needs to be moved between collaboration partners, transformed into open formats, processed with a variety of software tools, and exported to smaller-sized images for presentation. The path from image acquisition to final publication figures with quantitative results must be documented and reproducible.
In this workshop, participants learn how to use structured metadata annotations in the image data management platform OMERO (OME Remote Objects) to optimize their data handling. This strategy helps both with organizing data for easier processing and analysis and for the preparation of data publication in journal manuscripts and in public repositories such as the BioImage Archive. Participants learn the principles of leveraging object-oriented data organization in OMERO to enhance findability and usability of their data, also in collaborative settings. The integration of OMERO with image analysis tools, in particular ImageJ/Fiji, will be trained. Moreover, users learn about community-accepted metadata checklists (REMBI) to enrich the value of their data toward reproducibility and reusability. In this workshop, we will provide hands-on training and recommendations on:</p>
<p>Structured metadata annotation features in OMERO and how to use them
Types of metadata in bioimaging: Technical metadata, sample metadata, analysis metadata
The use of ontologies and terminologies for metadata annotation
REMBI, the recommended metadata for biological images
Metadata-assisted image analysis streamlining
Tools for metadata annotation in OMERO</p>
<p>The target group for this workshop
This workshop is directed at researchers at all career levels who have started using OMERO for their microscopy research data management. We encourage the workshop participants to bring example data from their research to discuss suitable metadata annotation for their everyday practice.
Who are the trainers (see trainer description below for more details)</p>
<p>Dr. Vanessa Fuchs (NFDI4BIOIMAGE Data Steward, Center for Advanced Imaging, Heinrich-Heine University of Düsseldorf)
Dr. Tom Boissonnet (OMERO admin and image metadata specialist, Center for Advanced Imaging, Heinrich-Heine University of Düsseldorf)
Dr. Christian Schmidt (Science Manager for Research Data Management in Bioimaging, German Cancer Research Center, Heidelberg)</p>
<p>Material Description
Published here are the presentation slides that were used for input from the trainers during the different sessions of the programme. Additionally, a Fiji Macro is published that depends on the OMERO Extensions Plugin by Pouchin et al, 2022, F100Research, <a class="reference external" href="https://doi.org/10.12688/f1000research.110385.2">https://doi.org/10.12688/f1000research.110385.2</a> 
Programme Overview
Day 1 - April 29th, 2024 09.00 a.m. to 10.00 a.m.: Session 1 - Welcome and Introduction
10.00 a.m. to 10.30 a.m.:  Session 2 - Introduction to the FAIR principles &amp; data annotation
10:30 a.m. to 10:45 a.m.: Coffee break
10.45 a.m. to 12.00 a.m.: Session 3 - Data structure (datasets in OMERO) and organization with Tags 
12.00 a.m. to 1.00 p.m.:  Lunch Break
1.00 p.m. to 2.00 p.m.:  Session 4 - REMBI, Key-Value pair annotations in bioimaging
2:00 p.m. to 2.30 p.m.:  Session 5 - Ontologies for Key-Value Pairs in OMERO
2:30 p.m. to 2:45 p.m. Coffee break
2.45 p.m. to 3.45 p.m.:  Wrap-up, discussion, outlook on day 2
Day 2 - April 30th, 2024
09.00 a.m. to 09.30 a.m.:  Arrival and Start into day 2
09.30 a.m. to 11.30 a.m.:  Session 6 - Hands-on : REMBI-based Key-Value Pair annotation in OMERO
11.30 a.m. to 12.30 a.m.:  Lunch Break
12.30 a.m. to 1.15 p.m.: Session 7 - OMERO and OMERO.plugins
1.15 p.m. to 2.00 p.m.: Session 8 - Loading OMERO-hosted data into Fiji
2.00 p.m. to 2.15 p.m.: Coffee break 
2.15 p.m. to 3.00 p.m.: Discussion, Outlook</p>
<p><a class="reference external" href="https://zenodo.org/records/11109616">https://zenodo.org/records/11109616</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11109616">https://doi.org/10.5281/zenodo.11109616</a></p>
</section>
<hr class="docutils" />
<section id="workshop-managing-fair-microscopy-data-at-scale-for-universities-and-research-institutions-an-introduction-for-non-imaging-stakeholders">
<h2>[Workshop] Managing FAIR microscopy data at scale for universities and research institutions: an introduction for non-imaging stakeholders<a class="headerlink" href="#workshop-managing-fair-microscopy-data-at-scale-for-universities-and-research-institutions-an-introduction-for-non-imaging-stakeholders" title="Link to this heading">#</a></h2>
<p>Christian Schmidt, Michele Bortolomeazzi, Ksenia Krooß, Jan-Philipp Mallm, Elisa Ferrando-May, Stefanie Weidtkamp-Peters</p>
<p>Published 2025-03-14</p>
<p>Licensed CC-BY-4.0</p>
<p>These slides were used in a workshop at the 2025 E-Science Tage in Heidelberg.
Workshop Abstract:
Effective Research Data Management (RDM) requires collaboration between infrastructure providers, support units, and domain-specific experts across scientific disciplines. Microscopy, or bioimaging, is a widely used technology at universities and research institutions, generating large, multi-dimensional datasets. Scientists now routinely produce microscopy data using advanced imaging modalities, often through centrally-provided instruments maintained by core facilities.
Bioimaging data management presents unique challenges: files are often large (e.g., 15+ GB for whole slide images), come in various proprietary formats, and are accessed frequently for viewing as well as for complex image processing and analysis workflows. Collaboration between experimenters, clinicians, group leaders, core facility staff, and image analysts adds to the complexity, increasing the risk of data fragmentation and metadata loss.
The DFG-funded project I3D:bio and the consortium NFDI4BIOIMAGE, part of Germany’s National Research Data Infrastructure (NFDI), are addressing these challenges by developing solutions and best practices for managing large, complex microscopy datasets. This workshop introduces the challenges of bioimaging RDM to institutional support personnel, including, for example, library staff, IT departments, and data stewards. Participants will explore the bioimaging RDM system OMERO, and apply structured metadata annotation and object-oriented data organization to a simple training dataset. OMERO offers centralized, secure access to data, allowing collaboration and reducing the data fragementation risk. Moreover, participants will experience the benefits of OME-Zarr, a chunked open file format designed for FAIR data sharing and remote access. OME-Zarr enables streaming of large, N-dimensional array-typed data over the Internet without the need to download whole files. An expanding toolbox for leveraging OME-Zarr for bioimaging data renders this file type a promising candidate for a standard file format suitable for use in FAIR Digital Object (FDO) implementations for microscopy data. OME-Zarr has become a pillar for imaging data sharing in two bioimaging-specific data repositories, i.e., the Image Data Resource (IDR) and the BioImage Archive (BIA). The team of Data Stewards from both abovenmentioned projects help researchers and research support staff to manage und publish bioimaging data.
By the end of the workshop, participants will have gained hands-on experience with bioimaging data and will be aware of support resources like the NFDI4BIOIMAGE Help Desk for addressing specific local use cases. Our goal is to promote collaboration across disciplines to effectively manage complex bioimaging data in compliance with the FAIR principles.
 </p>
<p><a class="reference external" href="https://zenodo.org/records/15026373">https://zenodo.org/records/15026373</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15026373">https://doi.org/10.5281/zenodo.15026373</a></p>
</section>
<hr class="docutils" />
<section id="workshop-research-data-management-for-microscopy-and-bioimage-analysis">
<h2>[Workshop] Research Data Management for Microscopy and BioImage Analysis<a class="headerlink" href="#workshop-research-data-management-for-microscopy-and-bioimage-analysis" title="Link to this heading">#</a></h2>
<p>Christian Schmidt, Tom Boissonnet, Michele Bortolomeazzi, Ksenia Krooß</p>
<p>Published 2024-09-30</p>
<p>Licensed CC-BY-4.0</p>
<p>Research Data Management for Microscopy and BioImage Analysis</p>
<p>Introduction to BioImaging Research Data Management, NFDI4BIOIMAGE and I3D:bioChristian Schmidt /DKFZ Heidelberg
OMERO as a tool for bioimaging data managementTom Boissonnet /Heinrich-Heine Universität Düsseldorf
Reproducible image analysis workflows with OMERO software APIsMichele Bortolomeazzi /DKFZ Heidelberg
Publishing datasets in public archives for bioimage dataKsenia Krooß /Heinrich-Heine Universität Düsseldorf</p>
<p>Date &amp; Venue:Thursday, Sept. 26, 5.30 p.m.Haus 22 / Paul Ehrlich Lecture Hall (H22-1)University Hospital Frankfurt</p>
<p>Tags: Nfdi4Bioimage, Research Data Management</p>
<p><a class="reference external" href="https://zenodo.org/records/13861026">https://zenodo.org/records/13861026</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13861026">https://doi.org/10.5281/zenodo.13861026</a></p>
</section>
<hr class="docutils" />
<section id="ilastik-interactive-machine-learning-for-bio-image-analysis">
<h2>ilastik: interactive machine learning for (bio)image analysis<a class="headerlink" href="#ilastik-interactive-machine-learning-for-bio-image-analysis" title="Link to this heading">#</a></h2>
<p>Anna Kreshuk, Dominik Kutra</p>
<p>Licensed CC-BY-4.0</p>
<p>Tags: Artificial Intelligence, Bioimage Analysis</p>
<p>Content type: Slides</p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.4330625">https://zenodo.org/doi/10.5281/zenodo.4330625</a></p>
</section>
<hr class="docutils" />
<section id="imaris-file-not-read-by-bfgetreader">
<h2>imaris file not read by bfGetReader()<a class="headerlink" href="#imaris-file-not-read-by-bfgetreader" title="Link to this heading">#</a></h2>
<p>Julien Dubrulle</p>
<p>Published 2025-03-10</p>
<p>Licensed CC-BY-4.0</p>
<p>This file cannot be read by bfGetReader() v8.1.1 on a Windows operating workstation.</p>
<p><a class="reference external" href="https://zenodo.org/records/15001649">https://zenodo.org/records/15001649</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15001649">https://doi.org/10.5281/zenodo.15001649</a></p>
</section>
<hr class="docutils" />
<section id="martinschatz-cz-scicount-v1-0-0-with-reusable-example-notebooks">
<h2>martinschatz-cz/SciCount: v1.0.0 with reusable example notebooks<a class="headerlink" href="#martinschatz-cz-scicount-v1-0-0-with-reusable-example-notebooks" title="Link to this heading">#</a></h2>
<p>Martin Schätz, Lukáš Mrazík, Karolina Máhlerova</p>
<p>Published 2022-08-02</p>
<p>Licensed OTHER-OPEN</p>
<p>The first version contains an example of augmentation of scientific data and object detection with YOLO_v5 on colony counting (2 classes), object counting in blood smears (can be used as semisupervised learning for faster annotation), and wildlife detection from night records with a camera trap.</p>
<p>The project is available on GitHub.</p>
<p><a class="reference external" href="https://zenodo.org/records/6953610">https://zenodo.org/records/6953610</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6953610">https://doi.org/10.5281/zenodo.6953610</a></p>
</section>
<hr class="docutils" />
<section id="quantixed-thedigitalcell-first-complete-code-set">
<h2>quantixed/TheDigitalCell: First complete code set<a class="headerlink" href="#quantixed-thedigitalcell-first-complete-code-set" title="Link to this heading">#</a></h2>
<p>Stephen Royle</p>
<p>Published 2019-04-17</p>
<p>Licensed GPL-3.0</p>
<p>First complete code set for The Digital Cell book.</p>
<p>Tags: Bioimage Analysis</p>
<p>Content type: Code</p>
<p><a class="github reference external" href="https://github.com/quantixed/TheDigitalCell">quantixed/TheDigitalCell</a></p>
<p><a class="reference external" href="https://zenodo.org/records/2643411">https://zenodo.org/records/2643411</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.2643411">https://doi.org/10.5281/zenodo.2643411</a></p>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./domain"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="www.youtube.com.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Www.youtube.com (28)</p>
      </div>
    </a>
    <a class="right-next"
       href="../statistics/readme.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Statistics</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zenodo-und-co-was-bringt-und-wer-braucht-ein-repositorium">“ZENODO und Co.” Was bringt und wer braucht ein Repositorium?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lif-files-misbehaving-in-fiji-but-fine-in-lasx">.lif files misbehaving in fiji but fine in LASX</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frames-of-fluorescent-particles">10 frames of fluorescent particles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-nuclei-annotations-and-stardist-3d-model-s-rat-brain">3D Nuclei annotations and StarDist 3D model(s) (rat brain)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-nuclei-instance-segmentation-dataset-of-fluorescence-microscopy-volumes-of-c-elegans">3D nuclei instance segmentation dataset of fluorescence microscopy volumes of C. elegans</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-towards-reproducible-research">6 Steps Towards Reproducible Research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-glimpse-of-the-open-source-flim-analysis-software-tools-flimfit-flute-and-napari-flim-phasor-plotter">A Glimpse of the Open-Source FLIM Analysis Software Tools FLIMfit, FLUTE and napari-flim-phasor-plotter</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deep-learning-approach-to-quantify-auditory-hair-cells">A deep learning approach to quantify auditory hair cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-journey-to-fair-microscopy-data">A journey to FAIR microscopy data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abdominal-imaging-window-aiw-for-intravital-imaging">Abdominal Imaging Window (AIW) for Intravital Imaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aberrated-bead-stack">Aberrated Bead Stack</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accessible-interactive-spatial-omics-data-visualizations-with-vitessce-and-omero">Accessible Interactive Spatial-Omics Data Visualizations with Vitessce and OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advancing-fair-image-analysis-in-galaxy-tools-workflows-and-training">Advancing FAIR Image Analysis in Galaxy: Tools, Workflows, and Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alles-meins-oder-urheberrechte-klaren-fur-forschungsdaten">Alles meins – oder!? Urheberrechte klären für Forschungsdaten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-annotated-high-content-fluorescence-microscopy-dataset-with-hoechst-33342-stained-nuclei-and-manually-labelled-outlines">An annotated high-content fluorescence microscopy dataset with Hoechst 33342-stained nuclei and manually labelled outlines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#andor-dragonfly-confocal-image-of-bpae-cells-stained-for-actin-ims-file-format">Andor Dragonfly confocal image of BPAE cells stained for actin, IMS file format</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#angebote-der-nfdi-fur-die-forschung-im-bereich-zoologie">Angebote der NFDI für die Forschung im Bereich Zoologie</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-blobs-and-labels-image">Artificial Blobs and Labels image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#astigmatic-4pi-bead-stack">Astigmatic 4Pi bead stack</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-labelling-of-hela-kyoto-cells-using-deep-learning-tools">Automatic labelling of HeLa “Kyoto” cells using Deep Learning tools</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#axioscan-7-fluorescent-channels-not-displaying-in-qupath">Axioscan 7 fluorescent channels not displaying in QuPath</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beads-imaged-over-time">Beads imaged over time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-data-strudel-for-workshop-on-research-data-management-in-tu-dresden-core-facilities">Bio-Image Data Strudel for Workshop on Research Data Management in TU Dresden Core Facilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-analysis-code-generation">Bio-image Analysis Code Generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-analysis-code-generation-using-bia-bob">Bio-image Analysis Code Generation using bia-bob</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-analysis-with-the-help-of-large-language-models">Bio-image Analysis with the Help of Large Language Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-data-science-lectures-2025-uni-leipzig-scads-ai">Bio-image Data Science Lectures 2025 @ Uni Leipzig / ScaDS.AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bio-image-data-science-lectures-uni-leipzig-scads-ai">Bio-image Data Science Lectures @ Uni Leipzig / ScaDS.AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bioimage-io-chatbot-globias-seminar">BioImage.IO Chatbot, GloBIAS Seminar</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#breast-cancer-nuclei-images-for-dl-training-zerocostdl4mic-stardist-model">Breast Cancer Nuclei images for DL Training + ZeroCostDL4Mic StarDist Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-fair-image-analysis-pipelines-for-high-content-screening-hcs-data-using-galaxy">Building FAIR image analysis pipelines for high-content-screening (HCS) data using Galaxy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-fair-image-data-ecosystem-for-microscopy-communities">Building a FAIR image data ecosystem for microscopy communities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-national-research-data-infrastructure">Building a National Research Data Infrastructure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#czi-carl-zeiss-image-dataset-with-artificial-test-camera-images-with-various-dimension-for-testing-libraries-reading">CZI (Carl Zeiss Image) dataset with artificial test camera images with various dimension for testing libraries reading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#czi-file-examples">CZI file examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#czi-open-science-program-collection">CZI: Open Science Program Collection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellbindb-a-large-scale-multimodal-annotated-dataset">CellBinDB: A Large-Scale Multimodal Annotated Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-model-for-digital-phase-contrast-images">Cellpose model for Digital Phase Contrast images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-models-for-label-prediction-from-brightfield-and-digital-phase-contrast-images">Cellpose models for Label Prediction from Brightfield and Digital Phase Contrast images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments">Cellpose training data and scripts from “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers">Cellpose training data and scripts from “Machine learning for histological annotation and quantification of cortical layers”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chatgpt-for-image-analysis">ChatGPT for Image Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collaborative-working-and-version-control-with-git-hub">Collaborative Working and Version Control with git[hub]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Collaborative working and  Version Control with Git[Hub]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-1-breast-cancer-cell-dataset">Combining StarDist and TrackMate example 1 -  Breast cancer cell dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-2-t-cell-dataset">Combining StarDist and TrackMate example 2 -  T cell dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-3-flow-chamber-dataset">Combining StarDist and TrackMate example 3 -  Flow chamber dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-the-bids-and-arc-directory-structures-for-multimodal-research-data-organization">Combining the BIDS and ARC Directory Structures for Multimodal Research Data Organization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conference-slides-4th-day-of-intravital-microscopy">Conference Slides - 4th Day of Intravital Microscopy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#crashkurs-forschungsdatenmanagement">Crashkurs Forschungsdatenmanagement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-open-computational-curricula">Creating open computational curricula</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cultivating-open-training">Cultivating Open Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cultivating-open-training-to-advance-bio-image-analysis">Cultivating Open Training to advance Bio-image Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dalia-interchange-format">DALIA Interchange Format</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-stewardship-and-research-data-management-tools-for-multimodal-linking-of-imaging-data-in-plasma-medicine">Data stewardship and research data management tools for multimodal linking of imaging data in plasma medicine</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataviz-protocols-an-introduction-to-data-visualization-protocols-for-wet-lab-scientists">DataViz protocols - An introduction to data visualization protocols for wet lab scientists</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-from-incell-2200-microscope-misread-as-a-plate">Dataset from InCell 2200 microscope misread as a plate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datenmanagement">Datenmanagement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datenmanagement-im-fokus-organisation-speicherstrategien-und-datenschutz">Datenmanagement im Fokus: Organisation, Speicherstrategien und Datenschutz</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datenmanagementplane-erstellen-teil-1">Datenmanagementpläne erstellen - Teil 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datenmanagementplane-erstellen-teil-2">Datenmanagementpläne erstellen - Teil 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deconvolution-test-dataset">Deconvolution Test Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-segmentation-projects-of-fib-sem-dataset-of-u2-os-cell">Deep learning segmentation projects of FIB-SEM dataset of U2-OS cell</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-training-data-jove">Deep learning training data (JOVE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-bacillus-subtilis-fluorescence-segmentation-dataset">DeepBacs – Bacillus subtilis fluorescence segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-escherichia-coli-bright-field-segmentation-dataset">DeepBacs – Escherichia coli bright field segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-mixed-segmentation-dataset-and-stardist-model">DeepBacs – Mixed segmentation dataset and StarDist model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-staphylococcus-aureus-widefield-segmentation-dataset">DeepBacs – Staphylococcus aureus widefield segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-a-training-strategy">Developing a Training Strategy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#development-of-a-platform-for-advanced-optics-education-training-and-prototyping">Development of a platform for advanced optics education, training and prototyping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#digital-phase-contrast-on-primary-dermal-human-fibroblasts-cells">Digital Phase Contrast on Primary Dermal Human Fibroblasts cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-local-topography-on-cell-division-of-staphylococci-sp">Effect of local topography on cell division of Staphylococci sp.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#efficiently-starting-institutional-research-data-management">Efficiently starting institutional research data management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#einblicke-ins-forschungsdatenmanagement-darf-ich-das-veroffentlichen-rechtsfragen-im-umgang-mit-forschungsdaten">Einblicke ins Forschungsdatenmanagement - Darf ich das veröffentlichen? Rechtsfragen im Umgang mit Forschungsdaten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#engineering-a-software-environment-for-research-data-management-of-microscopy-image-data-in-a-core-facility">Engineering a Software Environment for Research Data Management of Microscopy Image Data in a Core Facility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-scientific-ambassadors-program">Euro-BioImaging  Scientific Ambassadors Program</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-eric-annual-report-2022">Euro-BioImaging ERIC Annual Report 2022</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-s-guide-to-fair-bioimage-data-practical-tasks">Euro-BioImaging’s Guide to FAIR BioImage Data - Practical Tasks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-s-template-for-research-data-management-plans">Euro-BioImaging’s Template for Research Data Management Plans</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#euro-bioimaging-batchconvert-v0-0-4">Euro-BioImaging/BatchConvert: v0.0.4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evident-oir-sample-files-tiles-stitched-image-fv-4000">Evident OIR sample files tiles + stitched image - FV 4000</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evident-oir-sample-files-with-lambda-scan-fv-4000">Evident OIR sample files with lambda scan - FV 4000</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-imaris-ims-datasets">Example Imaris ims datasets.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-microscopy-metadata-json-files-produced-using-micro-meta-app-to-document-example-microscopy-experiments-performed-at-individual-core-facilities">Example Microscopy Metadata JSON files produced using Micro-Meta App to document example microscopy experiments performed at individual core facilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-operetta-dataset">Example Operetta Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#excel-template-for-adding-key-value-pairs-to-images">Excel template for adding Key-Value Pairs to images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expansion-and-fluctuations-enhanced-microscopy-for-nanoscale-molecular-profiling-of-cells-and-tissues-data-processing-manual">Expansion and fluctuations-enhanced microscopy for nanoscale molecular profiling of cells and tissues - Data processing manual</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explainable-ai-for-computer-vision">Explainable AI for Computer Vision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fiber-and-vessel-dataset-for-segmentation-and-characterization">Fiber and vessel dataset for segmentation and characterization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forschungsdatenmanagement-zukunftsfest-gestalten-impulse-fur-die-strukturevaluation-der-nationalen-forschungsdateninfrastruktur-nfdi">Forschungsdatenmanagement zukunftsfest gestalten – Impulse für die   Strukturevaluation der Nationalen Forschungsdateninfrastruktur (NFDI)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-cells-to-pixels-bridging-biologists-and-image-analysts-through-a-common-language">From Cells to Pixels: Bridging Biologists and  Image Analysts Through a Common Language</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-paper-to-pixels-navigation-through-your-research-data-presentations-of-speakers">From Paper to Pixels: Navigation through your Research Data - presentations of speakers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#galaxy-meets-omero-overview-on-the-galaxy-omero-suite-and-vizarr-viewer">Galaxy meets OMERO! Overview on the Galaxy OMERO-suite and Vizarr Viewer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gerbi-chat-teil-1-vom-bedarf-bis-zum-groszgerateantrag-schreiben">GerBI-Chat: Teil 1 - Vom Bedarf bis zum Großgeräteantrag-Schreiben</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gerbi-chat-teil-2-wie-schreibe-ich-am-besten-einen-groszegrateantrag">GerBI-Chat: Teil 2 - Wie schreibe ich am besten einen Großegräteantrag</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started-with-python-intro-and-set-up-a-conda-environment">Getting started with Python: intro and set-up a conda environment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#globias-in-person-workshop-2024">GloBIAS in-person workshop 2024</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gut-analysis-toolbox">Gut Analysis Toolbox</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gut-analysis-toolbox-training-data-and-2d-models-for-segmenting-enteric-neurons-neuronal-subtypes-and-ganglia">Gut Analysis Toolbox: Training data and 2D models for segmenting enteric neurons, neuronal subtypes and ganglia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hpa-nucleus-segmentation-dpnunet">HPA Nucleus Segmentation (DPNUnet)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking">HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hackaton-results-conversion-of-knime-image-analysis-workflows-to-galaxy">Hackaton Results - Conversion of KNIME image analysis workflows to Galaxy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hela-kyoto-cells-under-the-scope">HeLa “Kyoto” cells under the scope</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-throughput-automated-data-analysis-and-data-management-workflow-with-cellprofiler-and-omero">High throughput &amp; automated data analysis and data management workflow with Cellprofiler and OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-dab-staining-axioscan-bf-20x">Human DAB staining Axioscan BF 20x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-lung-tissue-microscopy-dic-fluorescence-cell-and-nuclei-semantic-instance-annotations">Human Lung Tissue Microscopy (DIC, Fluorescence, Cell and Nuclei Semantic Instance Annotations)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#i3d-bio-s-omero-training-material-re-usable-adjustable-multi-purpose-slides-for-local-user-training">I3D:bio’s OMERO training material: Re-usable, adjustable, multi-purpose slides for local user training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ics-ids-stitched-file">ICS/IDS stitched file</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-analysis-using-galaxy">Image Analysis using Galaxy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-repository-decision-tree-where-do-i-deposit-my-imaging-data">Image Repository Decision Tree - Where do I deposit my imaging data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-handling-using-fiji-training-materials">Image handling using Fiji - training materials</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imagej-tool-for-percentage-estimation-of-pneumonia-in-lungs">ImageJ tool for percentage estimation of pneumonia in lungs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-kidney">Implantation of abdominal imaging windows on the mouse kidney</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-kidney-short-version">Implantation of abdominal imaging windows on the mouse kidney - short version</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-liver">Implantation of abdominal imaging windows on the mouse liver</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implantation-of-abdominal-imaging-windows-on-the-mouse-liver-short-version">Implantation of abdominal imaging windows on the mouse liver - short version</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#incell-datasets-with-mix-of-2d-and-3d-failed-to-be-read">InCell datasets with mix of 2D and 3D failed to be read</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ink-in-a-dish">Ink in a dish</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#insights-and-impact-from-five-cycles-of-essential-open-source-software-for-science">Insights and Impact From Five Cycles of Essential Open Source Software for Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#insights-from-acquiring-open-medical-imaging-datasets-for-foundation-model-development">Insights from Acquiring Open Medical Imaging  Datasets for Foundation Model Development</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Insights from Acquiring Open Medical Imaging Datasets for Foundation Model Development</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#institutionalization-and-collaboration-as-a-way-of-addressing-the-challenges-open-science-presents-to-libraries-the-university-of-konstanz-as-a-national-pioneer">Institutionalization and Collaboration as a Way of Addressing the Challenges Open Science Presents to Libraries: The University of Konstanz as a National Pioneer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-of-bioimage-and-omics-data-resources">Integration of Bioimage and *Omics data resources</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-bioimage-analysis-workflow-with-clij-eabias-2025-training-event">Interactive Bioimage Analysis Workflow with CLIJ (@EABIAS 2025 training event)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-image-data-flow-graphs">Interactive Image Data Flow Graphs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intravital-microscopy-contrasting-agents-for-application-database">Intravital microscopy contrasting agents for application - Database</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-omero-vitessce-an-omero-web-plugin-for-multi-modal-data">Introducing OMERO-vitessce: an OMERO.web plugin for multi-modal data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-omero-frankfurt-online">Introduction to OMERO - Frankfurt - online</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-research-data-management-and-open-research">Introduction to Research Data Management and Open Research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-light-microscopy-widefield-microscopy">Introduction to light-microscopy / Widefield microscopy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-pair-template-for-annotation-in-omero-for-light-microscopy-data-acquired-with-axioscan7-core-facility-cellular-imaging-cfci">Key-Value pair template for annotation in OMERO for light microscopy data acquired with AxioScan7 - Core Facility Cellular Imaging (CFCI)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-pair-template-for-annotation-of-datasets-in-omero-perikles-study">Key-Value pair template for annotation of datasets in OMERO (PERIKLES study)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-pair-template-for-annotation-of-datasets-in-omero-for-light-and-electron-microscopy-data-within-the-research-group-of-prof-muller-reichert">Key-Value pair template for annotation of datasets in OMERO for light- and electron microscopy data within the research group of Prof. Müller-Reichert</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kollaboratives-arbeiten-und-versionskontrolle-mit-git">Kollaboratives Arbeiten und Versionskontrolle mit Git</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kriterienkatalog-fur-materialien-aus-dem-themenbereich-forschungsdatenmanagement">Kriterienkatalog für Materialien aus dem Themenbereich Forschungsdatenmanagement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leo-linking-eln-with-omero">LEO: Linking ELN with OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lmrg-image-analysis-study-fish-datasets">LMRG Image Analysis Study - FISH datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lmrg-image-analysis-study-nuclei-datasets">LMRG Image Analysis Study - nuclei datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lsm-example-j-dubrulle">LSM example J. Dubrulle</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lz4-compressed-imaris-ims-example-datasets">LZ4-compressed Imaris ims example datasets.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#large-language-models-an-introduction-for-life-scientists">Large Language Models: An Introduction for Life Scientists</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#large-tiling-confocal-acquisition-rat-brain">Large tiling confocal acquisition (rat brain)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laulauthom-maskfromrois-fiji-masks-from-rois-plugins-for-fiji-initial-release">LauLauThom/MaskFromRois-Fiji: Masks from ROIs plugins for Fiji - initial release</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laulauthom-maskfromrois-fiji-v1-0-1-better-handle-cancel">LauLauThom/MaskFromRois-Fiji: v1.0.1 - better handle “cancel”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-and-training-bio-image-analysis-in-the-age-of-ai">Learning and Training Bio-image Analysis in the Age of AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leica-lif-file-with-errors-in-channel-order-when-imported-with-bio-formats">Leica (.lif) file with errors in channel order when imported with Bio-formats</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leitfaden-zur-digitalen-datensparsamkeit-mit-praxisbeispielen">Leitfaden zur digitalen Datensparsamkeit (mit Praxisbeispielen)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limeseg-test-datasets">LimeSeg Test Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linked-open-data-for-microbial-population-biology">Linked (Open) Data for Microbial Population Biology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#liver-micrometastases-area-quantification-using-qupath-and-pixel-classifier">Liver Micrometastases area quantification using QuPath and pixel classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lynsec-lymphoma-nuclear-segmentation-and-classification">LyNSeC: Lymphoma Nuclear Segmentation and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#masterclasses-from-the-euro-bioimaging-evolve-mentoring-programme-2025">Masterclasses from the Euro-Bioimaging EVOLVE Mentoring programme 2025</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-reporter-activity-domain-in-epi-aggregates-and-gastruloids-ijm">Measuring reporter activity domain in EPI aggregates and Gastruloids.ijm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations">Melanoma Histopathology Dataset with Tissue and Nuclei Annotations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#membrain-seg-training-data">MemBrain-seg training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memorandum-of-understanding-of-nfdi-consortia-from-earth-chemical-and-life-sciences-to-support-a-network-called-the-geo-chem-life-science-helpdesk-cluster">Memorandum of Understanding of NFDI consortia from Earth-, Chemical and Life Sciences to support a network called the Geo-Chem-Life Science Helpdesk Cluster</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metadata-annotation-workflow-for-omero-with-tabbles">Metadata Annotation Workflow for OMERO with Tabbles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metadata-in-bioimaging">Metadata in Bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#microsam-talks">MicroSam-Talks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modular-training-resources-for-bioimage-analysis">Modular training resources for bioimage analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#morphological-analysis-of-neural-cells-with-weka-and-snt-fiji-plugins">Morphological analysis of neural cells with WEKA and SNT Fiji plugins</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-template-matching-for-object-detection-slides">Multi-Template-Matching for object-detection (slides)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiplexed-histology-of-covid-19-post-mortem-lung-samples-control-case-1-fov1">Multiplexed histology of COVID-19 post-mortem lung samples - CONTROL CASE 1 FOV1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#my-journey-through-bioimage-analysis-teaching-methods-from-classroom-to-cloud">My Journey Through Bioimage Analysis Teaching Methods From Classroom to Cloud</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage">NFDI4BIOIMAGE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis-online-kick-off-2023">NFDI4BIOIMAGE - National Research Data Infrastructure for Microscopy and BioImage Analysis - Online Kick-Off 2023</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis-conference-talk-the-pelagic-imaging-consortium-meets-helmholtz-imaging-5-10-2023-hamburg">NFDI4BIOIMAGE - National Research Data Infrastructure for Microscopy and BioImage Analysis [conference talk: The Pelagic Imaging Consortium meets Helmholtz Imaging, 5.10.2023, Hamburg]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-national-research-data-infrastructure-for-microscopy-and-bioimage-analysis">NFDI4BIOIMAGE - National Research Data Infrastructure for Microscopy and Bioimage Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-data-management-illustrations-by-henning-falk">NFDI4BIOIMAGE data management illustrations by Henning Falk</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-ta3-hackathon-uoc-2023-cologne-hackathon-2023-github-repository">NFDI4Bioimage - TA3-Hackathon - UoC-2023 (Cologne-Hackathon-2023, GitHub repository)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-calendar-2024-october-original-image">NFDI4Bioimage Calendar 2024 October; original image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi4bioimage-calendar-2025-march-original-image">NFDI4Bioimage Calendar 2025 March; original image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#navigating-the-bioimage-analysis-landscape-understanding-the-community-and-its-collaborative-dynamics">Navigating the Bioimage Analysis Landscape: Understanding the Community  and its Collaborative Dynamics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nd2-does-not-open-in-fiji-bio-formats-8-1-1">Nd2 does not open in Fiji Bio_formats 8.1.1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nd2-does-not-open-in-fiji-bio-formats-8-1-1-additional-files">Nd2 does not open in Fiji Bio_formats 8.1.1 (additional files)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurips-2022-cell-segmentation-competition-dataset">NeurIPS 2022 Cell Segmentation Competition Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-kid-on-the-nfdi-block-nfdi4bioimage-a-national-initiative-for-fair-data-management-in-bioimaging-and-bioimage-analysis">New Kid on the (NFDI) Block: NFDI4BIOIMAGE  - A National Initiative for FAIR Data Management in Bioimaging and Bioimage Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nextflow-scalable-and-reproducible-scientific-workflows">Nextflow: Scalable and reproducible scientific workflows</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ocelot-overlapped-cell-on-tissue-dataset-for-histopathology">OCELOT: Overlapped Cell on Tissue Dataset for Histopathology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ome2024-ngff-challenge-results">OME2024 NGFF Challenge Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#omexcavator-a-tool-for-exporting-and-connecting-bioimaging-specific-metadata-in-wider-knowledge-graphs">OMExcavator: a tool for exporting and connecting  Bioimaging-specific metadata in wider knowledge graphs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#omexcavator-a-tool-for-exporting-and-connecting-domain-specific-metadata-in-a-wider-knowledge-graph">OMExcavator: a tool for exporting and connecting domain-specific metadata in a wider knowledge graph</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#open-science-sharing-licensing">Open Science, Sharing &amp; Licensing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimisation-and-validation-of-a-swarm-intelligence-based-segmentation-algorithm-for-low-contrast-positron-emission-tomography">Optimisation and Validation of a Swarm Intelligence based Segmentation Algorithm for low Contrast Positron Emission Tomography</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimized-cranial-window-implantation-for-subcellular-and-functional-imaging-in-vivo">Optimized cranial window implantation for subcellular and functional imaging in vivo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parhyale-3d-segmentation-dataset">Parhyale 3D segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#platynereis-em-training-data">Platynereis EM training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprint-be-sustainable-recommendations-for-fair-resources-in-life-sciences-research-eosc-life-s-lessons">Preprint: “Be Sustainable”, Recommendations for FAIR Resources in Life Sciences research: EOSC-Life’s Lessons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prodgerlab-stardist-hiv-target-cell-training-set">ProdgerLab-StarDist-HIV Target Cell Training Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-engineering-agentic-workflows-and-multi-modal-large-language-models">Prompt Engineering, Agentic Workflows and Multi-modal Large Language Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qupath-open-source-software-for-analysing-awkward-images">QuPath: Open source software for analysing (awkward) images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rdf-as-a-bridge-to-domain-platforms-like-omero-or-there-and-back-again">RDF as a bridge to domain-platforms like OMERO, or There and back again.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-data-management-on-campus-and-in-nfdi4bioimage">RESEARCH DATA MANAGEMENT on Campus and in NFDI4BIOIMAGE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstructed-images-of-a-2dsim-multiposition-acquisition">Reconstructed images of a 2DSIM multiposition acquisition.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#report-on-a-pilot-study-implementation-of-omero-for-microscopy-data-management">Report on a pilot study:  Implementation of OMERO for  microscopy data management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-data-management-seminar-slides">Research Data Management Seminar - Slides</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-data-managemet-and-how-not-to-get-overwhelmed-with-data">Research Data Managemet and how not to get overwhelmed with data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-tissue-segmentation-dataset">Root tissue segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#round-table-workshop-1-sample-stabilization-in-intravital-imaging">Round Table Workshop 1 - Sample Stabilization in intravital Imaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#round-table-workshop-2-correction-of-drift-and-movement">Round Table Workshop 2 - Correction of Drift and Movement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-data-for-pr-4284-https-github-com-ome-bioformats-pull-4284">Sample data for PR#4284 (https://github.com/ome/bioformats/pull/4284)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sciaugment">SciAugment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmentation-of-nuclei-in-histopathology-images-by-deep-regression-of-the-distance-map">Segmentation of Nuclei in Histopathology Images by deep regression of the distance map</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmenting-cells-in-a-spheroid-in-3d-using-2d-stardist-within-trackmate">Segmenting cells in a spheroid in 3D using 2D StarDist within TrackMate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#single-cell-approach-dissecting-agr-quorum-sensing-dynamics-in-staphylococcus-aureus">Single-cell approach dissecting agr quorum sensing dynamics in Staphylococcus aureus</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#slides-about-flute-a-python-gui-for-interactive-phasor-analysis-of-flim-data">Slides about FLUTE: a Python GUI for interactive phasor analysis of FLIM data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#so-geschlossen-wie-notig-so-offen-wie-moglich-datenschutz-beim-umgang-mit-forschungsdaten">So geschlossen wie nötig, so offen wie möglich - Datenschutz beim Umgang mit Forschungsdaten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stackview-sliceplot-example-data">Stackview sliceplot example data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-adipocyte-segmentation-training-data-training-notebook-and-model">StarDist Adipocyte Segmentation Training data, Training Notebook and Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-model-and-data-for-the-segmentation-of-yersinia-enterocolitica-cells-in-widefield-images">StarDist model and data for the segmentation of Yersinia enterocolitica cells in widefield images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-aspc1-lifeact">StarDist_AsPC1_Lifeact</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-monocytes-dataset">StarDist_BF_Monocytes_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-neutrophil-dataset">StarDist_BF_Neutrophil_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-cancer-cell-dataset-10x">StarDist_BF_cancer_cell_dataset_10x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-cancer-cell-dataset-20x">StarDist_BF_cancer_cell_dataset_20x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-fluorescent-cells">StarDist_Fluorescent_cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-huvec-nuclei-dataset">StarDist_HUVEC_nuclei_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-tumorcell-nuclei">StarDist_TumorCell_nuclei</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-model-and-training-dataset-for-automated-tracking-of-mda-mb-231-and-bt20-cells">Stardist model and training dataset for automated tracking of MDA-MB-231 and BT20 cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-miapaca2-from-cd44">Stardist_MiaPaCa2_from_CD44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structuring-of-data-and-metadata-in-bioimaging-concepts-and-technical-solutions-in-the-context-of-linked-data">Structuring of Data and Metadata in Bioimaging: Concepts and technical Solutions in the Context of Linked Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sustainable-data-stewardship">Sustainable Data Stewardship</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synapsenet-training-data">SynapseNet Training Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-service-for-research-data-management-and-knowledge-discovery-in-low-temperature-plasma-physics">Terminology service for research data management and knowledge discovery in low-temperature plasma physics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-dataset-for-whole-slide-image-registration">Test Dataset for Whole Slide Image Registration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-information-infrastructure-for-bioimage-data-i3d-bio-project-to-advance-fair-microscopy-data-management-for-the-community">The Information Infrastructure for BioImage Data (I3D:bio) project to advance FAIR microscopy data management for the community</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-role-of-helmholtz-centers-in-nfdi4bioimage-a-national-consortium-enhancing-fair-data-management-for-microscopy-and-bioimage-analysis">The role of Helmholtz Centers in NFDI4BIOIMAGE - A national consortium enhancing FAIR data management for microscopy and bioimage analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#thinking-data-management-on-different-scales">Thinking data management on different scales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#towards-preservation-of-life-science-data-with-nfdi4bioimage">Towards Preservation of Life Science Data with NFDI4BIOIMAGE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#towards-transparency-and-knowledge-exchange-in-ai-assisted-data-analysis-code-generation">Towards Transparency and Knowledge Exchange in AI-assisted Data Analysis Code Generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#towards-open-and-standardised-imaging-data-an-introduction-to-bio-formats-ome-tiff-and-ome-zarr">Towards open and standardised imaging data: an introduction to Bio-Formats, OME-TIFF, and OME-Zarr</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-trainer-concept-on-research-data-management">Train-the-Trainer Concept on Research Data Management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-computational-skills-in-the-age-of-ai">Training Computational Skills in the Age of AI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-set-of-microscopy-images-for-dietler-et-al-nature-communications-2020">Training set of microscopy images for Dietler et al. Nature Communications 2020</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vision-language-models-for-bio-image-data-science">Vision Language Models for Bio-image Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset">Volumetric segmentation of biological cells and subcellular structures for optical diffraction tomography images - dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-not-to-do-when-creating-a-data-management-plan-dmp">WHAT NOT TO DO WHEN CREATING A DATA MANAGEMENT PLAN (DMP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#welcome-to-bioimage-town">Welcome to BioImage Town</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#who-you-gonna-call-data-stewards-to-the-rescue">Who you gonna call? - Data Stewards to the rescue</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-for-user-introduction-into-microscopy-omero-and-data-management-at-center-for-advanced-imaging">Workflow for user introduction into microscopy, OMERO and data management at Center for Advanced imaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#working-group-charter-rdm-helpdesk-network">Working Group Charter. RDM Helpdesk Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zeiss-axiozoom-stage-adapter">Zeiss AxioZoom Stage Adapter</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zeiss-axiozoom-stage-adapter-12-6well-plate">Zeiss AxioZoom Stage Adapter - 12/6Well Plate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zeiss-axiozoom-stage-adapter-em-block-holder">Zeiss AxioZoom Stage Adapter - EM block holder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zeiss-axiozoom-stage-adapter-microscope-slides">Zeiss AxioZoom Stage Adapter - Microscope slides</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zerocostdl4mic-stardist-2d-example-training-and-test-dataset-light">ZeroCostDL4Mic - Stardist 2D example training and test dataset (light)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zerocostdl4mic-stardist-example-training-and-test-dataset">ZeroCostDL4Mic - Stardist example training and test dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bina-cc-scalable-strategies-for-a-next-generation-of-fair-bioimaging">[BINA CC] Scalable strategies for a next-generation of FAIR bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cidas-scalable-strategies-for-a-next-generation-of-fair-bioimaging">[CIDAS] Scalable strategies for a next-generation of FAIR bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cmcb-scalable-strategies-for-a-next-generation-of-fair-bioimaging">[CMCB] Scalable strategies for a next-generation of FAIR bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cordi-2023-zarr-a-cloud-optimized-storage-for-interactive-access-of-large-arrays">[CORDI 2023] Zarr: A Cloud-Optimized Storage for Interactive Access of Large Arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#community-meeting-2024-overview-team-image-data-analysis-and-management">[Community Meeting 2024] Overview Team Image Data Analysis and Management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#community-meeting-2024-supporting-and-financing-rdm-projects-within-gerbi">[Community Meeting 2024] Supporting and financing RDM projects within GerBI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi-2024-ai-s-dirty-little-secret-without">[ELMI 2024]  AI’s Dirty Little Secret: Without</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi-2024-ai-s-dirty-little-secret-without-fair-data-it-s-just-fancy-math">[ELMI 2024] AI’s Dirty Little Secret: Without FAIR Data, It’s Just Fancy Math</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi2025-bridging-communities-with-ome-zarr">[ELMI2025] Bridging communities with OME-Zarr</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi2025-the-road-to-ome-zarr-1-0">[ELMI2025] The Road to OME-Zarr 1.0</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elmi2025-workshop-fair101-navigating-fair-data-from-principles-to-practice">[ELMI2025] Workshop: FAIR101 - Navigating FAIR data from principles to practice</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gbi-eoe-vii-five-or-ten-must-have-items-for-making-it-infrastructure-for-managing-bioimage-data">[GBI EOE VII] Five (or ten) must-have items for making IT infrastructure for managing bioimage data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gbi-eoe-ix-nfdi4bioimage">[GBI EoE IX] NFDI4BIOIMAGE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#i2k-scalable-strategies-for-a-next-generation-of-fair-bioimaging">[I2K] Scalable strategies for a next-generation of FAIR bioimaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n4bi-ahm-welcome-to-bioimage-town">[N4BI AHM] Welcome to BioImage Town</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nfdi-tech-talk-cloud-based-image-science">[NFDI Tech Talk] Cloud Based Image Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#swat4hcls-2023-nfdi4bioimage-perspective-for-a-national-bioimage-standard">[SWAT4HCLS 2023] NFDI4BIOIMAGE: Perspective for a national bioimage standard</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#short-talk-nfdi4bioimage-a-consortium-in-the-national-research-data-infrastructure">[Short Talk] NFDI4BIOIMAGE - A consortium in the National Research Data Infrastructure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webinar-a-journey-to-fair-bioimage-data">[Webinar] A journey to FAIR bioimage data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-material-fit-for-omero-how-imaging-facilities-and-it-departments-work-together-to-enable-rdm-for-bioimaging-october-16-17-2024-heidelberg">[Workshop Material] Fit for OMERO - How imaging facilities and IT departments work together to enable RDM for bioimaging, October 16-17, 2024, Heidelberg</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-bioimage-data-management-and-analysis-with-omero">[Workshop] Bioimage data management and analysis with OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-fair-data-handling-for-microscopy-structured-metadata-annotation-in-omero">[Workshop] FAIR data handling for microscopy: Structured metadata annotation in OMERO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-managing-fair-microscopy-data-at-scale-for-universities-and-research-institutions-an-introduction-for-non-imaging-stakeholders">[Workshop] Managing FAIR microscopy data at scale for universities and research institutions: an introduction for non-imaging stakeholders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-research-data-management-for-microscopy-and-bioimage-analysis">[Workshop] Research Data Management for Microscopy and BioImage Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ilastik-interactive-machine-learning-for-bio-image-analysis">ilastik: interactive machine learning for (bio)image analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imaris-file-not-read-by-bfgetreader">imaris file not read by bfGetReader()</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#martinschatz-cz-scicount-v1-0-0-with-reusable-example-notebooks">martinschatz-cz/SciCount: v1.0.0 with reusable example notebooks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantixed-thedigitalcell-first-complete-code-set">quantixed/TheDigitalCell: First complete code set</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Robert Haase, Clément Caporal,... and the NFDI4BioImage Initiative
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on 2025-07-16.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Copyright: Licensed <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC-BY 4.0</a> unless mentioned otherwise. 
Contributions and feedback are welcome.
</p>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>