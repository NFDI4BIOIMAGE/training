
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Ai-ready (84) &#8212; NFDI4BioImage Training Materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/css/searchbox.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/0.6.0/lunr.min.js"></script>
    <script src="../_static/js/searchbox.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tags/ai-ready';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Artificial intelligence (51)" href="artificial_intelligence.html" />
    <link rel="prev" title="YML format" href="../contributing/format.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="2025-09-20"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../readme.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="NFDI4BioImage Training Materials - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="NFDI4BioImage Training Materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../readme.html">
                    NFDI4BioImage Training Materials
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">What's new</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../whats_new.html">Recently added (10)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contributing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">How to contribute</a></li>

<li class="toctree-l1"><a class="reference internal" href="../contributing/submit_app.html">Using the Training Materials Submission App</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/format.html">YML format</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By tag</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Ai-ready (84)</a></li>
<li class="toctree-l1"><a class="reference internal" href="artificial_intelligence.html">Artificial intelligence (51)</a></li>
<li class="toctree-l1"><a class="reference internal" href="bioimage_analysis.html">Bioimage analysis (210)</a></li>
<li class="toctree-l1"><a class="reference internal" href="bioinformatics.html">Bioinformatics (19)</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_stewardship.html">Data stewardship (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="exclude_from_dalia.html">Exclude from dalia (455)</a></li>
<li class="toctree-l1"><a class="reference internal" href="fair-principles.html">Fair-principles (27)</a></li>
<li class="toctree-l1"><a class="reference internal" href="fiji.html">Fiji (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="galaxy.html">Galaxy (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagej.html">Imagej (20)</a></li>
<li class="toctree-l1"><a class="reference internal" href="include_in_dalia.html">Include in dalia (333)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licensing.html">Licensing (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="metadata.html">Metadata (15)</a></li>
<li class="toctree-l1"><a class="reference internal" href="napari.html">Napari (14)</a></li>
<li class="toctree-l1"><a class="reference internal" href="neubias.html">Neubias (27)</a></li>
<li class="toctree-l1"><a class="reference internal" href="nfdi4bioimage.html">Nfdi4bioimage (80)</a></li>
<li class="toctree-l1"><a class="reference internal" href="omero.html">Omero (39)</a></li>
<li class="toctree-l1"><a class="reference internal" href="open_science.html">Open science (9)</a></li>
<li class="toctree-l1"><a class="reference internal" href="open_source_software.html">Open source software (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python (71)</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="research_data_management.html">Research data management (145)</a></li>
<li class="toctree-l1"><a class="reference internal" href="sharing.html">Sharing (12)</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow.html">Workflow (9)</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow_engine.html">Workflow engine (13)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By content type</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../content_types/blog%20post.html">Blog post (28)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/book.html">Book (21)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/code.html">Code (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/collection.html">Collection (86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/data.html">Data (92)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/document.html">Document (8)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/documentation.html">Documentation (19)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/event.html">Event (8)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/github%20repository.html">Github repository (70)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/notebook.html">Notebook (57)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/online%20tutorial.html">Online tutorial (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/poster.html">Poster (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/preprint.html">Preprint (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/publication.html">Publication (71)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/slides.html">Slides (84)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/tutorial.html">Tutorial (48)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/video.html">Video (38)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/website.html">Website (11)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../content_types/workshop.html">Workshop (14)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By license</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../licenses/all_rights_reserved.html">All rights reserved (13)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/apache-2.0.html">Apache-2.0 (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/bsd-2-clause.html">Bsd-2-clause (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/bsd-3-clause.html">Bsd-3-clause (37)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc-by-3.0.html">Cc-by-3.0 (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc-by-4.0.html">Cc-by-4.0 (408)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc-by-nc-sa-4.0.html">Cc-by-nc-sa-4.0 (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc-by-sa-4.0.html">Cc-by-sa-4.0 (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/cc0-1.0.html">Cc0-1.0 (25)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/gpl-2.0.html">Gpl-2.0 (8)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/gpl-3.0.html">Gpl-3.0 (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/mit.html">Mit (32)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../licenses/unknown.html">Unknown (109)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By domain</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../domain/bbbc.broadinstitute.org.html">Bbbc.broadinstitute.org (12)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/biapol.github.io.html">Biapol.github.io (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/docs.google.com.html">Docs.google.com (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/doi.org.html">Doi.org (347)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/f1000research.com.html">F1000research.com (11)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/focalplane.biologists.com.html">Focalplane.biologists.com (14)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/git.mpi-cbg.de.html">Git.mpi-cbg.de (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/github.com.html">Github.com (151)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/training.galaxyproject.org.html">Training.galaxyproject.org (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/www.biorxiv.org.html">Www.biorxiv.org (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/www.ebi.ac.uk.html">Www.ebi.ac.uk (18)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/www.nature.com.html">Www.nature.com (18)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/www.youtube.com.html">Www.youtube.com (28)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain/zenodo.org.html">Zenodo.org (327)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../statistics/readme.html">Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../export/readme.html">Open data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gdpr_compliance.html">GDPR Compliance Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../imprint.html">Imprint</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NFDI4BIOIMAGE/training" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NFDI4BIOIMAGE/training/issues/new?title=Issue%20on%20page%20%2Ftags/ai-ready.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/tags/ai-ready.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Ai-ready (84)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-ground-truth-annotations-of-nuclei-in-3d-microscopy-volumes">3D Ground Truth Annotations of Nuclei in 3D Microscopy Volumes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-hl60-cell-line-synthetic-data">3D HL60 Cell line (synthetic data)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-cell-shape-of-drosophila-wing-disc">3D cell shape of Drosophila Wing Disc</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-light-sheet-microscopy-data-for-selma3d-2024-challenge-training-subset-with-annotations">3D light-sheet microscopy data for SELMA3D 2024 challenge - Training subset with annotations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-nuclei-instance-segmentation-dataset-of-fluorescence-microscopy-volumes-of-c-elegans">3D nuclei instance segmentation dataset of fluorescence microscopy volumes of C. elegans</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deep-learning-approach-to-quantify-auditory-hair-cells">A deep learning approach to quantify auditory hair cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-annotated-fluorescence-image-dataset-for-training-nuclear-segmentation-methods">An annotated fluorescence image dataset for training nuclear segmentation methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-annotated-high-content-fluorescence-microscopy-dataset-with-hoechst-33342-stained-nuclei-and-manually-labelled-outlines">An annotated high-content fluorescence microscopy dataset with Hoechst 33342-stained nuclei and manually labelled outlines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-image-based-data-driven-analysis-of-cellular-architecture-in-a-developing-tissue">An image-based data-driven analysis of cellular architecture in a developing tissue</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessment-of-residual-breast-cancer-cellularity-after-neoadjuvant-chemotherapy-using-digital-pathology">Assessment of Residual Breast Cancer Cellularity after Neoadjuvant Chemotherapy using Digital Pathology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-labelling-of-hela-kyoto-cells-using-deep-learning-tools">Automatic labelling of HeLa “Kyoto” cells using Deep Learning tools</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bccd-dataset">BCCD Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#breast-cancer-nuclei-images-for-dl-training-zerocostdl4mic-stardist-model">Breast Cancer Nuclei images for DL Training + ZeroCostDL4Mic StarDist Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#breast-cancer-semantic-segmentation-bcss-dataset">Breast Cancer Semantic Segmentation (BCSS) dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellbindb-a-large-scale-multimodal-annotated-dataset">CellBinDB: A Large-Scale Multimodal Annotated Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments">Cellpose training data and scripts from “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers">Cellpose training data and scripts from “Machine learning for histological annotation and quantification of cortical layers”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chinese-hamster-ovary-cells">Chinese Hamster Ovary Cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-1-breast-cancer-cell-dataset">Combining StarDist and TrackMate example 1 -  Breast cancer cell dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-2-t-cell-dataset">Combining StarDist and TrackMate example 2 -  T cell dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-3-flow-chamber-dataset">Combining StarDist and TrackMate example 3 -  Flow chamber dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cryonuseg">CryoNuSeg</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-segmentation-projects-of-fib-sem-dataset-of-u2-os-cell">Deep learning segmentation projects of FIB-SEM dataset of U2-OS cell</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-training-data-jove">Deep learning training data (JOVE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-bacillus-subtilis-fluorescence-segmentation-dataset">DeepBacs – Bacillus subtilis fluorescence segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-escherichia-coli-bright-field-segmentation-dataset">DeepBacs – Escherichia coli bright field segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-mixed-segmentation-dataset-and-stardist-model">DeepBacs – Mixed segmentation dataset and StarDist model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-staphylococcus-aureus-widefield-segmentation-dataset">DeepBacs – Staphylococcus aureus widefield segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#drosophila-kc167-cells">Drosophila Kc167 cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-local-topography-on-cell-division-of-staphylococci-sp">Effect of local topography on cell division of Staphylococci sp.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embryonic-mice-ultrasound-volumes-with-body-and-brain-volume-segmentation-masks">Embryonic mice ultrasound volumes with body and brain volume segmentation masks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fiber-and-vessel-dataset-for-segmentation-and-characterization">Fiber and vessel dataset for segmentation and characterization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#go-nuclear-a-deep-learning-based-toolkit-for-3d-nuclei-segmentation-and-quantitative-analysis-in-cellular-and-tissue-context">Go-Nuclear. A deep learning-based toolkit for 3D nuclei segmentation and quantitative analysis in cellular and tissue context</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ground-truth-cell-body-segmentation-used-for-starfinity-training">Ground-truth cell body segmentation used for Starfinity training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hpa-nucleus-segmentation-dpnunet">HPA Nucleus Segmentation (DPNUnet)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking">HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-ht29-colon-cancer-cells">Human HT29 colon-cancer cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-hepatocyte-and-murine-fibroblast-cells-co-culture-experiment">Human Hepatocyte and Murine Fibroblast cells Co-culture experiment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-lung-tissue-microscopy-dic-fluorescence-cell-and-nuclei-semantic-instance-annotations">Human Lung Tissue Microscopy (DIC, Fluorescence, Cell and Nuclei Semantic Instance Annotations)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-u2os-cells-out-of-focus">Human U2OS cells (out of focus)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lmrg-image-analysis-study-fish-datasets">LMRG Image Analysis Study - FISH datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lmrg-image-analysis-study-nuclei-datasets">LMRG Image Analysis Study - nuclei datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lynsec-lymphoma-nuclear-segmentation-and-classification">LyNSeC: Lymphoma Nuclear Segmentation and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#midog-2021">MIDOG 2021</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations">Melanoma Histopathology Dataset with Tissue and Nuclei Annotations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#membrain-seg-training-data">MemBrain-seg training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monuseg-dataset">MoNuSeg Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monusac-2020">MonuSAC 2020</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mouse-embryo-blastocyst-cells">Mouse embryo blastocyst cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurips-2022-cell-segmentation-competition-dataset">NeurIPS 2022 Cell Segmentation Competition Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nuinsseg">NuInsSeg</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nuclei-of-u2os-cells-in-a-chemical-screen">Nuclei of U2OS cells in a chemical screen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nuclei-of-mouse-embryonic-cells">Nuclei of mouse embryonic cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ocelot-overlapped-cell-on-tissue-dataset-for-histopathology">OCELOT: Overlapped Cell on Tissue Dataset for Histopathology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parhyale-3d-segmentation-dataset">Parhyale 3D segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#platynereis-em-training-data">Platynereis EM training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-axillary-lymph-node-metastasis-in-early-breast-cancer-using-deep-learning-on-primary-tumor-biopsy-slides">Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prodgerlab-stardist-hiv-target-cell-training-set">ProdgerLab-StarDist-HIV Target Cell Training Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-tissue-segmentation-dataset">Root tissue segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmentation-of-nuclei-in-histopathology-images-by-deep-regression-of-the-distance-map">Segmentation of Nuclei in Histopathology Images by deep regression of the distance map</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmenting-cells-in-a-spheroid-in-3d-using-2d-stardist-within-trackmate">Segmenting cells in a spheroid in 3D using 2D StarDist within TrackMate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulated-hl60-cells-from-the-cell-tracking-challenge">Simulated HL60 cells (from the Cell Tracking Challenge)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#single-cell-approach-dissecting-agr-quorum-sensing-dynamics-in-staphylococcus-aureus">Single-cell approach dissecting agr quorum sensing dynamics in Staphylococcus aureus</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-adipocyte-segmentation-training-data-training-notebook-and-model">StarDist Adipocyte Segmentation Training data, Training Notebook and Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-model-and-data-for-the-segmentation-of-yersinia-enterocolitica-cells-in-widefield-images">StarDist model and data for the segmentation of Yersinia enterocolitica cells in widefield images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-aspc1-lifeact">StarDist_AsPC1_Lifeact</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-monocytes-dataset">StarDist_BF_Monocytes_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-neutrophil-dataset">StarDist_BF_Neutrophil_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-cancer-cell-dataset-10x">StarDist_BF_cancer_cell_dataset_10x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-cancer-cell-dataset-20x">StarDist_BF_cancer_cell_dataset_20x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-fluorescent-cells">StarDist_Fluorescent_cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-huvec-nuclei-dataset">StarDist_HUVEC_nuclei_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-tumorcell-nuclei">StarDist_TumorCell_nuclei</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-model-and-training-dataset-for-automated-tracking-of-mda-mb-231-and-bt20-cells">Stardist model and training dataset for automated tracking of MDA-MB-231 and BT20 cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-miapaca2-from-cd44">Stardist_MiaPaCa2_from_CD44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synapsenet-training-data">SynapseNet Training Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-cells">Synthetic cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-images-and-segmentation-masks-simulating-hl-60-cell-nucleus-in-3d">Synthetic images and segmentation masks simulating HL-60 cell nucleus in 3D</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tnbc">TNBC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-set-of-microscopy-images-for-dietler-et-al-nature-communications-2020">Training set of microscopy images for Dietler et al. Nature Communications 2020</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset">Volumetric segmentation of biological cells and subcellular structures for optical diffraction tomography images - dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zerocostdl4mic-stardist-2d-example-training-and-test-dataset-light">ZeroCostDL4Mic - Stardist 2D example training and test dataset (light)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zerocostdl4mic-stardist-example-training-and-test-dataset">ZeroCostDL4Mic - Stardist example training and test dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data">cellpose training data</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="ai-ready-84">
<h1>Ai-ready (84)<a class="headerlink" href="#ai-ready-84" title="Link to this heading">#</a></h1>
<section id="d-ground-truth-annotations-of-nuclei-in-3d-microscopy-volumes">
<h2>3D Ground Truth Annotations of Nuclei in 3D Microscopy Volumes<a class="headerlink" href="#d-ground-truth-annotations-of-nuclei-in-3d-microscopy-volumes" title="Link to this heading">#</a></h2>
<p>Alain Chen, Liming Wu, Seth Winfree, Kenneth Dunn, Paul Salama, Edward Delp, Teresa Zulueta-Coarasa</p>
<p>Published 2024-12-20</p>
<p>Licensed CC-BY-4.0</p>
<p>This submission contains a set of 3D microscopy volumes of cell nuclei from different species and tissues that have been manually segmented. We also provide synthetically generated 3D microscopy volumes that can be used for training segmentation methods.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.ebi.ac.uk/bioimage-archive/galleries/ai/analysed-dataset/S-BIAD1518/">https://www.ebi.ac.uk/bioimage-archive/galleries/ai/analysed-dataset/S-BIAD1518/</a></p>
</section>
<hr class="docutils" />
<section id="d-hl60-cell-line-synthetic-data">
<h2>3D HL60 Cell line (synthetic data)<a class="headerlink" href="#d-hl60-cell-line-synthetic-data" title="Link to this heading">#</a></h2>
<p>David Svoboda, Michal Kozubkek, Stanislav Stejskal</p>
<p>Published 2009-06-01</p>
<p>Licensed CC-BY-3.0</p>
<p>One of the principal challenges in counting or segmenting nuclei is dealing with clustered nuclei. To help assess algorithms performance in this regard, this synthetic image set consists of four subsets with increasing degree of clustering. Each subset is also provided in two diferent levels of quality: high SNR and low SNR.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC024">https://bbbc.broadinstitute.org/BBBC024</a></p>
</section>
<hr class="docutils" />
<section id="d-cell-shape-of-drosophila-wing-disc">
<h2>3D cell shape of Drosophila Wing Disc<a class="headerlink" href="#d-cell-shape-of-drosophila-wing-disc" title="Link to this heading">#</a></h2>
<p>Giulia Paci, Ines Fernandez Mosquera, Pablo Vicente Munuera, Yanlan Mao</p>
<p>Published 2023-08-14</p>
<p>Licensed CC0-1.0</p>
<p>Segmentation masks of individual cells in Drosophila wing discs</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD843-ai.html">https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD843-ai.html</a></p>
</section>
<hr class="docutils" />
<section id="d-light-sheet-microscopy-data-for-selma3d-2024-challenge-training-subset-with-annotations">
<h2>3D light-sheet microscopy data for SELMA3D 2024 challenge - Training subset with annotations<a class="headerlink" href="#d-light-sheet-microscopy-data-for-selma3d-2024-challenge-training-subset-with-annotations" title="Link to this heading">#</a></h2>
<p>Ying Chen, Johannes C. Paetzold, Ali Erturk, Doris Kaltenecker, Mihail Todorov, Harsharan Singh Bhatia, Shan Zhao, Luciano Höher</p>
<p>Published 2024-06-05</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset is the training set with annotations for the SELMA3D challenge. The SELMA3D challenge focuses on self-supervised learning for 3D light-sheet microscopy image segmentation. Its objective is to encourage the development of self-supervised learning methods for general segmentation of various structures in 3D light-sheet microscopy images. The dataset comtains 3D image patches of different labeled biological structures in the brain, including blood vessels, c-Fos labeled brain cells involved in neural activity, cell nuclei, and Alzheimers disease plaques. Each patch includes corresponding pixel-wise annotations for the labeled structures.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.ebi.ac.uk/bioimage-archive/galleries/ai/analysed-dataset/S-BIAD1196/">https://www.ebi.ac.uk/bioimage-archive/galleries/ai/analysed-dataset/S-BIAD1196/</a></p>
</section>
<hr class="docutils" />
<section id="d-nuclei-instance-segmentation-dataset-of-fluorescence-microscopy-volumes-of-c-elegans">
<h2>3D nuclei instance segmentation dataset of fluorescence microscopy volumes of C. elegans<a class="headerlink" href="#d-nuclei-instance-segmentation-dataset-of-fluorescence-microscopy-volumes-of-c-elegans" title="Link to this heading">#</a></h2>
<p>Fuhui Long, Hanchuan Peng, Xiao Liu, Stuart K Kim, Eugene Myers, Dagmar Kainmüller, Martin Weigert</p>
<p>Published 2022-02-01</p>
<p>Licensed CC-BY-4.0</p>
<p>The dataset consists of 28 confocal microscopy volumes of C. elegans worms at the L1 stage and  corresponding stacks of densely annotated nuclei instance segmentation masks.</p>
<ul class="simple">
<li><p>28 raw images and corresponding masks of average dimension (xyz) 1050 x 140 x 140</p></li>
<li><p>Pixelsize (xyz): 0.116 x 0.116 x 0.122μm</p></li>
<li><p>Microscope: Leica confocal microscopy, 63x oil objective</p></li>
</ul>
<p>The original raw data and preliminary annotations were  part of the following publication (please cite if you use the dataset):
 
Long, F., Peng, H., Liu, X., Kim, S. K., &amp; Myers, E. (2009). A 3D digital atlas of C. elegans and its application to single-cell analyses. Nature methods, 6(9), 667-672.</p>
<p>The nuclei annotation masks were further manually curated by Dagmar Kainmueller (MDC Berlin) for the following publication:</p>
<p>Hirsch, P., &amp; Kainmueller, D. (2020). An auxiliary task for learning nuclei segmentation in 3d microscopy images. In Medical Imaging with Deep Learning (pp. 304-321). PMLR.</p>
<p>We provide the dataset already structured into the train/validation/test split as used by the above as well as the following publications: </p>
<p>Weigert, M., Schmidt, U., Haase, R., Sugawara, K., &amp; Myers, G. (2020). Star-convex polyhedra for 3d object detection and segmentation in microscopy. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 3666-3673).
 </p>
<p> </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5942575">https://zenodo.org/records/5942575</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5942575">https://doi.org/10.5281/zenodo.5942575</a></p>
</section>
<hr class="docutils" />
<section id="a-deep-learning-approach-to-quantify-auditory-hair-cells">
<h2>A deep learning approach to quantify auditory hair cells<a class="headerlink" href="#a-deep-learning-approach-to-quantify-auditory-hair-cells" title="Link to this heading">#</a></h2>
<p>Maurizio Cortada, Loïc Sauteur, Michael Lanz, Soledad Levano, Daniel Bodmer</p>
<p>Published 2021-03-09</p>
<p>Licensed CC-BY-4.0</p>
<p>StarDist 2D deep learning model and training dataset.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4590066">https://zenodo.org/records/4590066</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4590066">https://doi.org/10.5281/zenodo.4590066</a></p>
</section>
<hr class="docutils" />
<section id="an-annotated-fluorescence-image-dataset-for-training-nuclear-segmentation-methods">
<h2>An annotated fluorescence image dataset for training nuclear segmentation methods<a class="headerlink" href="#an-annotated-fluorescence-image-dataset-for-training-nuclear-segmentation-methods" title="Link to this heading">#</a></h2>
<p>Sabine Taschner-Mandl, Inge M. Ambros, Peter F. Ambros, Klaus Beiske, Allan Hanbury, Wolfgang Doerr, Tamara Weiss, Maria Berneder, Magdalena Ambros, Eva Bozsaky, Florian Kromp, Teresa Zulueta-Coarasa</p>
<p>Published 2023-03-07</p>
<p>Licensed CC0-1.0</p>
<p>Ground-truth annotated fluorescence image dataset for training nuclear segmentation methods</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD634-ai.html">https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD634-ai.html</a></p>
</section>
<hr class="docutils" />
<section id="an-annotated-high-content-fluorescence-microscopy-dataset-with-hoechst-33342-stained-nuclei-and-manually-labelled-outlines">
<h2>An annotated high-content fluorescence microscopy dataset with Hoechst 33342-stained nuclei and manually labelled outlines<a class="headerlink" href="#an-annotated-high-content-fluorescence-microscopy-dataset-with-hoechst-33342-stained-nuclei-and-manually-labelled-outlines" title="Link to this heading">#</a></h2>
<p>Malou Arvidsson, Salma Kazemi Rashed, Sonja Aits</p>
<p>Published 2022-06-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Here we present a benchmarking dataset of fluorescence microscopy images with Hoechst 33342-stained nuclei together with annotations of nuclei, nuclear fragments and micronuclei. Images were randomly selected from an RNA interference screen with a modified U2OS osteosarcoma cell line, acquired on a Thermo Fischer CX7 high-content imaging system at 20x magnification. Labelling was performed by a single annotator and reviewed by a biomedical expert.</p>
<p>The dataset contains 50 images showing over 2000 labelled nuclear objects in total, which is sufficiently large to train well-performing neural networks for instance or semantic segmentation. It is pre-split into training, development and test set, each in a zip file. The dataset should be referred to as Aitslab_bioimaging1. A brief article describing the dataset is also available (Arvidsson M, Kazemi Rashed S, Aits S. 10.1016/j.dib.2022.108769 )</p>
<p>Dataset description:</p>
<p>Fluorescence microscopy images: original .C01 files and files converted to 8-bit .png format (Grayscale)</p>
<p>Annotations: 24-bit .png format (RGB)</p>
<p>Script used to convert C01 to png images: C01_to_png.py file with python code and <a class="reference external" href="http://readme.md">readme.md</a> file with instructions to run it</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/6657260">https://zenodo.org/records/6657260</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6657260">https://doi.org/10.5281/zenodo.6657260</a></p>
</section>
<hr class="docutils" />
<section id="an-image-based-data-driven-analysis-of-cellular-architecture-in-a-developing-tissue">
<h2>An image-based data-driven analysis of cellular architecture in a developing tissue<a class="headerlink" href="#an-image-based-data-driven-analysis-of-cellular-architecture-in-a-developing-tissue" title="Link to this heading">#</a></h2>
<p>Jonas Hartmann, Mie Wong, Elisa Gallo, Darren Gilmour</p>
<p>Published 2022-12-13</p>
<p>Licensed CC-BY-4.0</p>
<p>3D zebrafish embryo images with single-cell segmentation and point cloud-based morphometry</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD599-ai.html">https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD599-ai.html</a></p>
</section>
<hr class="docutils" />
<section id="assessment-of-residual-breast-cancer-cellularity-after-neoadjuvant-chemotherapy-using-digital-pathology">
<h2>Assessment of Residual Breast Cancer Cellularity after Neoadjuvant Chemotherapy using Digital Pathology<a class="headerlink" href="#assessment-of-residual-breast-cancer-cellularity-after-neoadjuvant-chemotherapy-using-digital-pathology" title="Link to this heading">#</a></h2>
<p>Mohammad Peikari, Sherine Salama, Sharon Nofech-Mozes, Anne L. Martel</p>
<p>Published 2017-10-04</p>
<p>Licensed CC-BY-3.0</p>
<p>Breast cancer (BC) is the second most commonly diagnosed cancer in the U.S. with more than 250,000 new cases of invasive breast cancers reported in 2017. The majority of women with locally advanced and a subset of patients with operable breast cancer will undergo systemic therapy prior to their surgery (neoadjuvant therapy/ NAT) to reduce the size of tumor(s) and possibly further undergo breast conserving surgery. The Post-NAT-BRCA dataset is a collection of representative sections from breast resections in patients with residual invasive BC following NAT. Histologic sections were prepared and digitized to produce high resolution, microscopic images of treated BC tumors. Also included, are clinical features and expert pathology annotations of tumor cellularity and cell types. The Residual Cancer Burden Index (RCBi), is a clinically validated tool for assessment of response to NAT associated with prognosis. Tumor cellularity is one of the parameters used for calculating the RCBi. In this dataset, tumor cellularity refers to a measure of residual disease after NAT, in the form of proportion of malignant tumor inside the tumor bed region; also annotated. (See MD Anderson RCB Calculator for a detailed description of tumor cellularity.) Malignant, healthy, lymphocyte and other labels were also provided for individual cells to aid development of cell segmentation algorithms.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.cancerimagingarchive.net/collection/post-nat-brca/">https://www.cancerimagingarchive.net/collection/post-nat-brca/</a></p>
</section>
<hr class="docutils" />
<section id="automatic-labelling-of-hela-kyoto-cells-using-deep-learning-tools">
<h2>Automatic labelling of HeLa “Kyoto” cells using Deep Learning tools<a class="headerlink" href="#automatic-labelling-of-hela-kyoto-cells-using-deep-learning-tools" title="Link to this heading">#</a></h2>
<p>Romain Guiet</p>
<p>Published 2022-02-25</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: Automatic labelling of HeLa “Kyoto” cells using Deep Learning tools</p>
<p>Data type: Microscopy images from the dataset “HeLa “Kyoto” cells under the scope”, Brightfield (BF), Digital Phase Contrast (DPC, either “raw” or “square-rooted”), Tubulin and H2B fluorescent channel, paired with their corresponding nuclei or cell/cyto label images.</p>
<p>Labels images: Labels images were generated using the script “prepare_trainingDataset_cellpose.ijm”.</p>
<p>Briefly, for 5 defined time-points (1,10,50,100,150), channels of interest were duplicated, resaved and :</p>
<p>-        nuclei label images were obtained using StarDist on H2B channel</p>
<p>-        cell label images were obtained using Cellpose on Tubulin and H2B channels</p>
<p>A quick visual inspection of the resulting label images concluded that they were satisfying enough, despite certainly not being perfect.</p>
<p>Notes :</p>
<p>-       This labelling strategy:</p>
<p>o   will not produce 100% accurate labels, but they might be more reproducible than labels generated by humans and are (definitely) much faster to obtain.</p>
<p>o   is NOT a recommended way of generating labels images, but for educational purposes.</p>
<p>-       The fluorescent channels are part of the dataset to ease the process of review of the labels and are NOT used for training. We generated the labels from the fluorescent channels to later predict labels from the BF or DPC channels only. As such, the fluorescent channels should not be “reused” with our labels during training.</p>
<p>File format: .tif (16-bit)</p>
<p>Image size: 540x540 (Pixel size: 0.299 nm)</p>
<p> </p>
<p>NOTE: This dataset uses the “HeLa “Kyoto” cells under the scope”  dataset (<a class="reference external" href="https://doi.org/10.5281/zenodo.6139958">https://doi.org/10.5281/zenodo.6139958</a>) to automatically generate annotations</p>
<p>NOTE: This dataset was used to train cellpose models in the following Zenodo entry <a class="reference external" href="https://doi.org/10.5281/zenodo.6140111">https://doi.org/10.5281/zenodo.6140111</a></p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/6140064">https://zenodo.org/records/6140064</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6140064">https://doi.org/10.5281/zenodo.6140064</a></p>
</section>
<hr class="docutils" />
<section id="bccd-dataset">
<h2>BCCD Dataset<a class="headerlink" href="#bccd-dataset" title="Link to this heading">#</a></h2>
<p>Shenggan Gan, Nicolas Chen</p>
<p>Published 2017-12-07</p>
<p>Licensed MIT</p>
<p>BCCD Dataset is a small-scale dataset for blood cells detection.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="github reference external" href="https://github.com/Shenggan/BCCD_Dataset">Shenggan/BCCD_Dataset</a></p>
</section>
<hr class="docutils" />
<section id="breast-cancer-nuclei-images-for-dl-training-zerocostdl4mic-stardist-model">
<h2>Breast Cancer Nuclei images for DL Training + ZeroCostDL4Mic StarDist Model<a class="headerlink" href="#breast-cancer-nuclei-images-for-dl-training-zerocostdl4mic-stardist-model" title="Link to this heading">#</a></h2>
<p>Ofra Golani, Vishnu Mohan, Tamar Geiger</p>
<p>Published 2024-05-21</p>
<p>Licensed CC-BY-4.0</p>
<p>Training dataset:Paired microscopy images (fluorescence) and corresponding masks
Microscopy data type: Fluorescence microscopy and masks obtained via manual correction of automatic segmentation with pre-trained StarDist model (see <a class="github reference external" href="https://github.com/qupath/models/tree/main/stardist">qupath/models</a>) 
Cells were imaged using a 20x objective with a 1x camera adapter was used in conjunction with a pco.edge 4.2 4MP camera on Pannoramic SCAN 150 scanner.
Cell type: FFPE tissue sections were sliced from all cancer-containing paraffin blocks
File format: .tif (8-bit for fluorescence and 16-bit for the masks)
 
StarDist Model:The StarDist model was generated using the ZeroCostDL4Mic platform (Chamier et al., 2021). This custom StarDist model was trained for 100 epochs using 80 manually annotated paired images (image dimensions: (257, 257)) with a batch size of 2, an augmentation factor of 10 and a mae loss function. The StarDist “Versatile fluorescent nuclei” model was used as a training starting point. Key python packages used include TensorFlow (v 2.2.0), Keras (v 1.1.2), CSBdeep (v 0.7.2), NumPy (v 1.21.6), Cuda (v 11..1.105). The training was accelerated using a Tesla P100GPU.The model weights can be used in the ZeroCostDL4Mic StarDist 2D notebook or in the StarDist Fiji plugin. a QuPath-compatible model is also provided.
 
 </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/11235393">https://zenodo.org/records/11235393</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11235393">https://doi.org/10.5281/zenodo.11235393</a></p>
</section>
<hr class="docutils" />
<section id="breast-cancer-semantic-segmentation-bcss-dataset">
<h2>Breast Cancer Semantic Segmentation (BCSS) dataset<a class="headerlink" href="#breast-cancer-semantic-segmentation-bcss-dataset" title="Link to this heading">#</a></h2>
<p>Mohamed Amgad, Habiba Elfandy, Hagar Hussein, Lamees A Atteya, Mai A T Elsebaie, Lamia S Abo Elnasr, Rokia A Sakr, Hazem S E Salem, Ahmed F Ismail, Anas M Saad, Joumana Ahmed, Maha A T Elsebaie, Mustafijur Rahman, Inas A Ruhban, Nada M Elgazar, Yahya Alagha, Mohamed H Osman, Ahmed M Alhusseiny, Mariam M Khalaf, Abo-Alela F Younes, Ali Abdulkarim, Duaa M Younes, Ahmed M Gadallah, Ahmad M Elkashash, Salma Y Fala, Basma M Zaki, Jonathan Beezley, Deepak R Chittajallu, David Manthey, David A Gutman, Lee A D Cooper</p>
<p>Published 2019-11-09</p>
<p>Licensed CC0-1.0</p>
<p>This repo contains the necessary information and download instructions to download the dataset associated with the paper: Amgad M, Elfandy H, …, Gutman DA, Cooper LAD. Structured crowdsourcing enables convolutional segmentation of histology images. Bioinformatics. 2019. doi: 10.1093/bioinformatics/btz083. This data can be visualized in a public instance of the Digital Slide Archive at this link. If you click the “eye” image icon in the Annotations panel on the right side of the screen, you will see the results of a collaborative annotation.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="github reference external" href="https://github.com/PathologyDataScience/BCSS">PathologyDataScience/BCSS</a></p>
</section>
<hr class="docutils" />
<section id="cellbindb-a-large-scale-multimodal-annotated-dataset">
<h2>CellBinDB: A Large-Scale Multimodal Annotated Dataset<a class="headerlink" href="#cellbindb-a-large-scale-multimodal-annotated-dataset" title="Link to this heading">#</a></h2>
<p>Can Shi, Jinghong Fan, Zhonghan Deng, Huanlin Liu, Qiang Kang, Yumei Li, Jing Guo, Jingwen Wang, Jinjiang Gong, Sha Liao, Ao Chen, Ying Zhang, Mei Li</p>
<p>Published 2024-11-20</p>
<p>Licensed CC-ZERO</p>
<p>CellBinDB is a large-scale, multimodal annotated dataset for cell segmentation. It contains 1,044 annotated microscope images and 109,083 cell annotations, covering four staining types: DAPI, ssDNA, H&amp;E, and mIF. CellBinDB contains samples from two species, human and mouse, covering more than 30 histologically different tissue types, including disease-related tissues. The images in CellBinDB come from two sources: 844 mouse images from internal experiments and 200 human images from the open access platform 10x Genomics. We annotated all images in CellBinDB and provide two types of image annotations: semantic and instance masks. A xlsx file is attached to record the detailed information of each image.
In addition, we provide the images and annotations of nine other widely used publicly available cell segmentation datasets downloaded from their original sources, retaining their original formats for ease of use. 
The file ‘mixed_licenses.txt’ contains the original accessions of the public datasets used in our project and their associated licenses. Please refer to these links for more information about each dataset and its licensing terms, and use it according to the specifications.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/15370205">https://zenodo.org/records/15370205</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15370205">https://doi.org/10.5281/zenodo.15370205</a></p>
</section>
<hr class="docutils" />
<section id="cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments">
<h2>Cellpose training data and scripts from “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”<a class="headerlink" href="#cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments" title="Link to this heading">#</a></h2>
<p>Martin Wohlwend, Olivier Burri, Johan Auwerx</p>
<p>Published 2024-02-27</p>
<p>Licensed CC-BY-4.0</p>
<p>This Workflow contains all the material necessary to reproduce the results of the QuPath analysis performed in the paper
 “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”
Inside this workflow and dataset, you will find the following folders</p>
<p>QuPath Training Project: A QuPath 0.3.2 project containing all the manual annotations (ground truths) used to train the cellpose model, as well as the script to start the training
QuPath Demo Project: A QuPath 0.3.2 project containing an example image that can be segmented using cellpose, followed by the classification of the CD45 expressing fibers
Training Images and Demo Images: The raw whole slide scanner 20x images needed by the above QuPath projects
Model: The fodler contianing the trained cellpose model
Cellpose Training Folder: The exported raw and ground truth images that the above cellpose model was trained on
Scripts: The QuPath scripts, also located in their respective QuPath projects, that were created for this whole workflow
QC: A Jupyter notebook, based on ZeroCostDL4Mic that computes quality metrics in order to assess the performance of the trained cellpose model. The folder also contains the resulting metrics.</p>
<p>Installation and Use
If you are going to use the QuPath projects, you need a local QuPath Installation <a class="reference external" href="https://qupath.github.io/">https://qupath.github.io/</a> that is configured to run the QuPath Cellpose Extension <a class="github reference external" href="https://github.com/BIOP/qupath-extension-cellpose">BIOP/qupath-extension-cellpose</a> as well as a working Cellpose installation <a class="github reference external" href="https://github.com/MouseLand/cellpose">MouseLand/cellpose</a>
Instructions for installation are available from the links above.
After that, you should be able to open the QuPath project, navigate to the “Automate &gt; Project scripts” menu and locate the script you wish to run.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7041137">https://zenodo.org/records/7041137</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7041137">https://doi.org/10.5281/zenodo.7041137</a></p>
</section>
<hr class="docutils" />
<section id="cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers">
<h2>Cellpose training data and scripts from “Machine learning for histological annotation and quantification of cortical layers”<a class="headerlink" href="#cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers" title="Link to this heading">#</a></h2>
<p>Jean Jacquemier, Julie Meystre, Olivier Burri</p>
<p>Published 2024-07-04</p>
<p>Licensed CC-BY-4.0</p>
<p>This Workflow contains all the material necessary to reproduce the cells detection, thanks to the QuPath performed in the paper
 “Machine learning for histological annotation and quantification of cortical layers”
Inside this workflow and dataset, you will find the following folders</p>
<p>QuPath Training Project: A QuPath 0.5.0 project containing all the manual annotations (ground truths) used to train the cellpose model, as well as the script to start the training
Training Images and Demo Images: The raw whole slide scanner images needed by the above QuPath project
Model: The fodler containing the trained cellpose model
cellpose-training Folder: The exported raw and ground truth images that the above cellpose model was trained on
Scripts: The QuPath scripts, also located in their respective QuPath projects, that were created for this whole workflow
QC: A Jupyter notebook, based on ZeroCostDL4Mic that computes quality metrics in order to assess the performance of the trained cellpose model. The folder also contains the resulting metrics.</p>
<p>Installation and Use
If you are going to use the QuPath projects, you need a local QuPath Installation <a class="reference external" href="https://qupath.github.io/">https://qupath.github.io/</a> that is configured to run the QuPath Cellpose Extension <a class="github reference external" href="https://github.com/BIOP/qupath-extension-cellpose">BIOP/qupath-extension-cellpose</a> as well as a working Cellpose installation <a class="github reference external" href="https://github.com/MouseLand/cellpose">MouseLand/cellpose</a>
Instructions for installation are available from the links above.
After that, you should be able to open the QuPath project, navigate to the “Automate &gt; Project scripts” menu and locate the script you wish to run.</p>
<ol class="arabic simple">
<li><p>train a cell segmentation algorithm in the context of the rat brain Layer Boundaries project </p></li>
<li><p>trigger cell segmentation from a QuPath project in a semi-automated pipeline</p></li>
</ol>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/12656468">https://zenodo.org/records/12656468</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12656468">https://doi.org/10.5281/zenodo.12656468</a></p>
</section>
<hr class="docutils" />
<section id="chinese-hamster-ovary-cells">
<h2>Chinese Hamster Ovary Cells<a class="headerlink" href="#chinese-hamster-ovary-cells" title="Link to this heading">#</a></h2>
<p>Krisztian Koos, József Molnár, Lóránd Kelemen, Gábor Tamás, Peter Horvath</p>
<p>Published 2016-07-29</p>
<p>Licensed CC-BY-3.0</p>
<p>The image set consists of 60 Differential Interference Contrast (DIC) images of Chinese Hamster Ovary (CHO) cells. The images are taken on an Olympus Cell-R microscope with a 20x lens at the time when the cell initiated their attachment to the bottom of the dish.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC030">https://bbbc.broadinstitute.org/BBBC030</a></p>
</section>
<hr class="docutils" />
<section id="combining-stardist-and-trackmate-example-1-breast-cancer-cell-dataset">
<h2>Combining StarDist and TrackMate example 1 -  Breast cancer cell dataset<a class="headerlink" href="#combining-stardist-and-trackmate-example-1-breast-cancer-cell-dataset" title="Link to this heading">#</a></h2>
<p>Guillaume Jacquemet</p>
<p>Published 2020-09-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Description: Contains a StarDist example training dataset, a test dataset, and the StarDist model generated using ZeroCostDL4Mic (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Training dataset: 72 Paired microscopy images (fluorescence) and corresponding masks</p>
<p>Microscopy data type: Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Microscope: Spinning disk confocal microscope with a 20x 0.8 NA objective</p>
<p>Cell type: <a class="reference external" href="http://DCIS.COM">DCIS.COM</a> Lifeact-RFP cells</p>
<p>File format: .tif (16-bit for fluorescence and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 634 nm)</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4034976">https://zenodo.org/records/4034976</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4034976">https://doi.org/10.5281/zenodo.4034976</a></p>
</section>
<hr class="docutils" />
<section id="combining-stardist-and-trackmate-example-2-t-cell-dataset">
<h2>Combining StarDist and TrackMate example 2 -  T cell dataset<a class="headerlink" href="#combining-stardist-and-trackmate-example-2-t-cell-dataset" title="Link to this heading">#</a></h2>
<p>Nathan H. Roy, Guillaume Jacquemet</p>
<p>Published 2020-09-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Description: Contains a StarDist example training dataset, a test dataset, and the StarDist model generated using ZeroCostDL4Mic (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Training dataset: 209 Paired microscopy images (brightfield) and corresponding masks</p>
<p>Microscopy data type: brightfield microscopy and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Microscope: Imaging was done using a 10x phase contrast objective at 37°C on a Zeiss Axiovert 200M microscope equipped with an automated X-Y stage and a Roper EMCCD camera. Time-lapse images were collected every 30 sec for 10 min using SlideBook 6 software (Intelligent Imaging Innovations).</p>
<p>File format: .tif (16-bit for brightfield images and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 645 nm)</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4034929">https://zenodo.org/records/4034929</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4034929">https://doi.org/10.5281/zenodo.4034929</a></p>
</section>
<hr class="docutils" />
<section id="combining-stardist-and-trackmate-example-3-flow-chamber-dataset">
<h2>Combining StarDist and TrackMate example 3 -  Flow chamber dataset<a class="headerlink" href="#combining-stardist-and-trackmate-example-3-flow-chamber-dataset" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Guillaume Jacquemet</p>
<p>Published 2020-09-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Description: Contains a StarDist example training dataset, a test dataset, and the StarDist model generated using ZeroCostDL4Mic (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Training dataset: Paired microscopy images (brightfield) and corresponding masks</p>
<p>Microscopy data type: brightfield microscopy and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Microscope: Images were acquired with a brightfield microscope (Zeiss Laser-TIRF 3 Imaging System, Carl Zeiss) and a 10X objective.</p>
<p>File format: .tif (8-bit for brightfield images and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 650 nm)</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4034939">https://zenodo.org/records/4034939</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4034939">https://doi.org/10.5281/zenodo.4034939</a></p>
</section>
<hr class="docutils" />
<section id="cryonuseg">
<h2>CryoNuSeg<a class="headerlink" href="#cryonuseg" title="Link to this heading">#</a></h2>
<p>Amirreza Mahbod, Benjamin Bancher, Isabella Ellinger, Deyun Zhang, Syed Nauyan Rashid</p>
<p>Published 2019-12-31</p>
<p>Licensed CC-BY-NC-SA-4.0</p>
<p>A Dataset for Nuclei Segmentation of Cryosectioned H&amp;E-Stained Histologic Images</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.kaggle.com/datasets/ipateam/segmentation-of-nuclei-in-cryosectioned-he-images">https://www.kaggle.com/datasets/ipateam/segmentation-of-nuclei-in-cryosectioned-he-images</a></p>
</section>
<hr class="docutils" />
<section id="deep-learning-segmentation-projects-of-fib-sem-dataset-of-u2-os-cell">
<h2>Deep learning segmentation projects of FIB-SEM dataset of U2-OS cell<a class="headerlink" href="#deep-learning-segmentation-projects-of-fib-sem-dataset-of-u2-os-cell" title="Link to this heading">#</a></h2>
<p>Belevich Ilya, Eija Jokitalo</p>
<p>Published 2023-10-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This submission includes ground truth datasets that were used to segment the nuclear envelope (NE), mitochondria, endoplasmic reticulum (ER) and Golgi from a human bone osteosarcoma epithelial cell (U2-OS) imaged using focused-ion beam scanning electron microscopy (FIB-SEM).The full FIB-SEM dataset is deposited to EMPIAR (<a class="reference external" href="https://www.ebi.ac.uk/empiar">https://www.ebi.ac.uk/empiar</a>, EMPIAR-11746). </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10043461">https://zenodo.org/records/10043461</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10043461">https://doi.org/10.5281/zenodo.10043461</a></p>
</section>
<hr class="docutils" />
<section id="deep-learning-training-data-jove">
<h2>Deep learning training data (JOVE)<a class="headerlink" href="#deep-learning-training-data-jove" title="Link to this heading">#</a></h2>
<p>Jessica Heebner, Carson Purnell, Ryan Hylton, Mike Marsh, Michael Grillo, Matt Swulius</p>
<p>Published 2022-11-18</p>
<p>Licensed CC-ZERO</p>
<p>Cryo-electron tomography (cryo-ET) allows researchers to image cells in their native, hydrated state at the highest resolution currently possible. However, the technique has several limitations that make analyzing the data it generates time-intensive and difficult. Hand-segmenting a single tomogram can take hours to days of human effort, but the microscope can easily generate 50 or more tomograms a day. Current deep learning segmentation programs for cryo-ET do exist but are limited to segmenting one structure at a time. Here multi-slice U-Net convolutional neural networks are trained and applied to automatically segment multiple structures simultaneously within cryo-tomograms. With proper preprocessing, these networks can be robustly inferred to many tomograms without the need for training individual networks for each tomogram. This workflow dramatically improves the speed with which cryo-electron tomograms can be analyzed by cutting segmentation time down to under 30 min in most cases. Further, segmentations can be used to improve the accuracy of filament tracing within a cellular context and to rapidly extract coordinates for subtomogram averaging.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7335439">https://zenodo.org/records/7335439</a></p>
<p><a class="reference external" href="https://doi.org/10.5061/dryad.rxwdbrvct">https://doi.org/10.5061/dryad.rxwdbrvct</a></p>
</section>
<hr class="docutils" />
<section id="deepbacs-bacillus-subtilis-fluorescence-segmentation-dataset">
<h2>DeepBacs – Bacillus subtilis fluorescence segmentation dataset<a class="headerlink" href="#deepbacs-bacillus-subtilis-fluorescence-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Séamus Holden, Mia Conduit</p>
<p>Published 2021-10-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Training and test images of live B. subtilis cells expressing FtsZ-GFP for the task of segmentation.</p>
<p>Additional information can be found on this github wiki.</p>
<p>The example shows the fluorescence widefield image of live B. subtilis cells expressing FtsZ-GFP and the manually annotated segmentation mask.</p>
<p> </p>
<p>Data type: Paired fluorescence and segmented mask images</p>
<p>Microscopy data type: 2D widefield images (fluorescence) </p>
<p>Microscope: Custom-built 100x inverted microscope bearing a 100x TIRF objective (Nikon CFI Apochromat TIRF 100XC Oil); images were captured on a Prime BSI sCMOS camera (Teledyne Photometrics)</p>
<p>Cell type: B. subtilis strain SH130 grown under agarose pads</p>
<p>File format: .tiff (8-bit) or .png (8-bit)</p>
<p>For segmented masks, binary masks are used for training of CARE/U-Net models, 8-bit .tif ROI maps for training of StarDist models and .png images for training of pix2pix models</p>
<p>Image size: 1024 x 1024 px² (Pixel size: 65 nm)</p>
<p>Image preprocessing: Images were denoised using PureDenoise and resulting 32-bit images were converted into 8-bit images after normalizing to 1% and 99.98% percentiles. Images were manually annotated using the Labkit Fiji plugin</p>
<p> </p>
<p>Author(s): Mia Conduit1,2, Séamus Holden1,3</p>
<p>Contact email: <a class="reference external" href="mailto:Seamus&#46;Holden&#37;&#52;&#48;newcastle&#46;ac&#46;uk">Seamus<span>&#46;</span>Holden<span>&#64;</span>newcastle<span>&#46;</span>ac<span>&#46;</span>uk</a></p>
<p> </p>
<p>Affiliation:</p>
<ol class="arabic simple">
<li><p>Centre for Bacterial Cell Biology, Biosciences Institute, Newcastle University, NE2 4AX UK</p></li>
<li><p>ORCID: 0000-0002-7169-907X</p></li>
</ol>
<p> </p>
<p> Associated publications: Whitley et al., 2021, Nature Communications, <a class="reference external" href="https://doi.org/10.15252/embj.201696235">https://doi.org/10.15252/embj.201696235</a></p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5550968">https://zenodo.org/records/5550968</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5550968">https://doi.org/10.5281/zenodo.5550968</a></p>
</section>
<hr class="docutils" />
<section id="deepbacs-escherichia-coli-bright-field-segmentation-dataset">
<h2>DeepBacs – Escherichia coli bright field segmentation dataset<a class="headerlink" href="#deepbacs-escherichia-coli-bright-field-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Christoph Spahn, Mike Heilemann</p>
<p>Published 2021-10-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Training and test images of live E. coli cells imaged under bright field for the task of segmentation.</p>
<p>Additional information can be found on this github wiki.</p>
<p>The example shows a bright field image of live E. coli cells and the manually annotated segmentation mask.</p>
<p> </p>
<p>Data type: Paired bright field and segmented mask images </p>
<p>Microscopy data type: 2D bright field images recorded at 1 min interval</p>
<p>Microscope: Nikon Eclipse Ti-E equipped with an Apo TIRF 1.49NA 100x oil immersion objective</p>
<p>Cell type: E. coli MG1655 wild type strain (CGSC #6300).</p>
<p>File format: .tif (8-bit)</p>
<p>Image size: 1024 x 1024 px² (79 nm / pixel), 19/15 individual frames (training/test dataset)</p>
<p>1024 x 1024 px² (79 nm / pixel), 9 regions of interest with 80 frames &#64; 1 min time interval (live-cell time series)</p>
<p>Image preprocessing: Raw images were recorded in 16-bit mode (image size 512 x 512 px² &#64; 158 nm/px). Images were upscaled with a factor of 2 (no interpolation) to enable generation of higher-quality segmentation masks. Two sets of mask images are provided: RoiMaps for instance segmentation using e.g. StarDist or binary images for CARE or U-Net.</p>
<p>Author(s): Christoph Spahn1,2, Mike Heilemann1,3</p>
<p>Contact email: <a class="reference external" href="mailto:christoph&#46;spahn&#37;&#52;&#48;mpi-marburg&#46;mpg&#46;de">christoph<span>&#46;</span>spahn<span>&#64;</span>mpi-marburg<span>&#46;</span>mpg<span>&#46;</span>de</a></p>
<p> </p>
<p>Affiliation(s): </p>
<ol class="arabic simple">
<li><p>Institute of Physical and Theoretical Chemistry, Max-von-Laue Str. 7, Goethe-University Frankfurt, 60439 Frankfurt, Germany</p></li>
<li><p>ORCID: 0000-0001-9886-2263 </p></li>
<li><p>ORCID: 0000-0002-9821-3578</p></li>
</ol>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5550935">https://zenodo.org/records/5550935</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5550935">https://doi.org/10.5281/zenodo.5550935</a></p>
</section>
<hr class="docutils" />
<section id="deepbacs-mixed-segmentation-dataset-and-stardist-model">
<h2>DeepBacs – Mixed segmentation dataset and StarDist model<a class="headerlink" href="#deepbacs-mixed-segmentation-dataset-and-stardist-model" title="Link to this heading">#</a></h2>
<p>Christoph Spahn, Mike Heilemann, Séamus Holden, Mia Conduit, Pereira, Pedro Matos, Mariana Pinho</p>
<p>Published 2021-10-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Mixed training and test images of S. aureus, E. coli and B. subtilis for cell segmentation using StarDist, as well as the trained StarDist model.</p>
<p>Additional information can be found on this github wiki.</p>
<p> </p>
<p>Data type: Paired bright field / fluorescence and segmented mask images</p>
<p>Microscopy data type: 2D widefield images; DIC and fluorescence for S. aureus, bright field images for E. coli, and fluorescence images for B. subtilis</p>
<p>Microscopes: </p>
<p>S. aureus: </p>
<p>GE HealthCare Deltavision OMX system (with temperature and humidity control, 37°C) equipped with an Olympus 60x 1.42NA Oil immersion objective and 2 PCO Edge 5.5 sCMOS cameras (one for DIC, one for fluorescence)</p>
<p>E.coli:</p>
<p>Nikon Eclipse Ti-E equipped with an Apo TIRF 1.49NA 100x oil immersion objective</p>
<p>B. subtilis:</p>
<p>Custom-built 100x inverted microscope bearing a 100x TIRF objective (Nikon CFI Apochromat TIRF 100XC Oil); images were captured on a Prime BSI sCMOS camera (Teledyne Photometrics)</p>
<p> </p>
<p>Cell types: S. aureus strain JE2, E. coli MG1655 (CGSC #6300) and B. subtilis strain SH130; all grown under agarose pads</p>
<p>File format: .tif (8-bit and 16-bit)</p>
<p>Image size: 512 x 512 px² &#64; 80 nm pixel size (S. aureus); 1024 x 1024 px² &#64; 79 nm pixel size (E. coli); 1024 x 1024 px² &#64; 65 nm pixel size (B. subtilis)</p>
<p>Image preprocessing: </p>
<p>S. aureus:</p>
<p>Raw images were manually annotated by drawing ellipses in the NR fluorescence image and segmented images were created using the LOCI plugin (“ROI Map”). For training, images and masks were quartered into four 256 x 256 px² patches.</p>
<p>E. coli:</p>
<p>Raw images were recorded in 16-bit mode (image size 512x512 px² &#64; 158 nm/px). Images were upscaled with a factor of 2 (no interpolation) to enable generation of higher-quality segmentation masks.</p>
<p>B. subtilis:</p>
<p>Images were denoised using PureDenoise and resulting 32-bit images were converted into 8-bit images after normalizing to 1% and 99.98% percentiles. Images were manually annotated using the Labkit Fiji plugin</p>
<p> </p>
<p>StarDist model:</p>
<p>The StarDist 2D model was generated using the ZeroCostDL4Mic platform (Chamier et al., 2021). It was trained from scratch for 200 epochs (120 steps/epoch) on 155 paired image patches (image dimensions: (1024, 1024), patch size: (256,256)) with a batch size of 4, 10% validation data, 64 rays on grid 2, a learning rate of 0.0003 and a mae loss function, using the StarDist 2D ZeroCostDL4Mic notebook (v 1.12.2). Key python packages used include tensorflow (v 0.1.12), Keras (v 2.3.1), csbdeep (v 0.6.1), numpy (v 1.19.5), cuda (v 11.0.221). The training was accelerated using a Tesla P100GPU. The dataset was augmented by a factor of 3.</p>
<p> </p>
<p>The model weights can be used in the ZeroCostDL4Mic StarDist 2D notebook, the StarDist Fiji plugin or the TrackMate Fiji plugin (v7+).</p>
<p> </p>
<p>Author(s): Christoph Spahn1,2, Mike Heilemann1,3, Mia Conduit4, Séamus Holden4,5, Pedro Matos Pereira6,7, Mariana Pinho6,8</p>
<p>Contact email: <a class="reference external" href="mailto:christoph&#46;spahn&#37;&#52;&#48;mpi-marburg&#46;mpg&#46;de">christoph<span>&#46;</span>spahn<span>&#64;</span>mpi-marburg<span>&#46;</span>mpg<span>&#46;</span>de</a>, <a class="reference external" href="mailto:Seamus&#46;Holden&#37;&#52;&#48;newcastle&#46;ac&#46;uk">Seamus<span>&#46;</span>Holden<span>&#64;</span>newcastle<span>&#46;</span>ac<span>&#46;</span>uk</a>, <a class="reference external" href="mailto:pmatos&#37;&#52;&#48;itqb&#46;unl&#46;pt">pmatos<span>&#64;</span>itqb<span>&#46;</span>unl<span>&#46;</span>pt</a> and <a class="reference external" href="mailto:mgpinho&#37;&#52;&#48;itqb&#46;unl&#46;pt">mgpinho<span>&#64;</span>itqb<span>&#46;</span>unl<span>&#46;</span>pt</a></p>
<p> </p>
<p>Affiliation(s): </p>
<ol class="arabic simple">
<li><p>Institute of Physical and Theoretical Chemistry, Max-von-Laue Str. 7, Goethe-University Frankfurt, 60439 Frankfurt, Germany</p></li>
<li><p>ORCID: 0000-0001-9886-2263 </p></li>
<li><p>ORCID: 0000-0002-9821-3578</p></li>
<li><p>Centre for Bacterial Cell Biology, Biosciences Institute, Newcastle University, NE2 4AX UK</p></li>
<li><p>ORCID: 0000-0002-7169-907X</p></li>
<li><p>Bacterial Cell Biology, Instituto de Tecnologia Química e Biológica António Xavier, Universidade Nova de Lisboa, Oeiras, Portugal</p></li>
<li><p>ORCID: 0000-0002-1426-9540</p></li>
<li><p>ORCID: 0000-0002-7132-8842</p></li>
</ol>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5551009">https://zenodo.org/records/5551009</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5551009">https://doi.org/10.5281/zenodo.5551009</a></p>
</section>
<hr class="docutils" />
<section id="deepbacs-staphylococcus-aureus-widefield-segmentation-dataset">
<h2>DeepBacs – Staphylococcus aureus widefield segmentation dataset<a class="headerlink" href="#deepbacs-staphylococcus-aureus-widefield-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Pereira, Pedro Matos, Mariana Pinho</p>
<p>Published 2021-10-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Training and test images of live S. aureus cells for the task of cell segmentation.</p>
<p>Additional information can be found in the github wiki.</p>
<p>The example shows the bright field and Nile Red fluorescence image of live S. aureus cells, as well as the manually annotated segmentation mask.</p>
<p> </p>
<p>Data type: Paired DIC/fluorescence and segmented mask images</p>
<p>Microscopy data type: 2D widefield images (DIC and fluorescence)</p>
<p>Microscope:  GE HealthCare Deltavision OMX system (with temperature and humidity control, 37°C) equipped with an Olympus 60x 1.42NA Oil immersion objective and 2 PCO Edge 5.5 sCMOS cameras (one for DIC, one for fluorescence)</p>
<p>Cell type: S. aureus strain JE2 grown under agarose pads</p>
<p>File format: .tif (16-bit)</p>
<p>Image size: 512 x 512 px² (80 nm/px)
Image preprocessing: Raw images were manually annotated by drawing ellipses in the NR fluorescence image and segmented images were created using the LOCI plugin (“ROI Map”). For training, images and masks were quartered into four 256 x 256 px² patches.</p>
<p> </p>
<p>Author(s): Pedro Matos Pereira1,2, Mariana Pinho1,3</p>
<p>Contact email: <a class="reference external" href="mailto:pmatos&#37;&#52;&#48;itqb&#46;unl&#46;pt">pmatos<span>&#64;</span>itqb<span>&#46;</span>unl<span>&#46;</span>pt</a> and <a class="reference external" href="mailto:mgpinho&#37;&#52;&#48;itqb&#46;unl&#46;pt">mgpinho<span>&#64;</span>itqb<span>&#46;</span>unl<span>&#46;</span>pt</a></p>
<p> </p>
<p>Affiliation: </p>
<ol class="arabic simple">
<li><p>Bacterial Cell Biology, Instituto de Tecnologia Química e Biológica António Xavier, Universidade Nova de Lisboa, Oeiras, Portugal</p></li>
<li><p>ORCID: <a class="reference external" href="https://orcid.org/0000-0002-1426-9540">https://orcid.org/0000-0002-1426-9540</a></p></li>
<li><p>ORCID: <a class="reference external" href="https://orcid.org/0000-0002-7132-8842">https://orcid.org/0000-0002-7132-8842</a></p></li>
</ol>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5550933">https://zenodo.org/records/5550933</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5550933">https://doi.org/10.5281/zenodo.5550933</a></p>
</section>
<hr class="docutils" />
<section id="drosophila-kc167-cells">
<h2>Drosophila Kc167 cells<a class="headerlink" href="#drosophila-kc167-cells" title="Link to this heading">#</a></h2>
<p>Vebjorn Ljosa, Katherine L. Sokolnicki, Anne E. Carpenter</p>
<p>Published 2012-06-28</p>
<p>Licensed CC0-1.0</p>
<p>Drosophila melanogaster Kc167 cells were stained for DNA (to label nuclei) and actin (a cytoskeletal protein, to show the cell body). Automatic cytometry requires that cells be segmented, i.e., that the pixels belonging to each cell be identified. Because segmenting nuclei and distinguishing foreground from background is comparatively easy for these images, the focus here is on finding the boundaries between adjacent cells.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC007">https://bbbc.broadinstitute.org/BBBC007</a></p>
</section>
<hr class="docutils" />
<section id="effect-of-local-topography-on-cell-division-of-staphylococci-sp">
<h2>Effect of local topography on cell division of Staphylococci sp.<a class="headerlink" href="#effect-of-local-topography-on-cell-division-of-staphylococci-sp" title="Link to this heading">#</a></h2>
<p>Sorzabal Bellido, Ioritz, Luca Barbieri, Beckett, Alison J., Prior, Ian A., Arturo Susarrey-Arce, Tiggelaar, Roald M., Jo Forthergill, Rasmita Raval, Diaz Fernandez, Yuri A.</p>
<p>Published 2021-05-16</p>
<p>Licensed CC-BY-4.0</p>
<p>Dataset.zip</p>
<p>This dataset includes the raw and annotated images used to train a Stardist 2D deep learning model for segmentation of surface attached S.aureus as described in Effect of local topography on cell division of Staphylococci sp.</p>
<p> </p>
<p>Stardist2d_Model.zip</p>
<p>Stardist 2D deep learning model for segmentation of surface attached S.aureus, obtained using the StarDist 2D ZeroCostDL4Mic notebook (v 1.12.3).</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4765599">https://zenodo.org/records/4765599</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4765599">https://doi.org/10.5281/zenodo.4765599</a></p>
</section>
<hr class="docutils" />
<section id="embryonic-mice-ultrasound-volumes-with-body-and-brain-volume-segmentation-masks">
<h2>Embryonic mice ultrasound volumes with body and brain volume segmentation masks<a class="headerlink" href="#embryonic-mice-ultrasound-volumes-with-body-and-brain-volume-segmentation-masks" title="Link to this heading">#</a></h2>
<p>Ziming Qiu, Matthew Hartley</p>
<p>Published 2023-05-10</p>
<p>Licensed CC0-1.0</p>
<p>Ultrasound images of mouse embryos with body and brain volume segmentation masks</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD686-ai.html">https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD686-ai.html</a></p>
</section>
<hr class="docutils" />
<section id="fiber-and-vessel-dataset-for-segmentation-and-characterization">
<h2>Fiber and vessel dataset for segmentation and characterization<a class="headerlink" href="#fiber-and-vessel-dataset-for-segmentation-and-characterization" title="Link to this heading">#</a></h2>
<p>Saqib Qamar, Baba, Abu Imran, Stèphane Verger, Magnus Andersson</p>
<p>Published 2024-05-03</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository hosts a comprehensive collection of datasets used to develop an innovative deep learning model designed to enhance the segmentation and characterization of macerated fibers and vessel forms in microscopy images. Included in the deposit are raw images, alongside meticulously prepared training and validation datasets. We present an automated segmentation approach that utilizes the one-stage YOLOv8 model, which has been specifically adapted to process high-resolution microscopy images up to 32640 x 25920 pixels. Our model excels in cell detection and segmentation, demonstrating exceptional proficiency.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10913446">https://zenodo.org/records/10913446</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10913446">https://doi.org/10.5281/zenodo.10913446</a></p>
</section>
<hr class="docutils" />
<section id="go-nuclear-a-deep-learning-based-toolkit-for-3d-nuclei-segmentation-and-quantitative-analysis-in-cellular-and-tissue-context">
<h2>Go-Nuclear. A deep learning-based toolkit for 3D nuclei segmentation and quantitative analysis in cellular and tissue context<a class="headerlink" href="#go-nuclear-a-deep-learning-based-toolkit-for-3d-nuclei-segmentation-and-quantitative-analysis-in-cellular-and-tissue-context" title="Link to this heading">#</a></h2>
<p>Kay Schneitz, Athul Vijayan, Tejasvinee Mody</p>
<p>Published 2024-06-29</p>
<p>Licensed CC0-1.0</p>
<p>We present computational tools that allow versatile and accurate 3D nuclear segmentation in plant organs, enable the analysis of cell-nucleus geometric relationships, and improve the accuracy of 3D cell segmentation. This biostudies submission includes Arabidopsis ovule model training dataset used in the study. The training dataset is composed of strong and weak nuclei image channels, corresponding ground truth segmentation, cell wall image and associated cell segmentation mentioned in the study. Trained models from the study, a total of 47 trained models are made available from this study. This included 15 initial models, 30 gold models, and 2 platinum models. Models were trained using PlantSeg, Stardist and Cellpose. All image datasets and its segmentation as part of the figures in this study is also available as separate zip files. This includes image dataset from different species and organs as listed below.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.ebi.ac.uk/bioimage-archive/galleries/ai/analysed-dataset/S-BIAD1026/">https://www.ebi.ac.uk/bioimage-archive/galleries/ai/analysed-dataset/S-BIAD1026/</a></p>
</section>
<hr class="docutils" />
<section id="ground-truth-cell-body-segmentation-used-for-starfinity-training">
<h2>Ground-truth cell body segmentation used for Starfinity training<a class="headerlink" href="#ground-truth-cell-body-segmentation-used-for-starfinity-training" title="Link to this heading">#</a></h2>
<p>Yuhan Wang, Martin Weigert, Uwe Schmidt, Stephan Saalfeld, Eugene W. Myers, Tim Wang, Karel Svoboda, Mark Eddison, Greg Fleishman, Shengjin Xu, Fredrick E. Henry, Andrew L. Lemire, Hui Yang, Konrad Rokicki, Cristian Goina, Eugene W Myers, Wyatt Korff, Scott M. Sternson, Paul W. Tillberg</p>
<p>Published 2021-03-05</p>
<p>Licensed CC-BY-4.0</p>
<p>Accurate segmentation of volumetric fluorescence image data has been a long-standing challenge and it can considerably degrade the accuracy of multiplexed fluorescence in situ hybridization (FISH) analysis. To overcome this challenge, we developed a deep learning-based automatic 3D segmentation algorithm, called Starfinity. It first predicts its cell center probability and its radial distances to the nearest cell borders for each pixel. It then aggregates pixel affinity maps from the densely predicted distances and applies a watershed segmentation on the affinity maps using the thresholded center probability as seeds.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://janelia.figshare.com/articles/dataset/Ground-truth_cell_body_segmentation_used_for_Starfinity_training/13624268">https://janelia.figshare.com/articles/dataset/Ground-truth_cell_body_segmentation_used_for_Starfinity_training/13624268</a></p>
</section>
<hr class="docutils" />
<section id="hpa-nucleus-segmentation-dpnunet">
<h2>HPA Nucleus Segmentation (DPNUnet)<a class="headerlink" href="#hpa-nucleus-segmentation-dpnunet" title="Link to this heading">#</a></h2>
<p>Hao Xu, Wei Ouyang</p>
<p>Published 2023-03-02</p>
<p>Licensed CC-BY-4.0</p>
<p>Download RDF Package</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7690494">https://zenodo.org/records/7690494</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7690494">https://doi.org/10.5281/zenodo.7690494</a></p>
</section>
<hr class="docutils" />
<section id="ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking">
<h2>HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking<a class="headerlink" href="#ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking" title="Link to this heading">#</a></h2>
<p>Estibaliz Gómez-de-Mariscal, Hasini Jayatilaka, Denis Wirtz, Arrate Muñoz-Barrutia</p>
<p>Published 2021-12-13</p>
<p>Licensed CC-BY-4.0</p>
<p>Human fibrosarcoma HT1080WT (ATCC) cells at low cell densities embedded in 3D collagen type I matrices [1]. The time-lapse videos were recorded every 2 minutes for 16.7 hours and covered a field of view of 1002 pixels × 1004 pixels with a pixel size of 0.802 μm/pixel The videos were pre-processed to correct frame-to-frame drift artifacts, resulting in a final size of 983 pixels × 985 pixels pixels.</p>
<p>Hasini Jayatilaka, Anjil Giri, Michelle Karl, Ivie Aifuwa, Nicholaus J Trenton, Jude M Phillip, Shyam Khatau, and Denis Wirtz. EB1 and cytoplasmic dynein mediate protrusion dynamics for efficient 3-dimensional cell migration. FASEB J., 32(3):1207–1221, 2018. ISSN 0892-6638. doi: 10.1096/fj.201700444RR.</p>
<p>Further information about how to use this data is given in <a class="github reference external" href="https://github.com/esgomezm/microscopy-dl-suite-tf">esgomezm/microscopy-dl-suite-tf</a></p>
<p>This dataset is provided together with the following preprint and if you use it, we would like to kindly ask you to cite it properly:</p>
<p>Estibaliz Gómez-de-Mariscal, Hasini Jayatilaka, Özgün Çiçek, Thomas Brox, Denis Wirtz, Arrate Muñoz-Barrutia, <em>Search for temporal cell segmentation robustness in phase-contrast microscopy videos</em>, arXiv 2021 (arXiv:2112.08817)</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5979761">https://zenodo.org/records/5979761</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5979761">https://doi.org/10.5281/zenodo.5979761</a></p>
</section>
<hr class="docutils" />
<section id="human-ht29-colon-cancer-cells">
<h2>Human HT29 colon-cancer cells<a class="headerlink" href="#human-ht29-colon-cancer-cells" title="Link to this heading">#</a></h2>
<p>Vebjorn Ljosa, Katherine L. Sokolnicki, Anne E. Carpenter</p>
<p>Published 2012-06-28</p>
<p>Licensed CC-BY-NC-SA-3.0</p>
<p>These images are of human HT29 colon cancer cells, a cell line that has been widely used for the study of many normal and neoplastic processes. A set of about 43,000 such images was used by Moffat et al. (Cell, 2006) to screen for mitotic regulators. The analysis followed the common pattern of identifying and counting cells with a phenotype of interest (in this case, cells that were in mitosis), then normalizing the count by dividing by the total number of cells. Such experiments present two image analysis problems. First, identifying the cells that have the phenotype of interest requires that the nuclei and cells be segmented. Second, normalizing requires an accurate cell count.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC008">https://bbbc.broadinstitute.org/BBBC008</a></p>
</section>
<hr class="docutils" />
<section id="human-hepatocyte-and-murine-fibroblast-cells-co-culture-experiment">
<h2>Human Hepatocyte and Murine Fibroblast cells Co-culture experiment<a class="headerlink" href="#human-hepatocyte-and-murine-fibroblast-cells-co-culture-experiment" title="Link to this heading">#</a></h2>
<p>David J. Logan, Jing Shan, Sangeeta N. Bhatia, Anne E. Carpenter</p>
<p>Published 2016-03-01</p>
<p>Licensed CC-BY-3.0</p>
<p>This 384-well plate has images of co-cultured hepatocytes and fibroblasts. Every other well is populated (A01, A03, …, C01, C03, …) such that 96 wells comprise the data. Each well has 9 sites and thus 9 images associated, totaling 864 images.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC026">https://bbbc.broadinstitute.org/BBBC026</a></p>
</section>
<hr class="docutils" />
<section id="human-lung-tissue-microscopy-dic-fluorescence-cell-and-nuclei-semantic-instance-annotations">
<h2>Human Lung Tissue Microscopy (DIC, Fluorescence, Cell and Nuclei Semantic Instance Annotations)<a class="headerlink" href="#human-lung-tissue-microscopy-dic-fluorescence-cell-and-nuclei-semantic-instance-annotations" title="Link to this heading">#</a></h2>
<p>Melanie Dohmen, Mirja Mittermaier, Andreas Hocke</p>
<p>Published 2024-02-22</p>
<p>The zip file contains 3 folders (annotations, images and training_splits).The annotation folder contains 3 folders (cell_instances, nuclei_instances and semantic). Cell and nuclei instance annotations are long int tif images, containing numbered instance ids and 0 in the background. Semantic annotations are 8-bit int png files containing the class ids (0: background, 1: normal tissue, 2: erythrocytes, 3: alveolar epithelial type 2 cells, 4: alveolar macrophages, 5: other nuclei, 6: alveolar epithelial type 2 cell nuclei, 7: alveolar macrophage nuclei, 8: cell debris).
The image folder contains 4 folders (CD68, DAPI, DIC, proSPC), where DIC contains float valued background-corrected differential interference contrast images, the others contain normalized float-valued fluorescence channels of a multi-plex staining with CD-68 (whole alveolar macrophages), DAPI (any cell nuclei), proSPC (cytoplasm of alveolar epithelial type 2 cell). All images are in tif format.
The training split folder contains 3 text files, with the image prefix (compared to images and annotations without ending, i.e. e.g. without “_DIC.tif”) of all cases in the respective subset. With a total of 68 cases, there are 51 cases in the train set, 7 cases in the validation set and 10 cases in the test set.The lung tissue origins from lung surgery of patients, but does not include resected tumors. Please see reference [1]. The images were acquired with a laser scanning microscope with 40x magnification and 1024 x 1024 pixels per image.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10669918">https://zenodo.org/records/10669918</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10669918">https://doi.org/10.5281/zenodo.10669918</a></p>
</section>
<hr class="docutils" />
<section id="human-u2os-cells-out-of-focus">
<h2>Human U2OS cells (out of focus)<a class="headerlink" href="#human-u2os-cells-out-of-focus" title="Link to this heading">#</a></h2>
<p>Vebjorn Ljosa, Katherine L. Sokolnicki, Anne E. Carpenter</p>
<p>Published 2012-06-28</p>
<p>Licensed CC0-1.0</p>
<p>Since robust foreground/background separation and segmentation of cellular objects (i.e., identification of which pixels below to which objects) strongly depends on image quality, focus artifacts are detrimental to data quality. This image set provides examples of in- and out-of-focus HCS images which can be used for validation of focus metrics.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC006">https://bbbc.broadinstitute.org/BBBC006</a></p>
</section>
<hr class="docutils" />
<section id="lmrg-image-analysis-study-fish-datasets">
<h2>LMRG Image Analysis Study - FISH datasets<a class="headerlink" href="#lmrg-image-analysis-study-fish-datasets" title="Link to this heading">#</a></h2>
<p>Kristopoher Kubow, Thomas Pengo</p>
<p>Published 2022-05-18</p>
<p>Licensed CC-BY-4.0</p>
<p>Original image files, label (ground truth) files, and PSF files used in the ABRF Light Microscopy Research Group (LMRG) image analysis study. Simulated 3D confocal fluorescence images of sub-diffraction punctate staining (fluorescence in situ hybridization (FISH) in C. elegans).</p>
<p>See <a class="github reference external" href="https://github.com/ABRFLMRG/image-analysis-study">ABRFLMRG/image-analysis-study</a> for more details.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/6560910">https://zenodo.org/records/6560910</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6560910">https://doi.org/10.5281/zenodo.6560910</a></p>
</section>
<hr class="docutils" />
<section id="lmrg-image-analysis-study-nuclei-datasets">
<h2>LMRG Image Analysis Study - nuclei datasets<a class="headerlink" href="#lmrg-image-analysis-study-nuclei-datasets" title="Link to this heading">#</a></h2>
<p>Kristopher Kubow, Thomas Pengo</p>
<p>Published 2022-05-18</p>
<p>Licensed CC-BY-4.0</p>
<p>Original image files, label (ground truth) files, and PSF files used in the ABRF Light Microscopy Research Group (LMRG) image analysis study. Simulated 3D widefield fluorescence images of nuclei.</p>
<p>See <a class="github reference external" href="https://github.com/ABRFLMRG/image-analysis-study">ABRFLMRG/image-analysis-study</a> for more details.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/6560759">https://zenodo.org/records/6560759</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.6560759">https://doi.org/10.5281/zenodo.6560759</a></p>
</section>
<hr class="docutils" />
<section id="lynsec-lymphoma-nuclear-segmentation-and-classification">
<h2>LyNSeC: Lymphoma Nuclear Segmentation and Classification<a class="headerlink" href="#lynsec-lymphoma-nuclear-segmentation-and-classification" title="Link to this heading">#</a></h2>
<p>Naji Hussein, Büttner Reinhard, Simon Adrian, Eich Marie-Lisa, Lohneis Philipp, Bozek Katarzyna</p>
<p>Published 2023-06-21</p>
<p>Licensed CC-BY-4.0</p>
<p>Over the last years, there has been large progress in automated segmentation and classification methods in histological whole slide images (WSIs) stained with hematoxylin and eosin (H&amp;E). Current state-of-the-art techniques are based on diverse datasets of H&amp;E-stained WSIs of different types of predominantly solid cancer. However, there is a lack of publicly available annotated datasets of lymphoma, which is why we generated a labeled diffuse large B-cell lymphoma dataset and denoted it LyNSeC (lymphoma nuclear segmentation and classification). LyNSeC comprises three subsets: LyNSeC 1 consists of 379 IHC images of size 512 x 512 pixels at 40x magnification. In the images, we annotated the contours of each cell nuclei and the cell class: marker-positive or marker-negative.</p>
<p>In total, LyNSeC 1 contains 87,316 annotated cell nuclei of four different cases, with 48,171 of them assigned the class negative and 39,145 positive. We included three markers in this dataset showing visually different staining patterns: cluster of differentiation 3 (CD3), Ki67 as a marker of proliferation, and erythroblast transformation-specific (EST)-related gene (ERG).</p>
<p>LyNSeC 2 and 3 contain H&amp;E-stained images of 70 different patients. LyNSeC 2 consists of 280 images and LyNSeC 3 of 40 images of size 512 x 512 pixels at 40x magnification. 65,479 and 8,452 nuclei were annotated in LyNSeC 2 and 3, respectively. In LyNSeC 3, the nuclei were also assigned a class label (tumor and non-tumor). 3,747 nuclei were identified as tumors and 4,705 as non-tumors.</p>
<p>In the annotation procedure, the contours of the H&amp;E images (LyNSeC 2 and LyNSeC 3) were annotated by two pathologists and by two students (trained by the pathologists). Annotation of the cell classes in LyNSeC 3 was done by the pathologists only. LyNSeC 1 was annotated by the two students who were additionally trained to annotate the contours and to distinguish marker-positive and marker-negative cells. The pathologists inspected and (if necessary) adjusted the LyNSeC 3 annotations.</p>
<p>The files are uploaded in ‘.npy’ format. The files of LyNSeC 1 (x_l1.npy) and LyNSeC 3 (x_l3.npy) contain five channels, respectively: the first three are the RGB channels of the images, channel 4 contains the instance maps, and channel 5 the class type maps (for LyNSeC 1 a pixel value of 1 corresponds to the class negative and 2 to the class positive, whereas in LyNSeC 3 1 corresponds to the class non-tumor and 2 to the class tumor). The files of LyNSeC 2 (x_l2.npy) have 4 channels (without the class type map).</p>
<p>Additionally, we also make our HoVer-Net-based pre-trained nuclei segmentation and classification models available (he.tar for H&amp;E images and ihc.tar for IHC images).</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8065174">https://zenodo.org/records/8065174</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8065174">https://doi.org/10.5281/zenodo.8065174</a></p>
</section>
<hr class="docutils" />
<section id="midog-2021">
<h2>MIDOG 2021<a class="headerlink" href="#midog-2021" title="Link to this heading">#</a></h2>
<p>Marc Aubreville, Frauke Wilm</p>
<p>Published 2021-03-16</p>
<p>Licensed UNLICENSED</p>
<p>Mitosis domain generation. Here you can find code of our own evaluations and a dockered reference algorithm for mitotic figures to use as a template.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="github reference external" href="https://github.com/DeepMicroscopy/MIDOG">DeepMicroscopy/MIDOG</a></p>
</section>
<hr class="docutils" />
<section id="melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations">
<h2>Melanoma Histopathology Dataset with Tissue and Nuclei Annotations<a class="headerlink" href="#melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations" title="Link to this heading">#</a></h2>
<p>Mark Schuiveling</p>
<p>Published 2025-03-19</p>
<p>Licensed CC-ZERO</p>
<p>Description:
This dataset is designed for development of deep learning models for segmentation of nuclei and tissue in melanoma H&amp;E stained histopathology. Existing nuclei segmentation models that are trained on non-melanoma specific datasets have low performance due to the ability of melanocytes to mimic other cell types, whereas existing melanoma specific models utilize older, sub-optimal techniques. Moreover, these models do not provide tissue annotations necessary for determining the localization of tumor-infiltrating lymphocytes, which may hold value for predictive and prognostic tasks. To address this, we created a melanoma specific dataset with nuclei and tissue annotations. 
Methodology:
Sample Collection:
Regions of interest (ROIs) were sampled from H&amp;E stained slides of 103 primary melanoma specimens and 102 metastatic melanoma specimens, scanned using a Hamamatsu scanner at 40× magnification (0.23 μm per pixel). All slides were obtained from regular diagnostic procedures.From each specimen, a 40× magnified ROI of 1024×1024 pixels was selected for annotation. Additionally, a context ROI of 5120×5120 pixels was sampled to provide information about the broader context for the annotation process. Selection was performed by a trained medical expert (M.S.) and subsequently verified by a dermatopathologist (W.B.). Manual ROI selection ensured the inclusion of diverse tissue and nuclei types.
Annotation Process:</p>
<p>Nuclei segmentationNuclei segmentations were generated using Hover-Net pretrained on the PanNuke dataset. Manual annotation adjustments were performed by author M.S. using QuPath, with the following nuclei categories: tumor, stroma, vascular endothelium, histiocyte, melanophage, lymphocyte, plasma cell, neutrophil, apoptotic cell, and epithelium. All annotations were reviewed and corrected, where needed, by a dermatopathologist (W.B.).
Tissue segmentationTissue segmentations were created manually using QuPath by M.S., with the following categories: tumor, stroma, epidermis, necrosis, blood vessel, and background. Annotations were reviewed and corrected, where needed, by a dermatopathologist (W.B.).</p>
<p>Quality Control: To assess the reliability of the annotations, intra- and interobserver agreement (by pathologist G.B.) were determined on 12 randomly selected ROIs.</p>
<p>Nuclei segmentationThe intraobserver overall precision was 84.89%, with a recall of 86.45%, and an F1 score of 85.66%. Interobserver overall precision was 80.34%, with a recall of 80.62%, and an F1 score of 80.20%. These results are based on the sum of all true positive, false positive, and false negative counts for the 12 ROIs.
Tissue segmentationThe DICE score was determined on the same 12 randomly selected ROIs. The average intraobserver DICE score was 0.90, and the interobserver DICE score was also 0.90.</p>
<p> 
Version 3:Removed sample “training_set_metastatic_roi_103” due to inconsistencies in annotation file.
Version 4:Sample training_set_metastatic_roi_088 missed one color annotation for a nuclei_apoptosis in the geojson file rendering it qupath uncompatible. This is fixed in the new version. 
Version 5:Addition of correct sample of training_set_metastatic_roi_103” after deadline of panoptic segmentation of nuclei and tissue in advanced melanoma challenge test phase. </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/15050523">https://zenodo.org/records/15050523</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15050523">https://doi.org/10.5281/zenodo.15050523</a></p>
</section>
<hr class="docutils" />
<section id="membrain-seg-training-data">
<h2>MemBrain-seg training data<a class="headerlink" href="#membrain-seg-training-data" title="Link to this heading">#</a></h2>
<p>Lorenz Lamm</p>
<p>Published 2023-03-16</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset contains training data for segmenting membranes in cryo-electron tomograms.</p>
<p>More details will follow.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7739793">https://zenodo.org/records/7739793</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7739793">https://doi.org/10.5281/zenodo.7739793</a></p>
</section>
<hr class="docutils" />
<section id="monuseg-dataset">
<h2>MoNuSeg Dataset<a class="headerlink" href="#monuseg-dataset" title="Link to this heading">#</a></h2>
<p>Neeraj Kumar, Ruchika Verma, Sanuj Sharma, Surabhi Bhargava, Abhishek Vahadane, Amit Sethi</p>
<p>Published 2017-07-01</p>
<p>Licensed CC-BY-NC-SA-4.0</p>
<p>The dataset for this challenge was obtained by carefully annotating tissue images of several patients with tumors of different organs and who were diagnosed at multiple hospitals. This dataset was created by downloading H&amp;E stained tissue images captured at 40x magnification from TCGA archive. H&amp;E staining is a routine protocol to enhance the contrast of a tissue section and is commonly used for tumor assessment (grading, staging, etc.). Given the diversity of nuclei appearances across multiple organs and patients, and the richness of staining protocols adopted at multiple hospitals, the training datatset will enable the development of robust and generalizable nuclei segmentation techniques that will work right out of the box.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://monuseg.grand-challenge.org/Data/">https://monuseg.grand-challenge.org/Data/</a></p>
</section>
<hr class="docutils" />
<section id="monusac-2020">
<h2>MonuSAC 2020<a class="headerlink" href="#monusac-2020" title="Link to this heading">#</a></h2>
<p>Ruchika Verma, Neeraj Kumar, Abhijeet Patil, Nikhil Cherian Kurian, Swapnil Rane, Simon Graham</p>
<p>Published 2021-06-04</p>
<p>Licensed CC-BY-NC-SA-4.0</p>
<p>H&amp;E staining of human tissue sections is a routine and most common protocol used by pathologists to enhance the contrast of tissue sections for tumor assessment (grading, staging, etc.) at multiple microscopic resolutions. Hence, we will provide the annotated dataset of H&amp;E stained digitized tissue images of several patients acquired at multiple hospitals using one of the most common 40x scanner magnification. The annotations will be done with the help of expert pathologists.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://monusac-2020.grand-challenge.org/Data/">https://monusac-2020.grand-challenge.org/Data/</a></p>
</section>
<hr class="docutils" />
<section id="mouse-embryo-blastocyst-cells">
<h2>Mouse embryo blastocyst cells<a class="headerlink" href="#mouse-embryo-blastocyst-cells" title="Link to this heading">#</a></h2>
<p>Vebjorn Ljosa, Katherine L. Sokolnicki, Anne E. Carpenter</p>
<p>Published 2012-06-28</p>
<p>Licensed CC0-1.0</p>
<p>Segmenting nuclei in 3D images can be challenging especially when nuclei are clustered not only in XY plane but also in XZ and YZ planes. Manually annotated ground truth provides a reference for image analysis software testing purposes. These images of mouse embryo blastocyst cells also have changing nuclei intensity in Z plane which makes finding the right threshold for successful segmentation a difficult task. This image set also contains GAPDH transcripts that can be quantified in each cell.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC032">https://bbbc.broadinstitute.org/BBBC032</a></p>
</section>
<hr class="docutils" />
<section id="neurips-2022-cell-segmentation-competition-dataset">
<h2>NeurIPS 2022 Cell Segmentation Competition Dataset<a class="headerlink" href="#neurips-2022-cell-segmentation-competition-dataset" title="Link to this heading">#</a></h2>
<p>Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, Wei Lou, Haofeng Li, Eric Upschulte, Timo Dickscheid, de Almeida, José Guilherme, Yixin Wang, Lin Han, Xin Yang, Marco Labagnara, Vojislav Gligorovski, Maxime Scheder, Rahi, Sahand Jamal, Carly Kempster, Alice Pollitt, Leon Espinosa, Tam Mignot, Middeke, Jan Moritz, Jan-Niklas Eckardt, Wangkai Li, Zhaoyang Li, Xiaochen Cai, Bizhe Bai, Greenwald, Noah F., Van Valen, David, Erin Weisbart, Cimini, Beth A, Trevor Cheung, Oscar Brück, Bader, Gary D., Bo Wang</p>
<p>Published 2024-02-27</p>
<p>Licensed CC-BY-NC-ND-4.0</p>
<p>The official data set for the NeurIPS 2022 competition: cell segmentation in multi-modality microscopy images.
<a class="reference external" href="https://neurips22-cellseg.grand-challenge.org/">https://neurips22-cellseg.grand-challenge.org/</a>
Please cite the following paper if this dataset is used in your research. 
 
&#64;article{NeurIPS-CellSeg,
title = {The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions},
author = {Jun Ma and Ronald Xie and Shamini Ayyadhury and Cheng Ge and Anubha Gupta and Ritu Gupta and Song Gu and Yao Zhang and Gihun Lee and Joonkee Kim and Wei Lou and Haofeng Li and Eric Upschulte and Timo Dickscheid and José Guilherme de Almeida and Yixin Wang and Lin Han and Xin Yang and Marco Labagnara and Vojislav Gligorovski and Maxime Scheder and Sahand Jamal Rahi and Carly Kempster and Alice Pollitt and Leon Espinosa and Tâm Mignot and Jan Moritz Middeke and Jan-Niklas Eckardt and Wangkai Li and Zhaoyang Li and Xiaochen Cai and Bizhe Bai and Noah F. Greenwald and David Van Valen and Erin Weisbart and Beth A. Cimini and Trevor Cheung and Oscar Brück and Gary D. Bader and Bo Wang},
journal = {Nature Methods},      volume={21},      pages={1103–1113},      year = {2024},
doi = {<a class="reference external" href="https://doi.org/10.1038/s41592-024-02233-6">https://doi.org/10.1038/s41592-024-02233-6</a>}
}
 
This is an instance segmentation task where each cell has an individual label under the same category (cells). The training set contains both labeled images and unlabeled images. You can only use the labeled images to develop your model but we encourage participants to try to explore the unlabeled images through weakly supervised learning, semi-supervised learning, and self-supervised learning.
 
The images are provided with original formats, including tiff, tif, png, jpg, bmp… The original formats contain the most amount of information for competitors and you have free choice over different normalization methods. For the ground truth, we standardize them as tiff formats.
 
We aim to maintain this challenge as a sustainable benchmark platform. If you find the top algorithms (<a class="reference external" href="https://neurips22-cellseg.grand-challenge.org/awards/">https://neurips22-cellseg.grand-challenge.org/awards/</a>) don’t perform well on your images, welcome to send us the dataset (<a class="reference external" href="mailto:neurips&#46;cellseg&#37;&#52;&#48;gmail&#46;com">neurips<span>&#46;</span>cellseg<span>&#64;</span>gmail<span>&#46;</span>com</a>)! We will include them in the new testing set and credit your contributions on the challenge website!
 
Dataset License: CC-BY-NC-ND</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10719375">https://zenodo.org/records/10719375</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10719375">https://doi.org/10.5281/zenodo.10719375</a></p>
</section>
<hr class="docutils" />
<section id="nuinsseg">
<h2>NuInsSeg<a class="headerlink" href="#nuinsseg" title="Link to this heading">#</a></h2>
<p>Amirreza Mahbod, Christine Polak, Katharina Feldmann, Rumsha Khan, Katharina Gelles, Georg Dorffner, Ramona Woitek, Sepideh Hatamikia, Isabella Ellinger</p>
<p>Published 2024-05-14</p>
<p>Licensed CC-BY-4.0</p>
<p>A Fully Annotated Dataset for Nuclei Instance Segmentation in H&amp;E-Stained Images</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.kaggle.com/datasets/ipateam/nuinsseg">https://www.kaggle.com/datasets/ipateam/nuinsseg</a></p>
</section>
<hr class="docutils" />
<section id="nuclei-of-u2os-cells-in-a-chemical-screen">
<h2>Nuclei of U2OS cells in a chemical screen<a class="headerlink" href="#nuclei-of-u2os-cells-in-a-chemical-screen" title="Link to this heading">#</a></h2>
<p>Vebjorn Ljosa, Katherine L. Sokolnicki, Anne E. Carpenter</p>
<p>Published 2012-06-28</p>
<p>Licensed CC0-1.0</p>
<p>This image set is part of a high-throughput chemical screen on U2OS cells, with examples of 200 bioactive compounds. The effect of the treatments was originally imaged using the Cell Painting assay (fluorescence microscopy). This data set only includes the DNA channel of a single field of view per compound. These images present a variety of nuclear phenotypes, representative of high-throughput chemical perturbations. The main use of this data set is the study of segmentation algorithms that can separate individual nucleus instances in an accurate way, regardless of their shape and cell density. The collection has around 23,000 single nuclei manually annotated to establish a ground truth collection for segmentation evaluation.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC039">https://bbbc.broadinstitute.org/BBBC039</a></p>
</section>
<hr class="docutils" />
<section id="nuclei-of-mouse-embryonic-cells">
<h2>Nuclei of mouse embryonic cells<a class="headerlink" href="#nuclei-of-mouse-embryonic-cells" title="Link to this heading">#</a></h2>
<p>Vebjorn Ljosa, Katherine L. Sokolnicki, Anne E. Carpenter</p>
<p>Published 2012-06-28</p>
<p>Licensed CC0-1.0</p>
<p>Cell dynamics during the early mouse embryogenesis change spatiotemporally. For understanding the mechanism of this developmental process, imaging cell dynamics by live-cell imaging of fluorescently labeled nuclei and performing nuclei segmentation of these images by image processing are essential. This dataset contains the fluorescence images and Ground Truth used when performing nuclei segmentation using deep learning. Fluorescence images are time-series images from fertilization to blastocyst formation. Ground Truth is supervised data of the cell nuclear region.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC050">https://bbbc.broadinstitute.org/BBBC050</a></p>
</section>
<hr class="docutils" />
<section id="ocelot-overlapped-cell-on-tissue-dataset-for-histopathology">
<h2>OCELOT: Overlapped Cell on Tissue Dataset for Histopathology<a class="headerlink" href="#ocelot-overlapped-cell-on-tissue-dataset-for-histopathology" title="Link to this heading">#</a></h2>
<p>Jeongun Ryu, Aaron Valero Puche, JaeWoong Shin, Seonwook Park, Biagio Brattoli, Mohammad Mostafavi, Jinhee Lee, Sérgio Pereira, Wonkyung Jung, Soo Ick Cho, Chan-Young Ock, Kyunghyun Paeng, Donggeun Yoo</p>
<p>Published 2023-03-23</p>
<p>The OCELOT dataset is a histopathology dataset designed to facilitate the development of methods that utilize cell and tissue relationships. The dataset comprises both small and large field-of-view (FoV) patches extracted from digitally scanned whole slide images (WSIs), with overlapping regions. The small and large FoV patches are accompanied by annotations of cells and tissues, respectively. The WSIs are sourced from the publicly available TCGA database and were stained using the H&amp;E method before being scanned with an Aperio scanner.</p>
<p>For more details, please check <a class="reference external" href="https://lunit-io.github.io/research/ocelot_dataset/">https://lunit-io.github.io/research/ocelot_dataset/</a>.</p>
<p> </p>
<p>Before downloading the dataset, please make sure to carefully read and agree to the Terms and Conditions at (<a class="reference external" href="https://lunit-io.github.io/research/ocelot_tc/">https://lunit-io.github.io/research/ocelot_tc/</a>).</p>
<p>Also, please provide 1. name, 2. e-mail address, 3. organization/company name.</p>
<p> </p>
<hr class="docutils" />
<p>Release note.</p>
<p>In version 1.0.1, we exclude four test cases (586, 589, 609, 615) due to under-annotated issue.
In version 1.0.0, we include images and annotations of validation and test splits.
In version 0.1.2, we modified the coordinates of cell labels to range from 0 to 1023 (-1 from the previous coordinates).
In version 0.1.1, we removed non-H&amp;E stained patches from the dataset.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8417503">https://zenodo.org/records/8417503</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8417503">https://doi.org/10.5281/zenodo.8417503</a></p>
</section>
<hr class="docutils" />
<section id="parhyale-3d-segmentation-dataset">
<h2>Parhyale 3D segmentation dataset<a class="headerlink" href="#parhyale-3d-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Frederike Alwes, Ko Sugawara, Michalis Averof</p>
<p>Published 2023-08-11</p>
<p>Licensed CC-BY-4.0</p>
<p>The Parhyale 3D Segmentation dataset consists of 50 timepoints (TP01-TP50) of 3D images (512x512x34), where the manual annotations can be found at discrete 6 timepoints (at TP01, TP11, TP21, TP31, TP41 and TP50).</p>
<p>For further details, see README file.</p>
<p>This version fixes the duplicated label IDs found in the previous version of label files. This version ensures that each instance has a unique ID. Thanks to Jackson Borchardt for reporting that error.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8252039">https://zenodo.org/records/8252039</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8252039">https://doi.org/10.5281/zenodo.8252039</a></p>
</section>
<hr class="docutils" />
<section id="platynereis-em-training-data">
<h2>Platynereis EM training data<a class="headerlink" href="#platynereis-em-training-data" title="Link to this heading">#</a></h2>
<p>Constantin Pape</p>
<p>Published 2020-02-19</p>
<p>Licensed CC-BY-4.0</p>
<p>Training data for Convolutional Neural Networks used in the publication Whole-body integration of gene expression and single-cell morphology. We provide training data for segmenting structures in the SerialBlockface Electron Microscopy data-set containing a complete 6 day old Platynereis dumerilii larva, in particular for:</p>
<ul class="simple">
<li><p>cell membranes: 9 training blocks &#64; resolution 20x20x25 nm. Based on initial training data provided by <a class="reference external" href="https://ariadne.ai/">https://ariadne.ai/</a>.</p></li>
<li><p>cilia: 3 training and 2 validation blocks &#64; resolution 20x20x25 nm.</p></li>
<li><p>cuticle: 5 training blocks &#64; resolution 40x40x50 nm.</p></li>
<li><p>nuclei: 12 training blocks &#64; resolution 80x80x100 nm. Based on initial training data provided by <a class="reference external" href="https://ariadne.ai/">https://ariadne.ai/</a>.</p></li>
</ul>
<p>For details on how to use this data for training, see <a class="github reference external" href="https://github.com/platybrowser/platybrowser-backend/tree/master/segmentation">platybrowser/platybrowser-backend</a>.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/3675220">https://zenodo.org/records/3675220</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.3675220">https://doi.org/10.5281/zenodo.3675220</a></p>
</section>
<hr class="docutils" />
<section id="predicting-axillary-lymph-node-metastasis-in-early-breast-cancer-using-deep-learning-on-primary-tumor-biopsy-slides">
<h2>Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides<a class="headerlink" href="#predicting-axillary-lymph-node-metastasis-in-early-breast-cancer-using-deep-learning-on-primary-tumor-biopsy-slides" title="Link to this heading">#</a></h2>
<p>Wenqi Tang, MIC Group</p>
<p>Published 2021-12-12</p>
<p>Licensed UNLICENSED</p>
<p>This repo is the official implementation of our paper “Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides”.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="github reference external" href="https://github.com/bupt-ai-cz/BALNMP">bupt-ai-cz/BALNMP</a></p>
</section>
<hr class="docutils" />
<section id="prodgerlab-stardist-hiv-target-cell-training-set">
<h2>ProdgerLab-StarDist-HIV Target Cell Training Set<a class="headerlink" href="#prodgerlab-stardist-hiv-target-cell-training-set" title="Link to this heading">#</a></h2>
<p>Zhongtian Shao</p>
<p>Published 2023-06-28</p>
<p>Licensed CC-BY-4.0</p>
<p>40 annotated immunofluorescence microscopy images (600 microns x 600 microns) of foreskin tissue stained for CD3/CD4/CCR5/Nuclei. These images were used to train StarDist models used for the identification of HIV Target Cells in foreskin tissue section scans. </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8091914">https://zenodo.org/records/8091914</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8091914">https://doi.org/10.5281/zenodo.8091914</a></p>
</section>
<hr class="docutils" />
<section id="root-tissue-segmentation-dataset">
<h2>Root tissue segmentation dataset<a class="headerlink" href="#root-tissue-segmentation-dataset" title="Link to this heading">#</a></h2>
<p>Julian Wanner, Kuhn Cuellar, Luis, Friederike Wanke</p>
<p>Published 2022-01-12</p>
<p>Licensed CC-BY-4.0</p>
<p>The PHDFM dataset is composed of fluorescence microscopy images of root tissue samples from A. thaliana, using the ratiometric fluorescent indicator 8‐hydroxypyrene‐1,3,6‐trisulfonic acid trisodium salt (HPTS). This semantic segmentation training dataset consists of 2D microscopy images (the brightfield channel for excitation at 405 nm), each containing a segmentation mask as an additional image channel (manually annotated by plant biologists). The segmentation masks classify pixels into the following 5 labels with the corresponding IDs: background (0), root tissue (1), early elongation zone (2), late elongation zone (3), and meristematic zone (4).</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5841376">https://zenodo.org/records/5841376</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5841376">https://doi.org/10.5281/zenodo.5841376</a></p>
</section>
<hr class="docutils" />
<section id="segmentation-of-nuclei-in-histopathology-images-by-deep-regression-of-the-distance-map">
<h2>Segmentation of Nuclei in Histopathology Images by deep regression of the distance map<a class="headerlink" href="#segmentation-of-nuclei-in-histopathology-images-by-deep-regression-of-the-distance-map" title="Link to this heading">#</a></h2>
<p>Naylor Peter Jack, Walter Thomas, Laé Marick, Reyal Fabien</p>
<p>Published 2018-02-16</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset has been annonced in our accepted paper “Segmentation of Nuclei in Histopathology Images by deep regression of the distance map” in Transcation on Medical Imaging on the 13th of August.
This dataset consists of 50 annotated images, divided into 11 patients.</p>
<p> </p>
<p>v1.1 (27/02/19): Small corrections to a few pixel that were labelled nuclei but weren’t.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/2579118">https://zenodo.org/records/2579118</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.2579118">https://doi.org/10.5281/zenodo.2579118</a></p>
</section>
<hr class="docutils" />
<section id="segmenting-cells-in-a-spheroid-in-3d-using-2d-stardist-within-trackmate">
<h2>Segmenting cells in a spheroid in 3D using 2D StarDist within TrackMate<a class="headerlink" href="#segmenting-cells-in-a-spheroid-in-3d-using-2d-stardist-within-trackmate" title="Link to this heading">#</a></h2>
<p>Jean-Yves Tinevez, Joanna W. Pylvänäinen, Guillaume Jacquemet</p>
<p>Published 2021-08-19</p>
<p>Licensed CC-BY-4.0</p>
<p>3D image of cells in a spheroid, imaged on a confocal microscope, used in a tutorial to demonstrate how to hack TrackMate to segment cells in 3D using the 2D segmentation algorithms it ships.</p>
<p>Image by Guillaume Jacquemet.</p>
<p>For more details see <a class="reference external" href="https://imagej.net/plugins/trackmate/trackmate-stardist#generation-of-3d-labels-by-tracking-2d-labels-using-trackmate">https://imagej.net/plugins/trackmate/trackmate-stardist#generation-of-3d-labels-by-tracking-2d-labels-using-trackmate</a></p>
<p> </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5220610">https://zenodo.org/records/5220610</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5220610">https://doi.org/10.5281/zenodo.5220610</a></p>
</section>
<hr class="docutils" />
<section id="simulated-hl60-cells-from-the-cell-tracking-challenge">
<h2>Simulated HL60 cells (from the Cell Tracking Challenge)<a class="headerlink" href="#simulated-hl60-cells-from-the-cell-tracking-challenge" title="Link to this heading">#</a></h2>
<p>Vebjorn Ljosa, Katherine L. Sokolnicki, Anne E. Carpenter</p>
<p>Published 2012-06-28</p>
<p>Licensed CC0-1.0</p>
<p>These are synthetic images from the Cell Tracking Challenge. The images depict simulated nuclei of HL60 cells stained with Hoescht (training datasets). These synthetic images of HL60 cells provide an opportunity to test image analysis software by comparing segmentation results to the available ground truth for each time point. The number of clustered nuclei increases with time adding more complexity to the problem. This time-laps dataset can be used for simple segmentation or for nuclei tracking.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC035">https://bbbc.broadinstitute.org/BBBC035</a></p>
</section>
<hr class="docutils" />
<section id="single-cell-approach-dissecting-agr-quorum-sensing-dynamics-in-staphylococcus-aureus">
<h2>Single-cell approach dissecting agr quorum sensing dynamics in Staphylococcus aureus<a class="headerlink" href="#single-cell-approach-dissecting-agr-quorum-sensing-dynamics-in-staphylococcus-aureus" title="Link to this heading">#</a></h2>
<p>Julian Bär</p>
<p>Published 2024-02-28</p>
<p>Licensed CC-BY-4.0</p>
<p>Training data for the two StarDist2D models and the DeLTA 2.0 2D tracking model used in the publication on bioarxiv. The trained stardist models are included in the respective zip files of the training data. mm: mother-machine; cc: connected chamber. Each of them contains two folders, img and seg_label. They contain matching pairs of phasecontrast images (img) and label images (seg_label). 
 
tracking_set_subset.zip contains the training data for the DeLTA tracking model following the default folder structure. We used custom weight functions to create the training weight maps in the folder wei. The folder wei_bck contains weights generated with the original function.
The unet_pads_tracking.hdf5 is the retrained tracking model used in the associated publication.
See associated GitHub repository for example code on how to use the models for segmentation and tracking.
The four numbered zip files contain the data used to create all figures displaying image analysis output.
Abstract:
Staphylococcus aureus both colonizes humans and causes severe virulent infections. Virulence is regulated by the agr quorum sensing system and its autoinducing peptide (AIP), with dynamics at the single-cell level across four agr-types – each defined by distinct AIP sequences and capable of cross-inhibition – remaining elusive. Employing microfluidics, time-lapse microscopy, and deep-learning image analysis, we uncovered significant differences in AIP sensitivity among agr-types. We observed bimodal agr activation, attributed to intergenerational phenotypic stability and influenced by AIP concentration. Upon AIP stimulation, agr‑III showed AIP insensitivity, while agr‑II exhibited increased sensitivity and prolonged generation time. Beyond expected cross-inhibition of agr‑I by heterologous AIP‑II and ‑III, the presumably cross-activating AIP‑IV also inhibited agr‑I. Community interactions across different agr-type pairings revealed four main patterns: stable or switched dominance, and delayed or stable dual activation, influenced by community characteristics. These insights underscore the potential of personalized treatment strategies considering virulence and genetic diversity.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10720439">https://zenodo.org/records/10720439</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10720439">https://doi.org/10.5281/zenodo.10720439</a></p>
</section>
<hr class="docutils" />
<section id="stardist-adipocyte-segmentation-training-data-training-notebook-and-model">
<h2>StarDist Adipocyte Segmentation Training data, Training Notebook and Model<a class="headerlink" href="#stardist-adipocyte-segmentation-training-data-training-notebook-and-model" title="Link to this heading">#</a></h2>
<p>Sarkis Rita, Naveiras Olaia, Burri Olivier, Weigert Martin, De Leval Laurence</p>
<p>Published 2022-08-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Data from H&amp;E human bone marrow whole slide scanner images used in the paper: “MarrowQuant 2.0: a digital pathology workflow assisting bone marrow evaluation in clinical and experimental hematology” (<a class="reference external" href="https://doi.org/10.21203/rs.3.rs-1860140/v1">https://doi.org/10.21203/rs.3.rs-1860140/v1</a>)</p>
<p> </p>
<p>292 image patches</p>
<p>Ground truth were manually annotated using QuPath and split into 263 images for training and 29 for validation.</p>
<p>Training in StarDist was done on a Windows 10 PC with an RTX 2080 GPU. The requirements file for installing a Python 3.7 environment to run the attached notebooks is provided (stardist-val.txt).</p>
<p>The StarDist model configuration can be found in the Jupyter Notebook :</p>
<p>Adipocyte Training.ipynb</p>
<p>Model validation and metrics can be performed by running the notebook after finishing the Adipocyte Training notebook.</p>
<p>Quality Control.ipynb</p>
<p> </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7003909">https://zenodo.org/records/7003909</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7003909">https://doi.org/10.5281/zenodo.7003909</a></p>
</section>
<hr class="docutils" />
<section id="stardist-model-and-data-for-the-segmentation-of-yersinia-enterocolitica-cells-in-widefield-images">
<h2>StarDist model and data for the segmentation of Yersinia enterocolitica cells in widefield images<a class="headerlink" href="#stardist-model-and-data-for-the-segmentation-of-yersinia-enterocolitica-cells-in-widefield-images" title="Link to this heading">#</a></h2>
<p>Christoph Spahn, Andreas Diepold, Francesca Ermoli</p>
<p>Published 2024-05-02</p>
<p>Licensed CC-BY-4.0</p>
<p>Dataset and StarDist model for the segmentation of Yersinia enterocolitica cells
This dataset and StarDist model are part of the publication “Active downregulation of the type III secretion system at higher local cell densities promotes Yersinia replication and dissemination”.
It contains the dataset that was used for training the provided StarDist model using ZeroCostDL4Mic.
Data:
Yersinia enterocolitica cells were spotted on an agarose pad (1.5% low melting agarose (Sigma-Aldrich) in minimal medium, 1% Casamino acids, 5 mM EGTA,  glass depression slides (Marienfeld)). For imaging, a Deltavision Elite Optical Sectioning Microscope equipped with a UPlanSApo 100×/1.40 oil objective (Olympus) and an EDGE sCMOS_5.5 camera (Photometrics) was used. Z-stacks with 9 slices (∆z = 0.15 µm) per fluorescence channel were acquired and  5 slices were selected for network training. Images were annotated in Fiji using the Freehand selection tool, and brightlight and mask images were quartered to obtain the final dataset of 300 paired images. 260 images were used for training, while 40 images were used to test model performance.
Model:
The StarDist 2D model was trained from scratch for 100 epochs on 300 paired image patches (image dimensions: (480 x 480 px²), patch size: (480 x 480 px²)) with a batch size of 4 and a mae loss function, using the StarDist 2D ZeroCostDL4Mic notebook (v 1) (von Chamier &amp; Laine et al., 2020). Grid parameter was set to 2 and the number of rays to 120. The model was trained with an initial learning rate of 0.0003 using a 80/20 train/test split. The dataset was augmented 4-fold by flipping and rotation.
Key python packages used include tensorflow (v 0.1.12), Keras (v2.3.1), csbdeep (v 0.7.2), numpy (v 1.21.6), cuda (v 11.1.105Build cuda_11.1.TC455_06.29190527_0). The training was accelerated using a Tesla T4 GPU.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/11105050">https://zenodo.org/records/11105050</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.11105050">https://doi.org/10.5281/zenodo.11105050</a></p>
</section>
<hr class="docutils" />
<section id="stardist-aspc1-lifeact">
<h2>StarDist_AsPC1_Lifeact<a class="headerlink" href="#stardist-aspc1-lifeact" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-08-29</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model designed for segmenting AsPC1 cells labeled with Lifeact from fluorescence microscopy images. The model distinguishes individual AsPC1 cells within clusters and separates them from the background. The model was trained on a small dataset and achieved an Intersection over Union (IoU) score of 0.884 and an F1 Score of 0.967, indicating high accuracy in cell segmentation.
Specifications</p>
<p>Model: StarDist for segmenting AsPC1 cells in fluorescence microscopy images</p>
<p>Training Dataset:</p>
<p>Number of Images: 10 paired fluorescence microscopy images and label masks</p>
<p>Microscope: Spinning disk confocal microscope (3i CSU-W1) with a 20x objective, NA 0.8</p>
<p>Data Type: Fluorescence microscopy images of the AsPC1 Lifeact channel with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Fluorescence Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1024 pixels (Pixel size: 0.6337 x 0.6337 µm²)</p>
<p>Model Capabilities:</p>
<p>Segment AsPC1 Cells: Detects individual AsPC1 cells from a cluster and separates them from the background</p>
<p>Measure Intensity: Enables measurement of CD44, ICAM1, ICAM2, or Fibronectin intensity under individual cells in respective channels</p>
<p>Performance:</p>
<p>Average IoU: 0.884</p>
<p>Average F1 Score: 0.967</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a>
 </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/13442128">https://zenodo.org/records/13442128</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13442128">https://doi.org/10.5281/zenodo.13442128</a></p>
</section>
<hr class="docutils" />
<section id="stardist-bf-monocytes-dataset">
<h2>StarDist_BF_Monocytes_dataset<a class="headerlink" href="#stardist-bf-monocytes-dataset" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-01-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model and its training and validation datasets for detecting mononucleated cells perfused over an endothelial cell monolayer. The model was trained on 27 manually annotated images and achieved an average F1 Score of 0.941. The dataset and model are helpful for biomedical research, especially in studying interactions between mononucleated and endothelial cells.
Specifications</p>
<p>Model: StarDist for mononucleated cell detection on endothelial cells</p>
<p>Training Dataset:</p>
<p>Number of Images: 27 paired brightfield microscopy images and label masks</p>
<p>Microscope: Nikon Eclipse Ti2-E, 20x objective</p>
<p>Data Type: Brightfield microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1022 pixels (Pixel size: 650 nm)</p>
<p>Training Parameters:</p>
<p>Epochs: 400</p>
<p>Patch Size: 992 x 992 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.941</p>
<p>Average IoU: 0.831</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers
Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10572200">https://zenodo.org/records/10572200</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10572200">https://doi.org/10.5281/zenodo.10572200</a></p>
</section>
<hr class="docutils" />
<section id="stardist-bf-neutrophil-dataset">
<h2>StarDist_BF_Neutrophil_dataset<a class="headerlink" href="#stardist-bf-neutrophil-dataset" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-01-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model and its training and validation datasets for detecting neutrophils perfused over an endothelial cell monolayer. The model was trained on 36 manually annotated images, achieving an average F1 Score of 0.969. The dataset and model are intended for use in biomedical research, particularly for analyzing interactions between neutrophils and endothelial cells.
Specifications</p>
<p>Model: StarDist for neutrophil detection on endothelial cells</p>
<p>Training Dataset:</p>
<p>Number of Images: 36 paired brightfield microscopy images and label masks</p>
<p>Microscope: Nikon Eclipse Ti2-E, 20x objective</p>
<p>Data Type: Brightfield microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1022 pixels (Pixel size: 650 nm)</p>
<p>Training Parameters:</p>
<p>Epochs: 400</p>
<p>Patch Size: 992 x 992 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.969</p>
<p>Average IoU: 0.914</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10572231">https://zenodo.org/records/10572231</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10572231">https://doi.org/10.5281/zenodo.10572231</a></p>
</section>
<hr class="docutils" />
<section id="stardist-bf-cancer-cell-dataset-10x">
<h2>StarDist_BF_cancer_cell_dataset_10x<a class="headerlink" href="#stardist-bf-cancer-cell-dataset-10x" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-08-12</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model and its training dataset designed for segmenting cancer cells perfused over an endothelial cell monolayer captured at 10x magnification. The model was trained on 77 manually annotated images, with the dataset being computationally augmented during training by a factor of 8. The model was trained for 500 epochs and achieved an average F1 Score of 0.968, indicating high accuracy in segmenting cancer cells on endothelial cells.
Specifications</p>
<p>Model: StarDist for cancer cell segmentation on endothelial cells (10x magnification)</p>
<p>Training Dataset:</p>
<p>Number of Images: 77 paired brightfield microscopy images and label masks</p>
<p>Augmented Dataset: Computational augmentation by a factor of 8 during training</p>
<p>Microscope: Nikon Eclipse Ti2-E, 10x objective</p>
<p>Data Type: Brightfield microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit or 16-bit</p>
<p>Image Size: 1024 x 1022 pixels (pixel size: 1.3148 μm)</p>
<p>Training Parameters:</p>
<p>Epochs: 500</p>
<p>Patch Size: 992 x 992 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.968</p>
<p>Average IoU: 0.882</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/13304399">https://zenodo.org/records/13304399</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13304399">https://doi.org/10.5281/zenodo.13304399</a></p>
</section>
<hr class="docutils" />
<section id="stardist-bf-cancer-cell-dataset-20x">
<h2>StarDist_BF_cancer_cell_dataset_20x<a class="headerlink" href="#stardist-bf-cancer-cell-dataset-20x" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-01-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository contains a StarDist deep learning model and its training and validation datasets designed for segmenting cancer cells perfused over an endothelial cell monolayer captured at 20x magnification. Using computational methods, the initial dataset of 20 manually annotated images was augmented to 160 paired images. The model was trained over 400 epochs and achieved an average F1 Score of 0.921, demonstrating high accuracy in cell segmentation tasks.
Specifications</p>
<p>Model: StarDist for cancer cell segmentation on endothelial cells (20x magnification)</p>
<p>Training Dataset:</p>
<p>Number of Original Images: 20 paired brightfield microscopy images and label masks</p>
<p>Microscope: Nikon Eclipse Ti2-E, 20x objective</p>
<p>Data Type: Brightfield microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1022 pixels (Pixel size: 650 nm)</p>
<p>Training Parameters:</p>
<p>Epochs: 400</p>
<p>Patch Size: 992 x 992 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.921</p>
<p>Average IoU: 0.793</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p> </p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10572122">https://zenodo.org/records/10572122</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10572122">https://doi.org/10.5281/zenodo.10572122</a></p>
</section>
<hr class="docutils" />
<section id="stardist-fluorescent-cells">
<h2>StarDist_Fluorescent_cells<a class="headerlink" href="#stardist-fluorescent-cells" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-01-26</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository includes a StarDist deep learning model and its training and validation datasets for detecting fluorescently labeled cancer cells perfused over an endothelial cell monolayer. The model was trained on 66 images labeled with CellTrace and demonstrated high accuracy, achieving an average F1 Score of 0.877. The dataset and the trained model can be used for biomedical image analysis, particularly in cancer research.
Specifications</p>
<p>Model: StarDist for cancer cell detection</p>
<p>Training Dataset:</p>
<p>Number of Images: 66 paired fluorescent microscopy images and label masks</p>
<p>Microscope: Nikon Eclipse Ti2-E, 10x objective</p>
<p>Data Type: Fluorescent microscopy images with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1024 pixels (Pixel size: 1.3205 μm)</p>
<p>Training Parameters:</p>
<p>Epochs: 200</p>
<p>Patch Size: 1024 x 1024 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.877</p>
<p>Average IoU: 0.646</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10572310">https://zenodo.org/records/10572310</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10572310">https://doi.org/10.5281/zenodo.10572310</a></p>
</section>
<hr class="docutils" />
<section id="stardist-huvec-nuclei-dataset">
<h2>StarDist_HUVEC_nuclei_dataset<a class="headerlink" href="#stardist-huvec-nuclei-dataset" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-02-05</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository contains a StarDist deep learning model and its training and validation datasets for segmenting endothelial nuclei while ignoring cancer cells. The cancer cells were perfused over an endothelial cell monolayer. The initial dataset consisted of 17 images, where cancer cell nuclei were manually removed after segmentation with the StarDist Versatile Nuclei model. This dataset was augmented to 68 paired images using computational techniques like rotation and flipping. The model was trained for 200 epochs, achieving an average F1 Score of 0.976, demonstrating high accuracy in segmenting endothelial nuclei while excluding cancer cells.
Specifications</p>
<p>Model: StarDist for segmenting endothelial nuclei while ignoring cancer cells</p>
<p>Training Dataset:</p>
<p>Number of Original Images: 17 paired predictions of nuclei and label images</p>
<p>Augmented Dataset: Expanded to 68 paired images using rotation and flipping</p>
<p>Source Image Generation: Generated using a pix2pix model trained to predict nuclei from brightfield images of cancer cells on top of an endothelium (DOI: 10.5281/zenodo.10617532)</p>
<p>Target Image Generation: Masks obtained via manual segmentation</p>
<p>File Format: TIFF (.tif)</p>
<p>Brightfield Images: 8-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 1024 x 1022 pixels (uncalibrated)</p>
<p>Training Parameters:</p>
<p>Epochs: 200</p>
<p>Patch Size: 1024 x 1024 pixels</p>
<p>Batch Size: 2</p>
<p>Performance:</p>
<p>Average F1 Score: 0.976</p>
<p>Average IoU: 0.927</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10617532">https://zenodo.org/records/10617532</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10617532">https://doi.org/10.5281/zenodo.10617532</a></p>
</section>
<hr class="docutils" />
<section id="stardist-tumorcell-nuclei">
<h2>StarDist_TumorCell_nuclei<a class="headerlink" href="#stardist-tumorcell-nuclei" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-08-29</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository contains a StarDist deep learning model designed for segmenting tumor cell nuclei from the DAPI channel in fluorescence microscopy images while excluding HUVEC nuclei. The model was trained to accurately detect individual tumor cell nuclei for subsequent measurement of CD44, ICAM1, ICAM2, or Fibronectin intensity around or under the nuclei. The model achieved an Intersection over Union (IoU) score of 0.558 and an F1 Score of 0.793, reflecting its capability to distinguish tumor cell nuclei from HUVEC nuclei.
Specifications</p>
<p>Model: StarDist for segmenting tumor cell nuclei from the DAPI fluorescence channel</p>
<p>Training Dataset:</p>
<p>Number of Images: 48 paired fluorescence microscopy images and label masks</p>
<p>Microscope: Spinning disk confocal microscope (3i CSU-W1) with a 20x objective, NA 0.8</p>
<p>Data Type: Fluorescence microscopy images of the DAPI channel with manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Fluorescence Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 920 x 920 pixels (Pixel size: 0.6337 x 0.6337 µm²)</p>
<p>Model Capabilities:</p>
<p>Segment Tumor Cell Nuclei: Detects individual tumor cell nuclei in the DAPI channel while distinguishing them from HUVEC nuclei</p>
<p>Measure Intensity: Enables measurement of CD44, ICAM1, ICAM2, or Fibronectin intensity around or under tumor cell nuclei in respective channels</p>
<p>Performance:</p>
<p>Average IoU: 0.558</p>
<p>Average F1 Score: 0.793</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a>
 </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/13443221">https://zenodo.org/records/13443221</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13443221">https://doi.org/10.5281/zenodo.13443221</a></p>
</section>
<hr class="docutils" />
<section id="stardist-model-and-training-dataset-for-automated-tracking-of-mda-mb-231-and-bt20-cells">
<h2>Stardist model and training dataset for automated tracking of MDA-MB-231 and BT20 cells<a class="headerlink" href="#stardist-model-and-training-dataset-for-automated-tracking-of-mda-mb-231-and-bt20-cells" title="Link to this heading">#</a></h2>
<p>Hussein Al-Akhrass, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2021-05-26</p>
<p>Licensed CC-BY-4.0</p>
<p>StarDist Model:
The StarDist model was generated using the ZeroCostDL4Mic platform (Chamier et al., 2021). This custom StarDist model was trained for 300 epochs using 46 manually annotated paired images (image dimensions: (1024, 1024)) with a batch size of 2, an augmentation factor of 4 and a mae loss function. The StarDist “Versatile fluorescent nuclei” model was used as a training starting point. Key python packages used include TensorFlow (v 0.1.12), Keras (v 2.3.1), CSBdeep (v 0.6.1), NumPy (v 1.19.5), Cuda (v 11.0.221). The training was accelerated using a Tesla P100GPU.
The model weights can be used in the ZeroCostDL4Mic StarDist 2D notebook or in the StarDist Fiji plugin.</p>
<p>StarDist Training dataset:
Paired microscopy images (fluorescence) and corresponding masks</p>
<p>Microscopy data type: Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Cells were imaged using a 20x Nikon CFI Plan Apo Lambda objective (NA 0.75) one frame every 10 minutes for 16h.</p>
<p>Cell type: MDA-MB-231 cells and BT20 cells</p>
<p>File format: .tif (16-bit for fluorescence and 8 and 16-bit for the masks)</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/4811213">https://zenodo.org/records/4811213</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.4811213">https://doi.org/10.5281/zenodo.4811213</a></p>
</section>
<hr class="docutils" />
<section id="stardist-miapaca2-from-cd44">
<h2>Stardist_MiaPaCa2_from_CD44<a class="headerlink" href="#stardist-miapaca2-from-cd44" title="Link to this heading">#</a></h2>
<p>Gautier Follain, Sujan Ghimire, Joanna Pylvänäinen, Johanna Ivaska, Guillaume Jacquemet</p>
<p>Published 2024-08-29</p>
<p>Licensed CC-BY-4.0</p>
<p>This repository contains a StarDist deep learning model designed for segmenting MiaPaCa2 cells from the CD44 channel in fluorescence microscopy images. The model is capable of accurately segmenting individual MiaPaCa2 cells while excluding HUVECs. Trained on a small dataset, the model achieved an Intersection over Union (IoU) score of 0.884 and an F1 Score of 0.950, indicating high precision in cell segmentation.
Specifications</p>
<p>Model: StarDist for segmenting MiaPaCa2 cells from the CD44 fluorescence channel</p>
<p>Training Dataset:</p>
<p>Number of Images: 8 paired fluorescence microscopy images and label masks</p>
<p>Microscope: Spinning disk confocal microscope (3i CSU-W1) with a 20x objective, NA 0.8</p>
<p>Data Type: Fluorescence microscopy images of the CD44 channel, obtained after immunofluorescence staining with primary and secondary antibodies and manually segmented masks</p>
<p>File Format: TIFF (.tif)</p>
<p>Fluorescence Images: 16-bit</p>
<p>Masks: 8-bit</p>
<p>Image Size: 920 x 920 pixels (Pixel size: 0.6337 x 0.6337 µm²)</p>
<p>Model Capabilities:</p>
<p>Segment MiaPaCa2 Cells: Accurately detects individual MiaPaCa2 cells while ignoring HUVECs</p>
<p>Measure CD44 Intensity: Allows for the measurement of CD44 intensity around MiaPaCa2 cells, specifically from the CD44 channel</p>
<p>Performance:</p>
<p>Average IoU: 0.884</p>
<p>Average F1 Score: 0.950</p>
<p>Model Training: Conducted using ZeroCostDL4Mic (<a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki">HenriquesLab/ZeroCostDL4Mic</a>)</p>
<p>Reference
Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers</p>
<p>Gautier Follain, Sujan Ghimire, Joanna W. Pylvänäinen, Monika Vaitkevičiūtė, Diana Wurzinger, Camilo Guzmán, James RW Conway, Michal Dibus, Sanna Oikari, Kirsi Rilla, Marko Salmi, Johanna Ivaska, Guillaume Jacquemet
bioRxiv 2024.09.30.615654; doi: <a class="reference external" href="https://doi.org/10.1101/2024.09.30.615654">https://doi.org/10.1101/2024.09.30.615654</a></p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/13442877">https://zenodo.org/records/13442877</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.13442877">https://doi.org/10.5281/zenodo.13442877</a></p>
</section>
<hr class="docutils" />
<section id="synapsenet-training-data">
<h2>SynapseNet Training Data<a class="headerlink" href="#synapsenet-training-data" title="Link to this heading">#</a></h2>
<p>Constantin Pape</p>
<p>Published 2024-12-01</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset contains room-temperature single-axis TEM tomograms from Schaffer collateral and mossy fiber synapses in organotypic hippocampal slices. The tomograms were published in the two studies [1, 2]. The data was re-used for training deep neural networks to segment different synaptic structures in electron micrographs in [3]. For the tomograms, organotypic slices were prepared from the hippocampi of neonatal mice according to the interface protocol55 and vitrified after 28 days in vitro in culture medium supplemented with 20% (w/v) bovine serum albumin using an HPM100 (Leica) high-pressure freezing device. The dataset also contains 23 tomograms resulting from chemically-fixed material, which were also published in (Maus et al., 2020). For these tomograms, wild-type animals at postnatal day 28 were transcardially perfused under deep anesthesia, first with 0.9% sodium chloride solution, and then one of two fixatives (Fixative 1: Ice-cold 4% paraformaldehyde, 2.5% glutaraldehyde in 0.1 M phosphate buffer16; Fixative 2: 37° C 2% paraformaldehyde, 2.5% glutaraldehyde, 2 mM CaCl2, in 0.1 M cacodylate buffer56). Brains were rinsed and sectioned coronally through the dorsal hippocampus in an ice-cold 0.1 M phosphate buffer using a VT 1200S vibratome (Leica) (step size 100 µm; amplitude 1.5 mm, speed 0.1 mm/sec). Hippocampal CA3 subregions were excised using a 1.5 mm diameter biopsy punch and high-pressure frozen on the same day in 20% (w/v) bovine serum albumin using an HPM100 (Leica) high-pressure freezing device. For both sample preparations, automated freeze-substitution was performed. Tomograms were collected using a 200 kV JEM-2100 (JEOL) transmission electron microscope equipped with an 11 MP Orius SC1000 CCD camera (Gatan). Tilt-series (tilt range +/- 60°; 1° angular increments) were acquired at 30 000x magnification using SerialEM58. Tomographic reconstructions were generated using weighted back-projection with etomo.The data is organized into two different subfolders for data with annotations for “vesicles” and “active_zones”. Each of these subfolders is further subdivided into “train” and “test” folders, which containtomograms for the two different sample preparations in “chemical_fixation” and “single_axis_tem”.Each tomogram and the corresponding annotation is stored as a hdf5 file, containing the following internal datasets:- raw: The tomogram data.- labels/vesicles: Annotations for the synaptic vesicles, annotated with IMOD, further postprocessed and then exported to instance masks. (for tomograms in “vesicles”)- labels/AZ: Annotations for the active zone, annotated with IMOD and exported to binary masks.
[1] Imig et al., The Morphological and Molecular Nature of Synaptic Vesicle Priming at Presynaptic Active Zones, Neuron, 2014, DOI:10.1016/j.neuron.2014.10.009[2] Maus et al., Ultrastructural Correlates of Presynaptic Functional Heterogeneity in Hippocampal Synapses, Cell Reports, 2020, DOI: 10.1016/j.celrep.2020.02.083[3] Muth, Moschref et al., 2024, Preprint to be published</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/14330011">https://zenodo.org/records/14330011</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.14330011">https://doi.org/10.5281/zenodo.14330011</a></p>
</section>
<hr class="docutils" />
<section id="synthetic-cells">
<h2>Synthetic cells<a class="headerlink" href="#synthetic-cells" title="Link to this heading">#</a></h2>
<p>Vebjorn Ljosa, Katherine L. Sokolnicki, Anne E. Carpenter</p>
<p>Published 2012-06-28</p>
<p>Licensed CC-BY-NC-SA-3.0</p>
<p>One of the principal challenges in counting or segmenting nuclei is dealing with clustered nuclei. To help assess algorithms performance in this regard, this synthetic image set consists of five subsets with increasing degree of clustering.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://bbbc.broadinstitute.org/BBBC004">https://bbbc.broadinstitute.org/BBBC004</a></p>
</section>
<hr class="docutils" />
<section id="synthetic-images-and-segmentation-masks-simulating-hl-60-cell-nucleus-in-3d">
<h2>Synthetic images and segmentation masks simulating HL-60 cell nucleus in 3D<a class="headerlink" href="#synthetic-images-and-segmentation-masks-simulating-hl-60-cell-nucleus-in-3d" title="Link to this heading">#</a></h2>
<p>David Svoboda, Michal Kozubek, Stanislav Stejskal, Teresa Zulueta-Coarasa</p>
<p>Published 2024-11-26</p>
<p>Licensed CC-BY-3.0</p>
<p>One of the principal challenges in counting or segmenting nuclei is dealing with clustered nuclei. To help assess algorithms performance in this regard, this synthetic image set consists of four subsets with increasing degree of clustering. Each subset is also provided in two different levels of quality: high SNR and low SNR.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.ebi.ac.uk/bioimage-archive/galleries/ai/analysed-dataset/S-BIAD1492/">https://www.ebi.ac.uk/bioimage-archive/galleries/ai/analysed-dataset/S-BIAD1492/</a></p>
</section>
<hr class="docutils" />
<section id="tnbc">
<h2>TNBC<a class="headerlink" href="#tnbc" title="Link to this heading">#</a></h2>
<p>Naylor Peter Jack, Walter Thomas, Laé Marick, Reyal Fabien</p>
<p>Published 2018-02-16</p>
<p>Licensed CC-BY-4.0</p>
<p>Involves an annotated large number of cells, including normal epithelial and myoepithelial breast cells (localized in ducts and lobules), invasive carcinomatous cells, fibroblasts, endothelial cells, adipocytes, macrophages and inflammatory cells (lymphocytes and plasmocytes). In total, our data set consists of 50 images with a total of 4022 annotated cells, the maximum number of cells in one sample is 293 and the minimum number of cells in one sample is 5, with an average of 80 cells per sample and a high standard deviation of 58. The annotation was performed by three experts: an expert pathologist and two trained research fellows. Each sample was annotated by one of the annotators, checked by another one and in case of disagreement, a consensus was established by discussion among the 3 experts.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://paperswithcode.com/dataset/tnbc">https://paperswithcode.com/dataset/tnbc</a></p>
</section>
<hr class="docutils" />
<section id="training-set-of-microscopy-images-for-dietler-et-al-nature-communications-2020">
<h2>Training set of microscopy images for Dietler et al. Nature Communications 2020<a class="headerlink" href="#training-set-of-microscopy-images-for-dietler-et-al-nature-communications-2020" title="Link to this heading">#</a></h2>
<p>Nicola Dietler, Matthias Minder, Vojislav Gligorovski, Economou, Augoustina Maria, Joly, Denis Alain Henri Lucien, Ahmad Sadeghi, Chan, Chun Hei Michael, Mateusz Kozinski, Martin Weigert, Anne-Florence Bitbol, Rahi, Sahand Jamal</p>
<p>Published 2021-12-07</p>
<p>Licensed CC-BY-4.0</p>
<p>Training set of microscopy images for Dietler et al. Nature Communications 2020</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5765648">https://zenodo.org/records/5765648</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5765648">https://doi.org/10.5281/zenodo.5765648</a></p>
</section>
<hr class="docutils" />
<section id="volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset">
<h2>Volumetric segmentation of biological cells and subcellular structures for optical diffraction tomography images - dataset<a class="headerlink" href="#volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset" title="Link to this heading">#</a></h2>
<p>Martyna Mazur, Wojciech Krauze</p>
<p>Published 2023-06-16</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset includes 4 files with segmentation results for 4 different ODT reconstructions of SH-SY5Y neuroblastoma cell. The segmentation results contain:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>3D binary masks of biological cells obtained through Cellpose [1] and ODT-SAS;
3D binary masks of organelles: nucleoli and lipid structures (LS) obtained through slice-by-slice manual segmentation&amp;nbsp;and ODT-SAS.
</pre></div>
</div>
<p>All files are .*mat files.</p>
<p>The files REC_SH-SY5Y_1.mat, REC_SH-SY5Y_2.mat and REC_SH-SY5Y_3.mat consist of 7 variables:</p>
<p>RECON – tomographic reconstruction of SH-SY5Y neuroblastoma cell;
n_imm – refractive index of object immersion medium;
dx – object space sample size in XY [(\mu m)];
rayXY – xy-coordinates of illumination vectors;</p>
<p>maskManual – table with manually determined 3D binary masks of organelles;
maskCellpose – 3D binary mask of biological cell obtained through Cellpose;
maskODTSAS – table with 3D binary masks of biological cell and their organelles obtained through ODT-SAS.</p>
<p>File REC_SH-SY5Y_4.mat includes masks for the ODT-SAS and Cellpose segmentation of three closely packed cells and consists of 5 variables: RECON, n_imm, dx, maskCellpose and maskODTSAS.</p>
<p>Access a particular 3D binary mask from ‘maskManual’ and ‘maskODTSAS’ tables, using the following names: ‘Cell’, ‘Nucleoli’, ‘LS’.
For example:</p>
<p>cellMask = maskODTSAS.Cell{1};</p>
<p>[1] Stringer, C., Wang, T., Michaelos, M., &amp; Pachitariu, M. (2021). Cellpose: a generalist algorithm for cellular segmentation. Nature methods, 18(1), 100-106.</p>
<p> </p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8188948">https://zenodo.org/records/8188948</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8188948">https://doi.org/10.5281/zenodo.8188948</a></p>
</section>
<hr class="docutils" />
<section id="zerocostdl4mic-stardist-2d-example-training-and-test-dataset-light">
<h2>ZeroCostDL4Mic - Stardist 2D example training and test dataset (light)<a class="headerlink" href="#zerocostdl4mic-stardist-2d-example-training-and-test-dataset-light" title="Link to this heading">#</a></h2>
<p>Johanna Jukkala, Guillaume Jacquemet</p>
<p>Published 2023-05-19</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: ZeroCostDL4Mic - Stardist 2D example training and test dataset (light)</p>
<p>(see our Wiki for details)</p>
<p>Data type: Paired microscopy images (fluorescence) and corresponding masks</p>
<p>Microscopy data type: Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation (see <a class="reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist&amp;amp;nbsp;for">https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist&amp;nbsp;for</a> details about the segmentation)</p>
<p>Microscope: Spinning disk confocal microscope with a 20x 0.8 NA objective</p>
<p>Cell type: <a class="reference external" href="http://DCIS.COM">DCIS.COM</a> LifeAct-RFP cells</p>
<p>File format: .tif (16-bit for fluorescence and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 634 nm)</p>
<p> </p>
<p>Author(s): Johanna Jukkala1,2 and Guillaume Jacquemet1,2</p>
<p>Contact email: <a class="reference external" href="mailto:guillaume&#46;jacquemet&#37;&#52;&#48;abo&#46;fi">guillaume<span>&#46;</span>jacquemet<span>&#64;</span>abo<span>&#46;</span>fi</a></p>
<p>Affiliation : </p>
<ol class="arabic simple">
<li><p>Faculty of Science and Engineering, Cell Biology, Åbo Akademi University, 20520 Turku, Finland</p></li>
<li><p>Turku Bioscience Centre, University of Turku and Åbo Akademi University, FI-20520 Turku, Finland</p></li>
</ol>
<p>Funding bodies: G.J. was supported by grants awarded by the Academy of Finland, the Sigrid Juselius Foundation and Åbo Akademi University Research Foundation (CoE CellMech) and by Drug Discovery and Diagnostics strategic funding to Åbo Akademi University.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7949940">https://zenodo.org/records/7949940</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7949940">https://doi.org/10.5281/zenodo.7949940</a></p>
</section>
<hr class="docutils" />
<section id="zerocostdl4mic-stardist-example-training-and-test-dataset">
<h2>ZeroCostDL4Mic - Stardist example training and test dataset<a class="headerlink" href="#zerocostdl4mic-stardist-example-training-and-test-dataset" title="Link to this heading">#</a></h2>
<p>Johanna Jukkala, Guillaume Jacquemet</p>
<p>Published 2020-03-17</p>
<p>Licensed CC-BY-4.0</p>
<p>Name: ZeroCostDL4Mic - Stardist example training and test dataset</p>
<p>(see our Wiki for details)</p>
<p> </p>
<p>Data type: Paired microscopy images (fluorescence) and corresponding masks</p>
<p>Microscopy data type: Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation (see <a class="github reference external" href="https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki/Stardist">HenriquesLab/ZeroCostDL4Mic</a> for details about the segmentation)</p>
<p>Microscope: Spinning disk confocal microscope with a 20x 0.8 NA objective</p>
<p>Cell type: <a class="reference external" href="http://DCIS.COM">DCIS.COM</a> LifeAct-RFP cells</p>
<p>File format: .tif (16-bit for fluorescence and 8 and 16-bit for the masks)</p>
<p>Image size: 1024x1024 (Pixel size: 634 nm)</p>
<p> </p>
<p>Author(s): Johanna Jukkala1,2 and Guillaume Jacquemet1,2</p>
<p>Contact email: <a class="reference external" href="mailto:guillaume&#46;jacquemet&#37;&#52;&#48;abo&#46;fi">guillaume<span>&#46;</span>jacquemet<span>&#64;</span>abo<span>&#46;</span>fi</a></p>
<p>Affiliation : </p>
<ol class="arabic simple">
<li><p>Faculty of Science and Engineering, Cell Biology, Åbo Akademi University, 20520 Turku, Finland</p></li>
<li><p>Turku Bioscience Centre, University of Turku and Åbo Akademi University, FI-20520 Turku, Finland</p></li>
</ol>
<p> </p>
<p>Associated publications: Unpublished</p>
<p>Funding bodies: G.J. was supported by grants awarded by the Academy of Finland, the Sigrid Juselius Foundation and Åbo Akademi University Research Foundation (CoE CellMech) and by Drug Discovery and Diagnostics strategic funding to Åbo Akademi University.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/3715492">https://zenodo.org/records/3715492</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.3715492">https://doi.org/10.5281/zenodo.3715492</a></p>
</section>
<hr class="docutils" />
<section id="cellpose-training-data">
<h2>cellpose training data<a class="headerlink" href="#cellpose-training-data" title="Link to this heading">#</a></h2>
<p>Carsen Stringer, Tim Wang, Michalis Michaelos, Marius Pachitariu</p>
<p>Published 2020-12-14</p>
<p>Licensed CUSTOM LICENSE</p>
<p>This is a cellpose training dataset. Cellpose is a generalist deep learning model for cell segmentation.</p>
<p>Tags: Ai-Ready, Exclude From Dalia</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://www.cellpose.org/dataset">https://www.cellpose.org/dataset</a></p>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tags"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../contributing/format.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">YML format</p>
      </div>
    </a>
    <a class="right-next"
       href="artificial_intelligence.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Artificial intelligence (51)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-ground-truth-annotations-of-nuclei-in-3d-microscopy-volumes">3D Ground Truth Annotations of Nuclei in 3D Microscopy Volumes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-hl60-cell-line-synthetic-data">3D HL60 Cell line (synthetic data)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-cell-shape-of-drosophila-wing-disc">3D cell shape of Drosophila Wing Disc</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-light-sheet-microscopy-data-for-selma3d-2024-challenge-training-subset-with-annotations">3D light-sheet microscopy data for SELMA3D 2024 challenge - Training subset with annotations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-nuclei-instance-segmentation-dataset-of-fluorescence-microscopy-volumes-of-c-elegans">3D nuclei instance segmentation dataset of fluorescence microscopy volumes of C. elegans</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deep-learning-approach-to-quantify-auditory-hair-cells">A deep learning approach to quantify auditory hair cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-annotated-fluorescence-image-dataset-for-training-nuclear-segmentation-methods">An annotated fluorescence image dataset for training nuclear segmentation methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-annotated-high-content-fluorescence-microscopy-dataset-with-hoechst-33342-stained-nuclei-and-manually-labelled-outlines">An annotated high-content fluorescence microscopy dataset with Hoechst 33342-stained nuclei and manually labelled outlines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-image-based-data-driven-analysis-of-cellular-architecture-in-a-developing-tissue">An image-based data-driven analysis of cellular architecture in a developing tissue</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessment-of-residual-breast-cancer-cellularity-after-neoadjuvant-chemotherapy-using-digital-pathology">Assessment of Residual Breast Cancer Cellularity after Neoadjuvant Chemotherapy using Digital Pathology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-labelling-of-hela-kyoto-cells-using-deep-learning-tools">Automatic labelling of HeLa “Kyoto” cells using Deep Learning tools</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bccd-dataset">BCCD Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#breast-cancer-nuclei-images-for-dl-training-zerocostdl4mic-stardist-model">Breast Cancer Nuclei images for DL Training + ZeroCostDL4Mic StarDist Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#breast-cancer-semantic-segmentation-bcss-dataset">Breast Cancer Semantic Segmentation (BCSS) dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellbindb-a-large-scale-multimodal-annotated-dataset">CellBinDB: A Large-Scale Multimodal Annotated Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments">Cellpose training data and scripts from “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers">Cellpose training data and scripts from “Machine learning for histological annotation and quantification of cortical layers”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chinese-hamster-ovary-cells">Chinese Hamster Ovary Cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-1-breast-cancer-cell-dataset">Combining StarDist and TrackMate example 1 -  Breast cancer cell dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-2-t-cell-dataset">Combining StarDist and TrackMate example 2 -  T cell dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-stardist-and-trackmate-example-3-flow-chamber-dataset">Combining StarDist and TrackMate example 3 -  Flow chamber dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cryonuseg">CryoNuSeg</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-segmentation-projects-of-fib-sem-dataset-of-u2-os-cell">Deep learning segmentation projects of FIB-SEM dataset of U2-OS cell</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-training-data-jove">Deep learning training data (JOVE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-bacillus-subtilis-fluorescence-segmentation-dataset">DeepBacs – Bacillus subtilis fluorescence segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-escherichia-coli-bright-field-segmentation-dataset">DeepBacs – Escherichia coli bright field segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-mixed-segmentation-dataset-and-stardist-model">DeepBacs – Mixed segmentation dataset and StarDist model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepbacs-staphylococcus-aureus-widefield-segmentation-dataset">DeepBacs – Staphylococcus aureus widefield segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#drosophila-kc167-cells">Drosophila Kc167 cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-local-topography-on-cell-division-of-staphylococci-sp">Effect of local topography on cell division of Staphylococci sp.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embryonic-mice-ultrasound-volumes-with-body-and-brain-volume-segmentation-masks">Embryonic mice ultrasound volumes with body and brain volume segmentation masks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fiber-and-vessel-dataset-for-segmentation-and-characterization">Fiber and vessel dataset for segmentation and characterization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#go-nuclear-a-deep-learning-based-toolkit-for-3d-nuclei-segmentation-and-quantitative-analysis-in-cellular-and-tissue-context">Go-Nuclear. A deep learning-based toolkit for 3D nuclei segmentation and quantitative analysis in cellular and tissue context</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ground-truth-cell-body-segmentation-used-for-starfinity-training">Ground-truth cell body segmentation used for Starfinity training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hpa-nucleus-segmentation-dpnunet">HPA Nucleus Segmentation (DPNUnet)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking">HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-ht29-colon-cancer-cells">Human HT29 colon-cancer cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-hepatocyte-and-murine-fibroblast-cells-co-culture-experiment">Human Hepatocyte and Murine Fibroblast cells Co-culture experiment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-lung-tissue-microscopy-dic-fluorescence-cell-and-nuclei-semantic-instance-annotations">Human Lung Tissue Microscopy (DIC, Fluorescence, Cell and Nuclei Semantic Instance Annotations)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-u2os-cells-out-of-focus">Human U2OS cells (out of focus)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lmrg-image-analysis-study-fish-datasets">LMRG Image Analysis Study - FISH datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lmrg-image-analysis-study-nuclei-datasets">LMRG Image Analysis Study - nuclei datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lynsec-lymphoma-nuclear-segmentation-and-classification">LyNSeC: Lymphoma Nuclear Segmentation and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#midog-2021">MIDOG 2021</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations">Melanoma Histopathology Dataset with Tissue and Nuclei Annotations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#membrain-seg-training-data">MemBrain-seg training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monuseg-dataset">MoNuSeg Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monusac-2020">MonuSAC 2020</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mouse-embryo-blastocyst-cells">Mouse embryo blastocyst cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurips-2022-cell-segmentation-competition-dataset">NeurIPS 2022 Cell Segmentation Competition Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nuinsseg">NuInsSeg</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nuclei-of-u2os-cells-in-a-chemical-screen">Nuclei of U2OS cells in a chemical screen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nuclei-of-mouse-embryonic-cells">Nuclei of mouse embryonic cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ocelot-overlapped-cell-on-tissue-dataset-for-histopathology">OCELOT: Overlapped Cell on Tissue Dataset for Histopathology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parhyale-3d-segmentation-dataset">Parhyale 3D segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#platynereis-em-training-data">Platynereis EM training data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-axillary-lymph-node-metastasis-in-early-breast-cancer-using-deep-learning-on-primary-tumor-biopsy-slides">Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prodgerlab-stardist-hiv-target-cell-training-set">ProdgerLab-StarDist-HIV Target Cell Training Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-tissue-segmentation-dataset">Root tissue segmentation dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmentation-of-nuclei-in-histopathology-images-by-deep-regression-of-the-distance-map">Segmentation of Nuclei in Histopathology Images by deep regression of the distance map</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#segmenting-cells-in-a-spheroid-in-3d-using-2d-stardist-within-trackmate">Segmenting cells in a spheroid in 3D using 2D StarDist within TrackMate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulated-hl60-cells-from-the-cell-tracking-challenge">Simulated HL60 cells (from the Cell Tracking Challenge)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#single-cell-approach-dissecting-agr-quorum-sensing-dynamics-in-staphylococcus-aureus">Single-cell approach dissecting agr quorum sensing dynamics in Staphylococcus aureus</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-adipocyte-segmentation-training-data-training-notebook-and-model">StarDist Adipocyte Segmentation Training data, Training Notebook and Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-model-and-data-for-the-segmentation-of-yersinia-enterocolitica-cells-in-widefield-images">StarDist model and data for the segmentation of Yersinia enterocolitica cells in widefield images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-aspc1-lifeact">StarDist_AsPC1_Lifeact</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-monocytes-dataset">StarDist_BF_Monocytes_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-neutrophil-dataset">StarDist_BF_Neutrophil_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-cancer-cell-dataset-10x">StarDist_BF_cancer_cell_dataset_10x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-bf-cancer-cell-dataset-20x">StarDist_BF_cancer_cell_dataset_20x</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-fluorescent-cells">StarDist_Fluorescent_cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-huvec-nuclei-dataset">StarDist_HUVEC_nuclei_dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-tumorcell-nuclei">StarDist_TumorCell_nuclei</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-model-and-training-dataset-for-automated-tracking-of-mda-mb-231-and-bt20-cells">Stardist model and training dataset for automated tracking of MDA-MB-231 and BT20 cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stardist-miapaca2-from-cd44">Stardist_MiaPaCa2_from_CD44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synapsenet-training-data">SynapseNet Training Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-cells">Synthetic cells</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-images-and-segmentation-masks-simulating-hl-60-cell-nucleus-in-3d">Synthetic images and segmentation masks simulating HL-60 cell nucleus in 3D</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tnbc">TNBC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-set-of-microscopy-images-for-dietler-et-al-nature-communications-2020">Training set of microscopy images for Dietler et al. Nature Communications 2020</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset">Volumetric segmentation of biological cells and subcellular structures for optical diffraction tomography images - dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zerocostdl4mic-stardist-2d-example-training-and-test-dataset-light">ZeroCostDL4Mic - Stardist 2D example training and test dataset (light)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zerocostdl4mic-stardist-example-training-and-test-dataset">ZeroCostDL4Mic - Stardist example training and test dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data">cellpose training data</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Robert Haase, Clément Caporal,... and the NFDI4BioImage Initiative
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on 2025-09-20.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Copyright: Licensed <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC-BY 4.0</a> unless mentioned otherwise. 
Contributions and feedback are welcome.
</p>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>