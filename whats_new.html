
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Recently added (10) &#8212; NFDI4BioImage Training Materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'whats_new';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="How to contribute" href="contributing/index.html" />
    <link rel="prev" title="NFDI4BioImage Training Materials" href="readme.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="2025-05-29"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="readme.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="NFDI4BioImage Training Materials - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="NFDI4BioImage Training Materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="readme.html">
                    NFDI4BioImage Training Materials
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">What's new</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Recently added (10)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contributing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="contributing/index.html">How to contribute</a></li>

<li class="toctree-l1"><a class="reference internal" href="contributing/submit_app.html">Using the Training Materials Submission App</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing/format.html">YML format</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By tag</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="tags/ai-ready.html">Ai-ready (54)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/artificial_intelligence.html">Artificial intelligence (46)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/bioimage_analysis.html">Bioimage analysis (199)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/bioinformatics.html">Bioinformatics (19)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/data_stewardship.html">Data stewardship (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/fair-principles.html">Fair-principles (27)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/fiji.html">Fiji (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/galaxy.html">Galaxy (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/imagej.html">Imagej (20)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/licensing.html">Licensing (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/metadata.html">Metadata (15)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/napari.html">Napari (14)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/neubias.html">Neubias (27)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/nfdi4bioimage.html">Nfdi4bioimage (46)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/omero.html">Omero (38)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/open_science.html">Open science (9)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/open_source_software.html">Open source software (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/python.html">Python (71)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/reproducibility.html">Reproducibility (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/research_data_management.html">Research data management (140)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/sharing.html">Sharing (12)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/workflow.html">Workflow (9)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tags/workflow_engine.html">Workflow engine (13)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By content type</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="content_types/blog%20post.html">Blog post (28)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/book.html">Book (21)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/code.html">Code (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/collection.html">Collection (84)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/data.html">Data (62)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/document.html">Document (8)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/documentation.html">Documentation (19)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/event.html">Event (8)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/github%20repository.html">Github repository (60)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/notebook.html">Notebook (55)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/online%20tutorial.html">Online tutorial (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/poster.html">Poster (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/preprint.html">Preprint (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/publication.html">Publication (70)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/slides.html">Slides (84)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/tutorial.html">Tutorial (48)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/video.html">Video (38)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/website.html">Website (11)</a></li>
<li class="toctree-l1"><a class="reference internal" href="content_types/workshop.html">Workshop (14)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By license</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="licenses/all_rights_reserved.html">All rights reserved (13)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses/apache-2.0.html">Apache-2.0 (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses/bsd-2-clause.html">Bsd-2-clause (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses/bsd-3-clause.html">Bsd-3-clause (31)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses/cc-by-4.0.html">Cc-by-4.0 (330)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses/cc0-1.0.html">Cc0-1.0 (13)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses/gpl-2.0.html">Gpl-2.0 (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses/gpl-3.0.html">Gpl-3.0 (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses/mit.html">Mit (30)</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses/unknown.html">Unknown (109)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">By domain</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="domain/biapol.github.io.html">Biapol.github.io (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/docs.google.com.html">Docs.google.com (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/doi.org.html">Doi.org (264)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/f1000research.com.html">F1000research.com (11)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/focalplane.biologists.com.html">Focalplane.biologists.com (14)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/git.mpi-cbg.de.html">Git.mpi-cbg.de (7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/github.com.html">Github.com (137)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/training.galaxyproject.org.html">Training.galaxyproject.org (5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/www.biorxiv.org.html">Www.biorxiv.org (6)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/www.ebi.ac.uk.html">Www.ebi.ac.uk (10)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/www.nature.com.html">Www.nature.com (18)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/www.youtube.com.html">Www.youtube.com (28)</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain/zenodo.org.html">Zenodo.org (252)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="statistics/readme.html">Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="export/readme.html">Open data</a></li>
<li class="toctree-l1"><a class="reference internal" href="gdpr_compliance.html">GDPR Compliance Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="imprint.html">Imprint</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NFDI4BIOIMAGE/training" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NFDI4BIOIMAGE/training/issues/new?title=Issue%20on%20page%20%2Fwhats_new.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/whats_new.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Recently added (10)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellbindb-a-large-scale-multimodal-annotated-dataset">CellBinDB: A Large-Scale Multimodal Annotated Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments">Cellpose training data and scripts from “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers">Cellpose training data and scripts from “Machine learning for histological annotation and quantification of cortical layers”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-training-data-jove">Deep learning training data (JOVE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking">HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lynsec-lymphoma-nuclear-segmentation-and-classification">LyNSeC: Lymphoma Nuclear Segmentation and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations">Melanoma Histopathology Dataset with Tissue and Nuclei Annotations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurips-2022-cell-segmentation-competition-dataset">NeurIPS 2022 Cell Segmentation Competition Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ocelot-overlapped-cell-on-tissue-dataset-for-histopathology">OCELOT: Overlapped Cell on Tissue Dataset for Histopathology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset">Volumetric segmentation of biological cells and subcellular structures for optical diffraction tomography images - dataset</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="recently-added-10">
<h1>Recently added (10)<a class="headerlink" href="#recently-added-10" title="Link to this heading">#</a></h1>
<section id="cellbindb-a-large-scale-multimodal-annotated-dataset">
<h2>CellBinDB: A Large-Scale Multimodal Annotated Dataset<a class="headerlink" href="#cellbindb-a-large-scale-multimodal-annotated-dataset" title="Link to this heading">#</a></h2>
<p>Can Shi, Jinghong Fan, Zhonghan Deng, Huanlin Liu, Qiang Kang, Yumei Li, Jing Guo, Jingwen Wang, Jinjiang Gong, Sha Liao, Ao Chen, Ying Zhang, Mei Li</p>
<p>Published 2024-11-20</p>
<p>Licensed CC-ZERO</p>
<p>CellBinDB is a large-scale, multimodal annotated dataset for cell segmentation. It contains 1,044 annotated microscope images and 109,083 cell annotations, covering four staining types: DAPI, ssDNA, H&amp;E, and mIF. CellBinDB contains samples from two species, human and mouse, covering more than 30 histologically different tissue types, including disease-related tissues. The images in CellBinDB come from two sources: 844 mouse images from internal experiments and 200 human images from the open access platform 10x Genomics. We annotated all images in CellBinDB and provide two types of image annotations: semantic and instance masks. A xlsx file is attached to record the detailed information of each image.
In addition, we provide the images and annotations of nine other widely used publicly available cell segmentation datasets downloaded from their original sources, retaining their original formats for ease of use. 
The file ‘mixed_licenses.txt’ contains the original accessions of the public datasets used in our project and their associated licenses. Please refer to these links for more information about each dataset and its licensing terms, and use it according to the specifications.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/15370205">https://zenodo.org/records/15370205</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15370205">https://doi.org/10.5281/zenodo.15370205</a></p>
</section>
<hr class="docutils" />
<section id="cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments">
<h2>Cellpose training data and scripts from “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”<a class="headerlink" href="#cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments" title="Link to this heading">#</a></h2>
<p>Martin Wohlwend, Olivier Burri, Johan Auwerx</p>
<p>Published 2024-02-27</p>
<p>Licensed CC-BY-4.0</p>
<p>This Workflow contains all the material necessary to reproduce the results of the QuPath analysis performed in the paper
 “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”
Inside this workflow and dataset, you will find the following folders</p>
<p>QuPath Training Project: A QuPath 0.3.2 project containing all the manual annotations (ground truths) used to train the cellpose model, as well as the script to start the training
QuPath Demo Project: A QuPath 0.3.2 project containing an example image that can be segmented using cellpose, followed by the classification of the CD45 expressing fibers
Training Images and Demo Images: The raw whole slide scanner 20x images needed by the above QuPath projects
Model: The fodler contianing the trained cellpose model
Cellpose Training Folder: The exported raw and ground truth images that the above cellpose model was trained on
Scripts: The QuPath scripts, also located in their respective QuPath projects, that were created for this whole workflow
QC: A Jupyter notebook, based on ZeroCostDL4Mic that computes quality metrics in order to assess the performance of the trained cellpose model. The folder also contains the resulting metrics.</p>
<p>Installation and Use
If you are going to use the QuPath projects, you need a local QuPath Installation <a class="reference external" href="https://qupath.github.io/">https://qupath.github.io/</a> that is configured to run the QuPath Cellpose Extension <a class="github reference external" href="https://github.com/BIOP/qupath-extension-cellpose">BIOP/qupath-extension-cellpose</a> as well as a working Cellpose installation <a class="github reference external" href="https://github.com/MouseLand/cellpose">MouseLand/cellpose</a>
Instructions for installation are available from the links above.
After that, you should be able to open the QuPath project, navigate to the “Automate &gt; Project scripts” menu and locate the script you wish to run.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7041137">https://zenodo.org/records/7041137</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.7041137">https://doi.org/10.5281/zenodo.7041137</a></p>
</section>
<hr class="docutils" />
<section id="cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers">
<h2>Cellpose training data and scripts from “Machine learning for histological annotation and quantification of cortical layers”<a class="headerlink" href="#cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers" title="Link to this heading">#</a></h2>
<p>Jean Jacquemier, Julie Meystre, Olivier Burri</p>
<p>Published 2024-07-04</p>
<p>Licensed CC-BY-4.0</p>
<p>This Workflow contains all the material necessary to reproduce the cells detection, thanks to the QuPath performed in the paper
 “Machine learning for histological annotation and quantification of cortical layers”
Inside this workflow and dataset, you will find the following folders</p>
<p>QuPath Training Project: A QuPath 0.5.0 project containing all the manual annotations (ground truths) used to train the cellpose model, as well as the script to start the training
Training Images and Demo Images: The raw whole slide scanner images needed by the above QuPath project
Model: The fodler containing the trained cellpose model
cellpose-training Folder: The exported raw and ground truth images that the above cellpose model was trained on
Scripts: The QuPath scripts, also located in their respective QuPath projects, that were created for this whole workflow
QC: A Jupyter notebook, based on ZeroCostDL4Mic that computes quality metrics in order to assess the performance of the trained cellpose model. The folder also contains the resulting metrics.</p>
<p>Installation and Use
If you are going to use the QuPath projects, you need a local QuPath Installation <a class="reference external" href="https://qupath.github.io/">https://qupath.github.io/</a> that is configured to run the QuPath Cellpose Extension <a class="github reference external" href="https://github.com/BIOP/qupath-extension-cellpose">BIOP/qupath-extension-cellpose</a> as well as a working Cellpose installation <a class="github reference external" href="https://github.com/MouseLand/cellpose">MouseLand/cellpose</a>
Instructions for installation are available from the links above.
After that, you should be able to open the QuPath project, navigate to the “Automate &gt; Project scripts” menu and locate the script you wish to run.</p>
<ol class="arabic simple">
<li><p>train a cell segmentation algorithm in the context of the rat brain Layer Boundaries project </p></li>
<li><p>trigger cell segmentation from a QuPath project in a semi-automated pipeline</p></li>
</ol>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/12656468">https://zenodo.org/records/12656468</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.12656468">https://doi.org/10.5281/zenodo.12656468</a></p>
</section>
<hr class="docutils" />
<section id="deep-learning-training-data-jove">
<h2>Deep learning training data (JOVE)<a class="headerlink" href="#deep-learning-training-data-jove" title="Link to this heading">#</a></h2>
<p>Jessica Heebner, Carson Purnell, Ryan Hylton, Mike Marsh, Michael Grillo, Matt Swulius</p>
<p>Published 2022-11-18</p>
<p>Licensed CC-ZERO</p>
<p>Cryo-electron tomography (cryo-ET) allows researchers to image cells in their native, hydrated state at the highest resolution currently possible. However, the technique has several limitations that make analyzing the data it generates time-intensive and difficult. Hand-segmenting a single tomogram can take hours to days of human effort, but the microscope can easily generate 50 or more tomograms a day. Current deep learning segmentation programs for cryo-ET do exist but are limited to segmenting one structure at a time. Here multi-slice U-Net convolutional neural networks are trained and applied to automatically segment multiple structures simultaneously within cryo-tomograms. With proper preprocessing, these networks can be robustly inferred to many tomograms without the need for training individual networks for each tomogram. This workflow dramatically improves the speed with which cryo-electron tomograms can be analyzed by cutting segmentation time down to under 30 min in most cases. Further, segmentations can be used to improve the accuracy of filament tracing within a cellular context and to rapidly extract coordinates for subtomogram averaging.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/7335439">https://zenodo.org/records/7335439</a></p>
<p><a class="reference external" href="https://doi.org/10.5061/dryad.rxwdbrvct">https://doi.org/10.5061/dryad.rxwdbrvct</a></p>
</section>
<hr class="docutils" />
<section id="ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking">
<h2>HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking<a class="headerlink" href="#ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking" title="Link to this heading">#</a></h2>
<p>Estibaliz Gómez-de-Mariscal, Hasini Jayatilaka, Denis Wirtz, Arrate Muñoz-Barrutia</p>
<p>Published 2021-12-13</p>
<p>Licensed CC-BY-4.0</p>
<p>Human fibrosarcoma HT1080WT (ATCC) cells at low cell densities embedded in 3D collagen type I matrices [1]. The time-lapse videos were recorded every 2 minutes for 16.7 hours and covered a field of view of 1002 pixels × 1004 pixels with a pixel size of 0.802 μm/pixel The videos were pre-processed to correct frame-to-frame drift artifacts, resulting in a final size of 983 pixels × 985 pixels pixels.</p>
<p>Hasini Jayatilaka, Anjil Giri, Michelle Karl, Ivie Aifuwa, Nicholaus J Trenton, Jude M Phillip, Shyam Khatau, and Denis Wirtz. EB1 and cytoplasmic dynein mediate protrusion dynamics for efficient 3-dimensional cell migration. FASEB J., 32(3):1207–1221, 2018. ISSN 0892-6638. doi: 10.1096/fj.201700444RR.</p>
<p>Further information about how to use this data is given in <a class="github reference external" href="https://github.com/esgomezm/microscopy-dl-suite-tf">esgomezm/microscopy-dl-suite-tf</a></p>
<p>This dataset is provided together with the following preprint and if you use it, we would like to kindly ask you to cite it properly:</p>
<p>Estibaliz Gómez-de-Mariscal, Hasini Jayatilaka, Özgün Çiçek, Thomas Brox, Denis Wirtz, Arrate Muñoz-Barrutia, <em>Search for temporal cell segmentation robustness in phase-contrast microscopy videos</em>, arXiv 2021 (arXiv:2112.08817)</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/5979761">https://zenodo.org/records/5979761</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.5979761">https://doi.org/10.5281/zenodo.5979761</a></p>
</section>
<hr class="docutils" />
<section id="lynsec-lymphoma-nuclear-segmentation-and-classification">
<h2>LyNSeC: Lymphoma Nuclear Segmentation and Classification<a class="headerlink" href="#lynsec-lymphoma-nuclear-segmentation-and-classification" title="Link to this heading">#</a></h2>
<p>Naji Hussein, Büttner Reinhard, Simon Adrian, Eich Marie-Lisa, Lohneis Philipp, Bozek Katarzyna</p>
<p>Published 2023-06-21</p>
<p>Licensed CC-BY-4.0</p>
<p>Over the last years, there has been large progress in automated segmentation and classification methods in histological whole slide images (WSIs) stained with hematoxylin and eosin (H&amp;E). Current state-of-the-art techniques are based on diverse datasets of H&amp;E-stained WSIs of different types of predominantly solid cancer. However, there is a lack of publicly available annotated datasets of lymphoma, which is why we generated a labeled diffuse large B-cell lymphoma dataset and denoted it LyNSeC (lymphoma nuclear segmentation and classification). LyNSeC comprises three subsets: LyNSeC 1 consists of 379 IHC images of size 512 x 512 pixels at 40x magnification. In the images, we annotated the contours of each cell nuclei and the cell class: marker-positive or marker-negative.</p>
<p>In total, LyNSeC 1 contains 87,316 annotated cell nuclei of four different cases, with 48,171 of them assigned the class negative and 39,145 positive. We included three markers in this dataset showing visually different staining patterns: cluster of differentiation 3 (CD3), Ki67 as a marker of proliferation, and erythroblast transformation-specific (EST)-related gene (ERG).</p>
<p>LyNSeC 2 and 3 contain H&amp;E-stained images of 70 different patients. LyNSeC 2 consists of 280 images and LyNSeC 3 of 40 images of size 512 x 512 pixels at 40x magnification. 65,479 and 8,452 nuclei were annotated in LyNSeC 2 and 3, respectively. In LyNSeC 3, the nuclei were also assigned a class label (tumor and non-tumor). 3,747 nuclei were identified as tumors and 4,705 as non-tumors.</p>
<p>In the annotation procedure, the contours of the H&amp;E images (LyNSeC 2 and LyNSeC 3) were annotated by two pathologists and by two students (trained by the pathologists). Annotation of the cell classes in LyNSeC 3 was done by the pathologists only. LyNSeC 1 was annotated by the two students who were additionally trained to annotate the contours and to distinguish marker-positive and marker-negative cells. The pathologists inspected and (if necessary) adjusted the LyNSeC 3 annotations.</p>
<p>The files are uploaded in ‘.npy’ format. The files of LyNSeC 1 (x_l1.npy) and LyNSeC 3 (x_l3.npy) contain five channels, respectively: the first three are the RGB channels of the images, channel 4 contains the instance maps, and channel 5 the class type maps (for LyNSeC 1 a pixel value of 1 corresponds to the class negative and 2 to the class positive, whereas in LyNSeC 3 1 corresponds to the class non-tumor and 2 to the class tumor). The files of LyNSeC 2 (x_l2.npy) have 4 channels (without the class type map).</p>
<p>Additionally, we also make our HoVer-Net-based pre-trained nuclei segmentation and classification models available (he.tar for H&amp;E images and ihc.tar for IHC images).</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8065174">https://zenodo.org/records/8065174</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8065174">https://doi.org/10.5281/zenodo.8065174</a></p>
</section>
<hr class="docutils" />
<section id="melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations">
<h2>Melanoma Histopathology Dataset with Tissue and Nuclei Annotations<a class="headerlink" href="#melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations" title="Link to this heading">#</a></h2>
<p>Mark Schuiveling</p>
<p>Published 2025-03-19</p>
<p>Licensed CC-ZERO</p>
<p>Description:
This dataset is designed for development of deep learning models for segmentation of nuclei and tissue in melanoma H&amp;E stained histopathology. Existing nuclei segmentation models that are trained on non-melanoma specific datasets have low performance due to the ability of melanocytes to mimic other cell types, whereas existing melanoma specific models utilize older, sub-optimal techniques. Moreover, these models do not provide tissue annotations necessary for determining the localization of tumor-infiltrating lymphocytes, which may hold value for predictive and prognostic tasks. To address this, we created a melanoma specific dataset with nuclei and tissue annotations. 
Methodology:
Sample Collection:
Regions of interest (ROIs) were sampled from H&amp;E stained slides of 103 primary melanoma specimens and 102 metastatic melanoma specimens, scanned using a Hamamatsu scanner at 40× magnification (0.23 μm per pixel). All slides were obtained from regular diagnostic procedures.From each specimen, a 40× magnified ROI of 1024×1024 pixels was selected for annotation. Additionally, a context ROI of 5120×5120 pixels was sampled to provide information about the broader context for the annotation process. Selection was performed by a trained medical expert (M.S.) and subsequently verified by a dermatopathologist (W.B.). Manual ROI selection ensured the inclusion of diverse tissue and nuclei types.
Annotation Process:</p>
<p>Nuclei segmentationNuclei segmentations were generated using Hover-Net pretrained on the PanNuke dataset. Manual annotation adjustments were performed by author M.S. using QuPath, with the following nuclei categories: tumor, stroma, vascular endothelium, histiocyte, melanophage, lymphocyte, plasma cell, neutrophil, apoptotic cell, and epithelium. All annotations were reviewed and corrected, where needed, by a dermatopathologist (W.B.).
Tissue segmentationTissue segmentations were created manually using QuPath by M.S., with the following categories: tumor, stroma, epidermis, necrosis, blood vessel, and background. Annotations were reviewed and corrected, where needed, by a dermatopathologist (W.B.).</p>
<p>Quality Control: To assess the reliability of the annotations, intra- and interobserver agreement (by pathologist G.B.) were determined on 12 randomly selected ROIs.</p>
<p>Nuclei segmentationThe intraobserver overall precision was 84.89%, with a recall of 86.45%, and an F1 score of 85.66%. Interobserver overall precision was 80.34%, with a recall of 80.62%, and an F1 score of 80.20%. These results are based on the sum of all true positive, false positive, and false negative counts for the 12 ROIs.
Tissue segmentationThe DICE score was determined on the same 12 randomly selected ROIs. The average intraobserver DICE score was 0.90, and the interobserver DICE score was also 0.90.</p>
<p> 
Version 3:Removed sample “training_set_metastatic_roi_103” due to inconsistencies in annotation file.
Version 4:Sample training_set_metastatic_roi_088 missed one color annotation for a nuclei_apoptosis in the geojson file rendering it qupath uncompatible. This is fixed in the new version. 
Version 5:Addition of correct sample of training_set_metastatic_roi_103” after deadline of panoptic segmentation of nuclei and tissue in advanced melanoma challenge test phase. </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/15050523">https://zenodo.org/records/15050523</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.15050523">https://doi.org/10.5281/zenodo.15050523</a></p>
</section>
<hr class="docutils" />
<section id="neurips-2022-cell-segmentation-competition-dataset">
<h2>NeurIPS 2022 Cell Segmentation Competition Dataset<a class="headerlink" href="#neurips-2022-cell-segmentation-competition-dataset" title="Link to this heading">#</a></h2>
<p>Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, Wei Lou, Haofeng Li, Eric Upschulte, Timo Dickscheid, de Almeida, José Guilherme, Yixin Wang, Lin Han, Xin Yang, Marco Labagnara, Vojislav Gligorovski, Maxime Scheder, Rahi, Sahand Jamal, Carly Kempster, Alice Pollitt, Leon Espinosa, Tam Mignot, Middeke, Jan Moritz, Jan-Niklas Eckardt, Wangkai Li, Zhaoyang Li, Xiaochen Cai, Bizhe Bai, Greenwald, Noah F., Van Valen, David, Erin Weisbart, Cimini, Beth A, Trevor Cheung, Oscar Brück, Bader, Gary D., Bo Wang</p>
<p>Published 2024-02-27</p>
<p>Licensed CC-BY-NC-ND-4.0</p>
<p>The official data set for the NeurIPS 2022 competition: cell segmentation in multi-modality microscopy images.
<a class="reference external" href="https://neurips22-cellseg.grand-challenge.org/">https://neurips22-cellseg.grand-challenge.org/</a>
Please cite the following paper if this dataset is used in your research. 
 
&#64;article{NeurIPS-CellSeg,
title = {The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions},
author = {Jun Ma and Ronald Xie and Shamini Ayyadhury and Cheng Ge and Anubha Gupta and Ritu Gupta and Song Gu and Yao Zhang and Gihun Lee and Joonkee Kim and Wei Lou and Haofeng Li and Eric Upschulte and Timo Dickscheid and José Guilherme de Almeida and Yixin Wang and Lin Han and Xin Yang and Marco Labagnara and Vojislav Gligorovski and Maxime Scheder and Sahand Jamal Rahi and Carly Kempster and Alice Pollitt and Leon Espinosa and Tâm Mignot and Jan Moritz Middeke and Jan-Niklas Eckardt and Wangkai Li and Zhaoyang Li and Xiaochen Cai and Bizhe Bai and Noah F. Greenwald and David Van Valen and Erin Weisbart and Beth A. Cimini and Trevor Cheung and Oscar Brück and Gary D. Bader and Bo Wang},
journal = {Nature Methods},      volume={21},      pages={1103–1113},      year = {2024},
doi = {<a class="reference external" href="https://doi.org/10.1038/s41592-024-02233-6">https://doi.org/10.1038/s41592-024-02233-6</a>}
}
 
This is an instance segmentation task where each cell has an individual label under the same category (cells). The training set contains both labeled images and unlabeled images. You can only use the labeled images to develop your model but we encourage participants to try to explore the unlabeled images through weakly supervised learning, semi-supervised learning, and self-supervised learning.
 
The images are provided with original formats, including tiff, tif, png, jpg, bmp… The original formats contain the most amount of information for competitors and you have free choice over different normalization methods. For the ground truth, we standardize them as tiff formats.
 
We aim to maintain this challenge as a sustainable benchmark platform. If you find the top algorithms (<a class="reference external" href="https://neurips22-cellseg.grand-challenge.org/awards/">https://neurips22-cellseg.grand-challenge.org/awards/</a>) don’t perform well on your images, welcome to send us the dataset (<a class="reference external" href="mailto:neurips&#46;cellseg&#37;&#52;&#48;gmail&#46;com">neurips<span>&#46;</span>cellseg<span>&#64;</span>gmail<span>&#46;</span>com</a>)! We will include them in the new testing set and credit your contributions on the challenge website!
 
Dataset License: CC-BY-NC-ND</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/10719375">https://zenodo.org/records/10719375</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.10719375">https://doi.org/10.5281/zenodo.10719375</a></p>
</section>
<hr class="docutils" />
<section id="ocelot-overlapped-cell-on-tissue-dataset-for-histopathology">
<h2>OCELOT: Overlapped Cell on Tissue Dataset for Histopathology<a class="headerlink" href="#ocelot-overlapped-cell-on-tissue-dataset-for-histopathology" title="Link to this heading">#</a></h2>
<p>Jeongun Ryu, Aaron Valero Puche, JaeWoong Shin, Seonwook Park, Biagio Brattoli, Mohammad Mostafavi, Jinhee Lee, Sérgio Pereira, Wonkyung Jung, Soo Ick Cho, Chan-Young Ock, Kyunghyun Paeng, Donggeun Yoo</p>
<p>Published 2023-03-23</p>
<p>The OCELOT dataset is a histopathology dataset designed to facilitate the development of methods that utilize cell and tissue relationships. The dataset comprises both small and large field-of-view (FoV) patches extracted from digitally scanned whole slide images (WSIs), with overlapping regions. The small and large FoV patches are accompanied by annotations of cells and tissues, respectively. The WSIs are sourced from the publicly available TCGA database and were stained using the H&amp;E method before being scanned with an Aperio scanner.</p>
<p>For more details, please check <a class="reference external" href="https://lunit-io.github.io/research/ocelot_dataset/">https://lunit-io.github.io/research/ocelot_dataset/</a>.</p>
<p> </p>
<p>Before downloading the dataset, please make sure to carefully read and agree to the Terms and Conditions at (<a class="reference external" href="https://lunit-io.github.io/research/ocelot_tc/">https://lunit-io.github.io/research/ocelot_tc/</a>).</p>
<p>Also, please provide 1. name, 2. e-mail address, 3. organization/company name.</p>
<p> </p>
<hr class="docutils" />
<p>Release note.</p>
<p>In version 1.0.1, we exclude four test cases (586, 589, 609, 615) due to under-annotated issue.
In version 1.0.0, we include images and annotations of validation and test splits.
In version 0.1.2, we modified the coordinates of cell labels to range from 0 to 1023 (-1 from the previous coordinates).
In version 0.1.1, we removed non-H&amp;E stained patches from the dataset.</p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8417503">https://zenodo.org/records/8417503</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8417503">https://doi.org/10.5281/zenodo.8417503</a></p>
</section>
<hr class="docutils" />
<section id="volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset">
<h2>Volumetric segmentation of biological cells and subcellular structures for optical diffraction tomography images - dataset<a class="headerlink" href="#volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset" title="Link to this heading">#</a></h2>
<p>Martyna Mazur, Wojciech Krauze</p>
<p>Published 2023-06-16</p>
<p>Licensed CC-BY-4.0</p>
<p>This dataset includes 4 files with segmentation results for 4 different ODT reconstructions of SH-SY5Y neuroblastoma cell. The segmentation results contain:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>3D binary masks of biological cells obtained through Cellpose [1] and ODT-SAS;
3D binary masks of organelles: nucleoli and lipid structures (LS) obtained through slice-by-slice manual segmentation&amp;nbsp;and ODT-SAS.
</pre></div>
</div>
<p>All files are .*mat files.</p>
<p>The files REC_SH-SY5Y_1.mat, REC_SH-SY5Y_2.mat and REC_SH-SY5Y_3.mat consist of 7 variables:</p>
<p>RECON – tomographic reconstruction of SH-SY5Y neuroblastoma cell;
n_imm – refractive index of object immersion medium;
dx – object space sample size in XY [(\mu m)];
rayXY – xy-coordinates of illumination vectors;</p>
<p>maskManual – table with manually determined 3D binary masks of organelles;
maskCellpose – 3D binary mask of biological cell obtained through Cellpose;
maskODTSAS – table with 3D binary masks of biological cell and their organelles obtained through ODT-SAS.</p>
<p>File REC_SH-SY5Y_4.mat includes masks for the ODT-SAS and Cellpose segmentation of three closely packed cells and consists of 5 variables: RECON, n_imm, dx, maskCellpose and maskODTSAS.</p>
<p>Access a particular 3D binary mask from ‘maskManual’ and ‘maskODTSAS’ tables, using the following names: ‘Cell’, ‘Nucleoli’, ‘LS’.
For example:</p>
<p>cellMask = maskODTSAS.Cell{1};</p>
<p>[1] Stringer, C., Wang, T., Michaelos, M., &amp; Pachitariu, M. (2021). Cellpose: a generalist algorithm for cellular segmentation. Nature methods, 18(1), 100-106.</p>
<p> </p>
<p>Tags: Ai-Ready</p>
<p>Content type: Data</p>
<p><a class="reference external" href="https://zenodo.org/records/8188948">https://zenodo.org/records/8188948</a></p>
<p><a class="reference external" href="https://doi.org/10.5281/zenodo.8188948">https://doi.org/10.5281/zenodo.8188948</a></p>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="readme.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">NFDI4BioImage Training Materials</p>
      </div>
    </a>
    <a class="right-next"
       href="contributing/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How to contribute</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellbindb-a-large-scale-multimodal-annotated-dataset">CellBinDB: A Large-Scale Multimodal Annotated Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-inhibition-of-cers1-in-aging-skeletal-muscle-exacerbates-age-related-muscle-impairments">Cellpose training data and scripts from “Inhibition of CERS1 in aging skeletal muscle exacerbates age-related muscle impairments”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellpose-training-data-and-scripts-from-machine-learning-for-histological-annotation-and-quantification-of-cortical-layers">Cellpose training data and scripts from “Machine learning for histological annotation and quantification of cortical layers”</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-training-data-jove">Deep learning training data (JOVE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ht1080wt-cells-embedded-in-3d-collagen-type-i-matrices-manual-annotations-for-cell-instance-segmentation-and-tracking">HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lynsec-lymphoma-nuclear-segmentation-and-classification">LyNSeC: Lymphoma Nuclear Segmentation and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melanoma-histopathology-dataset-with-tissue-and-nuclei-annotations">Melanoma Histopathology Dataset with Tissue and Nuclei Annotations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurips-2022-cell-segmentation-competition-dataset">NeurIPS 2022 Cell Segmentation Competition Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ocelot-overlapped-cell-on-tissue-dataset-for-histopathology">OCELOT: Overlapped Cell on Tissue Dataset for Histopathology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#volumetric-segmentation-of-biological-cells-and-subcellular-structures-for-optical-diffraction-tomography-images-dataset">Volumetric segmentation of biological cells and subcellular structures for optical diffraction tomography images - dataset</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Robert Haase, Clément Caporal,... and the NFDI4BioImage Initiative
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on 2025-05-29.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Copyright: Licensed <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC-BY 4.0</a> unless mentioned otherwise. 
Contributions and feedback are welcome.
</p>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>